{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ec44e5",
   "metadata": {},
   "source": [
    "#### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb00f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.vision.models.unet import *\n",
    "from fastai.torch_basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c867ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialExDict(nn.Sequential):\n",
    "    \"Like `nn.Sequential`, but has a dictionary passed along with x.\"\n",
    "    def __init__(self, *layers,dict_names=['seq_dict']): \n",
    "        super().__init__(*layers)\n",
    "        self.dict_names=dict_names\n",
    "    def forward(self, x,**kwargs):\n",
    "        dicts = getattrs(x,*self.dict_names,default=kwargs)\n",
    "        for module in self:\n",
    "            for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n",
    "            x = module(x)\n",
    "        for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed6534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.n_channels // 4, self.n_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.n_channels, self.n_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        t=torch.tensor(x.seq_dict['t']) if isinstance(x.seq_dict['t'],int) else x.seq_dict['t']\n",
    "        t=t.view(t.shape[0])\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.layers(emb)\n",
    "        x.seq_dict['time']=emb\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cb4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnKey(nn.Module):\n",
    "    def __init__(self,k_in,module,k_out=None):\n",
    "        super().__init__()\n",
    "        if(k_out is None): k_out=k_in+'_out'\n",
    "        self.k_in=k_in\n",
    "        self.k_out=k_out\n",
    "        self.f=module\n",
    "    def forward(self, x):\n",
    "        x.seq_dict[self.k_out]=self.f(x.seq_dict[self.k_in])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe70f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack(nn.Module):\n",
    "    def __init__(self,key,f=lambda x:x):\n",
    "        super().__init__()\n",
    "        self.key,self.f=key,f\n",
    "    def forward(self,x):\n",
    "        if(self.key not in x.seq_dict): x.seq_dict[self.key]=[]\n",
    "        x.seq_dict[self.key]+=[self.f(x)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968207d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pop(nn.Module):\n",
    "    def __init__(self,key,f,clear=True,**kwargs):\n",
    "        super().__init__()\n",
    "        self.key,self.clear,self.f,self.kwargs=key,clear,f,kwargs\n",
    "    def forward(self, x): \n",
    "        o=x.seq_dict[self.key]\n",
    "        if(is_listy(o)): \n",
    "            o =  x.seq_dict[self.key].pop(-1) if(self.clear) else o[-1]\n",
    "        elif(self.clear): x.seq_dict[self.key]=None\n",
    "        return self.f(x,o,**self.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c5cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(x,o,dense=False): return torch.cat((x,o),dim=1) if(dense) else x+o.view(o.shape+(1,)*(x.ndim-o.ndim)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f1048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetTime(nn.Module):\n",
    "    \"A little Unet with time embeddings\"\n",
    "    def __init__(self,dims=[96, 192, 384, 768, 768],img_channels=3,ks=7,stem_stride=4,t_channels=128):\n",
    "        super().__init__()\n",
    "        i_d=0\n",
    "        h=dims[i_d]\n",
    "        self.time_emb=TimeEmbedding(t_channels)\n",
    "        # Not putting in for loop for ease of understanding arch\n",
    "        self.down=SequentialExDict(\n",
    "            nn.Conv2d(img_channels,h,ks,1,ks//2),\n",
    "            Stack('u'),\n",
    "            Stack('s',lambda x:x.shape[-2:]),\n",
    "            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,stem_stride,1),\n",
    "            nn.GroupNorm(1,h),\n",
    "            Stack('u'),\n",
    "            Stack('s',lambda x:x.shape[-2:]),\n",
    "            self.basic_block(h,t_channels,ks=ks),\n",
    "            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n",
    "            Stack('u'),\n",
    "            Stack('s',lambda x:x.shape[-2:]),\n",
    "            self.basic_block(h,t_channels,ks=ks),\n",
    "            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n",
    "            Stack('u'),\n",
    "            Stack('s',lambda x:x.shape[-2:]),\n",
    "            self.basic_block(h,t_channels,ks=ks),\n",
    "            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n",
    "            Stack('u'),\n",
    "        )\n",
    "        self.middle=SequentialExDict(\n",
    "            self.basic_block(h,t_channels)\n",
    "        )\n",
    "        self.up=SequentialExDict(\n",
    "            Pop('u',merge,dense=True),\n",
    "            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n",
    "            self.basic_block(h,t_channels),\n",
    "            Pop('u',merge,dense=True),\n",
    "            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n",
    "            self.basic_block(h,t_channels),\n",
    "            Pop('u',merge,dense=True),\n",
    "            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n",
    "            self.basic_block(h,t_channels),\n",
    "            Pop('u',merge,dense=True),\n",
    "            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n",
    "            self.basic_block(h,t_channels),\n",
    "            Pop('u',merge,dense=True),\n",
    "            self.down_sample(h*2,img_channels,5,1,2,bias=True),\n",
    "            self.basic_block(img_channels,t_channels,bias=True),\n",
    "        )\n",
    "        self.layers=SequentialExDict(\n",
    "            self.time_emb,\n",
    "            self.down,\n",
    "            self.middle,\n",
    "            self.up\n",
    "        )\n",
    "    @delegates(nn.Conv2d.__init__)\n",
    "    def up_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n",
    "        return SequentialExDict(\n",
    "            Pop('s',lambda x,o:F.interpolate(x, size=[oi+1 for oi in o], mode='bilinear')),\n",
    "            self.down_sample(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n",
    "        )\n",
    "    @delegates(nn.Conv2d.__init__)\n",
    "    def down_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n",
    "        return SequentialExDict(\n",
    "            nn.GroupNorm(1,in_channels),\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n",
    "        )\n",
    "    def basic_block(self,channels,time_channels,expansion=4,ks=7,stride=1,pad=None,bias=False):\n",
    "        if pad is None: pad=ks//2\n",
    "        return SequentialExDict(\n",
    "            Stack('r'),\n",
    "            nn.Conv2d(channels,channels,ks,padding=pad,bias=bias,stride=stride),\n",
    "            nn.GroupNorm(1,channels),\n",
    "            nn.Conv2d(channels,channels*expansion,1,bias=bias),\n",
    "            OnKey('time',nn.Linear(time_channels,channels*expansion)),\n",
    "            Pop('time_out',merge),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels*expansion,channels,1,bias=bias),\n",
    "            Pop('r',merge),\n",
    "        )\n",
    "    def forward(self,x,x_o,t):\n",
    "        return self.layers(x,t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361b194",
   "metadata": {},
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba946c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
