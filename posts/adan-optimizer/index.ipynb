{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49e8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b34290",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d485ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prev_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n",
    "    \"Keeps track of the previous gradient, should be one of last cbs. \"\n",
    "    return {'prev_grad': p.grad.data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d79cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sqr_grad??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "567f780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_avg_grad(p,lr,nmom=None,n_avg=None,**kwags):\n",
    "    if n_avg is None: \n",
    "        prev_grad=torch.zeros_like(p.grad.data)\n",
    "        n_avg = p.grad.data-prev_grad\n",
    "    else:\n",
    "        n_avg = (1-nmom)*n_avg+nmom*(p.grad.data-prev_grad)\n",
    "    return {'n_avg': n_avg,'prev_grad':prev_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2821e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_average_sqr_grad(p,nmom,sqr_mom, prev_grad=None, dampening=True, sqr_avg=None, **kwargs):\n",
    "    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    grad = (2-nmom)*p.grad.data+(nmom-1)*prev_grad\n",
    "    sqr_avg.mul_(sqr_mom).addcmul_(grad,grad, value=damp)\n",
    "    return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d21c72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adan(params, lr, mom=0.9, sqr_mom=0.99,nmom=0.9, eps=1e-5, wd=0.01, decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True),n_avg_grad, n_average_sqr_grad,adan_step, update_prev_grad]\n",
    "    return Optimizer(params, cbs, lr=lr,nmom=nmom, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dabcc027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2699,  0.2463,  0.2389,  0.3609],\n",
      "        [-0.4823,  0.1599, -0.1185,  0.1422],\n",
      "        [ 0.4541, -0.1880,  0.0794, -0.3760],\n",
      "        [-0.2717,  0.3392,  0.1453, -0.1475]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'prev_grad' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [116]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      7\u001b[0m F\u001b[38;5;241m.\u001b[39mmse_loss(l(inp),torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;241m3.\u001b[39m,\u001b[38;5;241m4.\u001b[39m]))\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/fastai/fastai/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastai optimizers currently do not support closure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,pg,state,hyper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_params(with_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: state \u001b[38;5;241m=\u001b[39m _update(state, \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyper\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p] \u001b[38;5;241m=\u001b[39m state\n",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36mn_avg_grad\u001b[0;34m(p, lr, nmom, n_avg, **kwags)\u001b[0m\n\u001b[1;32m      4\u001b[0m     n_avg \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m-\u001b[39mprev_grad\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     n_avg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mnmom)\u001b[38;5;241m*\u001b[39mn_avg\u001b[38;5;241m+\u001b[39mnmom\u001b[38;5;241m*\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m-\u001b[39m\u001b[43mprev_grad\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_avg\u001b[39m\u001b[38;5;124m'\u001b[39m: n_avg,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_grad\u001b[39m\u001b[38;5;124m'\u001b[39m:prev_grad}\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'prev_grad' referenced before assignment"
     ]
    }
   ],
   "source": [
    "l=nn.Linear(4,4)\n",
    "opt=Adan(l.parameters(),0.01)\n",
    "print(l.weight)\n",
    "inp=torch.tensor([.1,.2,.3,.4])\n",
    "F.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\n",
    "opt.step()\n",
    "F.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "062d9961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2711, -0.0468,  0.0525, -0.1353],\n",
       "        [ 0.1062, -0.3647,  0.4919, -0.2543],\n",
       "        [-0.3699,  0.1654,  0.1101,  0.0434],\n",
       "        [-0.2083,  0.3848, -0.1086,  0.3713]], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea1d0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adan_step(p,lr,grad_avg=None,nmom=None,n_avg=None,sqr_avg=None,\n",
    "             eps=None,**kwargs):\n",
    "    p.data.addcdiv_(grad_avg+(1-nmom)*n_avg, \n",
    "                    (sqr_avg).sqrt() + eps, \n",
    "                    value = -lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a24e1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0.01, decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay] if decouple_wd else [l2_reg]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
