[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "marii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/adan-optimizer/index.html",
    "href": "posts/adan-optimizer/index.html",
    "title": "blog",
    "section": "",
    "text": "def update_prev_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n    \"Keeps track of the previous gradient, should be one of last cbs. \"\n    return {'prev_grad': p.grad.data}\n\n\ndef n_avg_grad(p,lr,nmom=None,n_avg=None,prev_grad=None,**kwags):\n    if n_avg is None: \n        prev_grad=torch.zeros_like(p.grad.data)\n        n_avg = p.grad.data-prev_grad\n    else:\n        n_avg = (1-nmom)*n_avg+nmom*(p.grad.data-prev_grad)\n    return {'n_avg': n_avg,'prev_grad':prev_grad}\n\n\ndef n_average_sqr_grad(p,nmom,sqr_mom, prev_grad=None, dampening=True, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    damp = 1-sqr_mom if dampening else 1.\n    grad = (2-nmom)*p.grad.data+(nmom-1)*prev_grad\n    sqr_avg.mul_(sqr_mom).addcmul_(grad,grad, value=damp)\n    return {'sqr_avg': sqr_avg}\n\n\ndef adan_step(p,lr,grad_avg=None,nmom=None,n_avg=None,sqr_avg=None,\n             eps=None,**kwargs):\n    p.data.addcdiv_(grad_avg+(1-nmom)*n_avg, \n                    (sqr_avg).sqrt() + eps, \n                    value = -lr)\n\n\ndef Adan(params, lr, mom=0.9, sqr_mom=0.99,nmom=0.9, eps=1e-5, wd=0.01, decouple_wd=True):\n    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n    cbs = [weight_decay] if decouple_wd else [l2_reg]\n    cbs += [partial(average_grad, dampening=True),n_avg_grad, n_average_sqr_grad,adan_step, update_prev_grad]\n    return Optimizer(params, cbs, lr=lr,nmom=nmom, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n\n\nl=nn.Linear(4,4)\nopt=Adan(l.parameters(),0.01)\nprint(l.weight)\ninp=torch.tensor([.1,.2,.3,.4])\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\n\nParameter containing:\ntensor([[ 0.4984,  0.2108,  0.3309, -0.1065],\n        [-0.4451,  0.3669, -0.2573,  0.1675],\n        [-0.3011, -0.4368, -0.3770,  0.4079],\n        [-0.0182,  0.3828,  0.4397, -0.0060]], requires_grad=True)\n\n\n\nl.weight\n\nParameter containing:\ntensor([[ 0.5296,  0.2421,  0.3622, -0.0752],\n        [-0.4137,  0.3981, -0.2259,  0.1988],\n        [-0.2698, -0.4054, -0.3455,  0.4392],\n        [ 0.0132,  0.4141,  0.4709,  0.0254]], requires_grad=True)"
  },
  {
    "objectID": "posts/diffusion/index.html",
    "href": "posts/diffusion/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.vision.all import *\nfrom fastai.torch_basics import *\nfrom denoising_diffusion_pytorch import Unet\n\n\nimport wandb\nwandb.init(reinit=True)\nfrom fastai.callback.wandb import *\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: marii. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /home/molly/Projects/quatro-blog/posts/diffusion/wandb/run-20221006_011034-nhpww4wt\n\n\nSyncing run super-universe-63 to Weights & Biases (docs)\n\n\n\ndef gather(consts: torch.Tensor, t: torch.Tensor):\n    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n    c = consts.gather(-1, t)\n    return c.reshape(-1, 1, 1, 1)\n\n\nclass DenoiseDiffusion:\n    \"\"\"\n    ## Denoise Diffusion\n    \"\"\"\n\n    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n        \"\"\"\n        * `eps_model` is $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n        * `n_steps` is $t$\n        * `device` is the device to place constants on\n        \"\"\"\n        super().__init__()\n        self.eps_model = eps_model\n\n        # Create $\\beta_1, \\dots, \\beta_T$ linearly increasing variance schedule\n        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n\n        # $\\alpha_t = 1 - \\beta_t$\n        self.alpha = 1. - self.beta\n        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n        # $T$\n        self.n_steps = n_steps\n        # $\\sigma^2 = \\beta$\n        self.sigma2 = self.beta\n\n    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        #### Get $q(x_t|x_0)$ distribution\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # [gather](utils.html) $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n        mean = gather(self.alpha_bar, t) ** 0.5 * tensor(x0)\n        # $(1-\\bar\\alpha_t) \\mathbf{I}$\n        var = 1 - gather(self.alpha_bar, t)\n        #\n        return mean, var\n\n    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n        \"\"\"\n        #### Sample from $q(x_t|x_0)$\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        if eps is None:\n            eps = torch.randn_like(x0)\n\n        # get $q(x_t|x_0)$\n        mean, var = self.q_xt_x0(x0, t)\n        # Sample from $q(x_t|x_0)$\n        return mean + (var ** 0.5) * eps\n\n    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n        \\begin{align}\n        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n        \n        # NOTEDDDD REMOVED t\n        \n        eps_theta = self.eps_model(xt,t)\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n        # $\\alpha_t$\n        alpha = gather(self.alpha, t)\n        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n        # $\\sigma^2$\n        var = gather(self.sigma2, t)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        eps = torch.randn(xt.shape, device=xt.device)\n        # Sample\n        return mean + (var ** .5) * eps\n\n\nclass Q_sample(ItemTransform):\n    order=101\n    def __init__(self,diffusion):\n        self.diffusion=diffusion\n    def encodes(self,xy):\n        x=xy[0]\n        y=xy[-1]\n        ts = xy[2][:,0]#torch.randint(0, self.diffusion.n_steps, (x.shape[0],), device=x.device, dtype=torch.long)\n        x_type=type(x)\n        x=self.diffusion.q_sample(x, x_type(ts), eps=y)\n        return (x,*xy[1:-1],y)\n\n\nclass LabelToNoise(ItemTransform):\n    order=100\n    def encodes(self,xy):\n        y=xy[-1]\n        return (*xy[:-1],retain_type(torch.randn(y.shape,device=y.device),old=y))\n\n\ndef sample():\n    \"\"\"\n    ### Sample images\n    \"\"\"\n    with torch.no_grad():\n        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n        x = torch.randn([n_samples, image_channels, 32, 32],\n                        device=device)\n\n        # Remove noise for $T$ steps\n        for t_ in range(n_steps):\n            # $t$\n            t = n_steps - t_ - 1\n            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n            x = diffusion.p_sample(x, x.new_full((n_samples,), t, dtype=torch.long))\n        return x\n\n\nn_steps=1000\n\n\npath = untar_data(URLs.MNIST)\npath = untar_data(URLs.CIFAR)\n\n\nm=Unet(dim=32,channels=3)#UnetTime(img_channels=1,dims=[32, 64, 128, 256, 256],ks=3,stem_stride=2).cuda()\n\n\n@typedispatch\ndef show_batch(x:tuple, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(3*min(len(samples), max_n), nrows=nrows, ncols=3, figsize=figsize, title='Input/Original/Target')\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::3],range(max_n))]\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::3],range(max_n))]\n    ctxs[1::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::3],range(max_n))]\n    ctxs[2::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[2::3],range(max_n))]\n    return ctxs\n\n\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(cls=PILImageBW),\n               ImageBlock(cls=PILImageBW),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock(cls=PILImageBW)),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n).dataloaders(path,bs=128,val_bs=2*128)\ndls.show_batch()\n\nIndexError: list index out of range\n\n\n\npath.ls()\n\n(#3) [Path('/home/molly/data/cifar10/labels.txt'),Path('/home/molly/data/cifar10/test'),Path('/home/molly/data/cifar10/train')]\n\n\n\nbs=128\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(),\n               ImageBlock(),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock()),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=IndexSplitter(range(bs)),\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n\n\n\n\nclass FlattenCallback(Callback):\n    def before_batch(self):\n        self.learn.xb=(self.xb[0],self.xb[-1].view(self.xb[-1].shape[::2]),)\n\n\nlearn = Learner(dls,m,MSELossFlat(),opt_func=Lamb,cbs=[FlattenCallback,WandbCallback(log_preds_every_epoch=True)])\n\ninp=m.layers0:0 inp.seq_dict[‘t’]=torch.tensor([5]).cuda() m.layers1:4.shape\n\nlearn.fit_flat_cos(6,lr=1e-4,wd=0.)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.699252\n      0.648226\n      01:58\n    \n    \n      1\n      0.375051\n      0.354264\n      01:54\n    \n    \n      2\n      0.185453\n      0.168021\n      01:52\n    \n    \n      3\n      0.102715\n      0.080112\n      01:53\n    \n    \n      4\n      0.064207\n      0.043586\n      01:56\n    \n    \n      5\n      0.055305\n      0.053669\n      01:52\n    \n  \n\n\n\nWandbCallback was not able to get prediction samples -> Match length mismatch\n\n\n\n@typedispatch\ndef show_results(x:tuple, y:TensorImage, samples, outs, ctxs=None, max_n=10, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(6*min(len(samples), max_n), ncols=6, figsize=figsize, title='Input/Original/DenoisedImage/Target/Prediction/Diff')\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::6],range(max_n))]\n    ctxs[1::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::6],range(max_n))]\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::6],range(max_n))]\n    ctxs[2::6] = [(b-o).show(ctx=c, **kwargs) for b,o,c,_ in zip(samples.itemgot(0),outs.itemgot(0),ctxs[2::6],range(max_n))]\n    ctxs[3::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[3::6],range(max_n))]\n    ctxs[4::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs[4::6],range(max_n))]\n    ctxs[5::6] = [(b-targ).show(ctx=c, **kwargs) for b,targ,c,_ in zip(outs.itemgot(0),samples.itemgot(3),ctxs[5::6],range(max_n))]\n    return ctxs\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nn_samples=12\nimage_channels=3\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndevice=torch.device(0)\nxs = sample()\n\n\nshow_images((logit((xs.repeat(1,3,1,1)-xs.repeat(1,3,1,1).mean())/xs.repeat(1,3,1,1).std()).sigmoid()),nrows=4)\n\n\nshow_images((logit((xs-xs.mean())/xs.std()).sigmoid()),nrows=4)\n\n\n\n\n\nxs.min()\n\n\nshow_images(xs)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nwith learn.removed_cbs(WandbCallback):\n    show_images(dls.one_batch()[0][:4])\n\n\nlearn.show_results??\n\n\ndls.show_batch(show=False)[2]\n\n\n@typedispatch\ndef wandb_process(x:tuple, y, samples, outs, preds):\n    \"Process `sample` and `out` depending on the type of `x/y`\"\n    res_input, res_pred, res_label = [],[],[]\n    for s,o in zip(samples, outs):\n        img = s[0].permute(1,2,0)\n        res_input.append(wandb.Image(img, caption='Input_data'))\n        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground_Truth\", res_label)):\n            fig, ax = _make_plt(img)\n            # Superimpose label or prediction to input image\n            ax = img.show(ctx=ax)\n            ax = t.show(ctx=ax)\n            res.append(wandb.Image(fig, caption=capt))\n            plt.close(fig)\n    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground_Truth\":res_label}\n\n\nlearn.show_results()\n\n\n%debug\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev import nbdev_export\nnbdev_export()\n:::"
  },
  {
    "objectID": "posts/unet/index.html",
    "href": "posts/unet/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.torch_basics import *\n\n\nclass SequentialExDict(nn.Sequential):\n    \"Like `nn.Sequential`, but has a dictionary passed along with x.\"\n    def __init__(self, *layers,dict_names=['seq_dict']): \n        super().__init__(*layers)\n        self.dict_names=dict_names\n    def forward(self, x,**kwargs):\n        dicts = getattrs(x,*self.dict_names,default=kwargs)\n        for module in self:\n            for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n            x = module(x)\n        for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n        return x\n\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    ### Embeddings for $t$\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n        \"\"\"\n        * `n_channels` is the number of dimensions in the embedding\n        \"\"\"\n        super().__init__()\n        self.n_channels = n_channels\n        # First linear layer\n        self.layers = nn.Sequential(\n            nn.Linear(self.n_channels // 4, self.n_channels),\n            nn.ReLU(True),\n            nn.Linear(self.n_channels, self.n_channels)\n        )\n\n    def forward(self, x):\n        # Create sinusoidal position embeddings\n        # [same as those from the transformer](../../transformers/positional_encoding.html)\n        #\n        # \\begin{align}\n        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n        # \\end{align}\n        #\n        # where $d$ is `half_dim`\n        t=torch.tensor(x.seq_dict['t']) if isinstance(x.seq_dict['t'],int) else x.seq_dict['t']\n        t=t.view(t.shape[0])\n        half_dim = self.n_channels // 8\n        emb = math.log(10_000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n\n        # Transform with the MLP\n        emb = self.layers(emb)\n        x.seq_dict['time']=emb\n        return x\n\n\nclass OnKey(nn.Module):\n    def __init__(self,k_in,module,k_out=None):\n        super().__init__()\n        if(k_out is None): k_out=k_in+'_out'\n        self.k_in=k_in\n        self.k_out=k_out\n        self.f=module\n    def forward(self, x):\n        x.seq_dict[self.k_out]=self.f(x.seq_dict[self.k_in])\n        return x\n\n\nclass Stack(nn.Module):\n    def __init__(self,key,f=lambda x:x):\n        super().__init__()\n        self.key,self.f=key,f\n    def forward(self,x):\n        if(self.key not in x.seq_dict): x.seq_dict[self.key]=[]\n        x.seq_dict[self.key]+=[self.f(x)]\n        return x\n\n\nclass Pop(nn.Module):\n    def __init__(self,key,f,clear=True,**kwargs):\n        super().__init__()\n        self.key,self.clear,self.f,self.kwargs=key,clear,f,kwargs\n    def forward(self, x): \n        o=x.seq_dict[self.key]\n        if(is_listy(o)): \n            o =  x.seq_dict[self.key].pop(-1) if(self.clear) else o[-1]\n        elif(self.clear): x.seq_dict[self.key]=None\n        return self.f(x,o,**self.kwargs)\n\n\ndef merge(x,o,dense=False): return torch.cat((x,o),dim=1) if(dense) else x+o.view(o.shape+(1,)*(x.ndim-o.ndim)) \n\n\nclass UnetTime(nn.Module):\n    \"A little Unet with time embeddings\"\n    def __init__(self,dims=[96, 192, 384, 768, 768],img_channels=3,ks=7,stem_stride=4,t_channels=128):\n        super().__init__()\n        i_d=0\n        h=dims[i_d]\n        self.time_emb=TimeEmbedding(t_channels)\n        # Not putting in for loop for ease of understanding arch\n        self.down=SequentialExDict(\n            nn.Conv2d(img_channels,h,ks,1,ks//2),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,stem_stride,1),\n            nn.GroupNorm(1,h),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n        )\n        self.middle=SequentialExDict(\n            self.basic_block(h,t_channels)\n        )\n        self.up=SequentialExDict(\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.down_sample(h*2,img_channels,5,1,2,bias=True),\n            self.basic_block(img_channels,t_channels,bias=True),\n        )\n        self.layers=SequentialExDict(\n            self.time_emb,\n            self.down,\n            self.middle,\n            self.up\n        )\n    @delegates(nn.Conv2d.__init__)\n    def up_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            Pop('s',lambda x,o:F.interpolate(x, size=[oi+1 for oi in o], mode='bilinear')),\n            self.down_sample(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    @delegates(nn.Conv2d.__init__)\n    def down_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            nn.GroupNorm(1,in_channels),\n            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    def basic_block(self,channels,time_channels,expansion=4,ks=7,stride=1,pad=None,bias=False):\n        if pad is None: pad=ks//2\n        return SequentialExDict(\n            Stack('r'),\n            nn.Conv2d(channels,channels,ks,padding=pad,bias=bias,stride=stride),\n            nn.GroupNorm(1,channels),\n            nn.Conv2d(channels,channels*expansion,1,bias=bias),\n            OnKey('time',nn.Linear(time_channels,channels*expansion)),\n            Pop('time_out',merge),\n            nn.GELU(),\n            nn.Conv2d(channels*expansion,channels,1,bias=bias),\n            Pop('r',merge),\n        )\n    def forward(self,x,x_o,t):\n        return self.layers(x,t=t)\n\nhello"
  },
  {
    "objectID": "posts/efficient-zero/index.html",
    "href": "posts/efficient-zero/index.html",
    "title": "blog",
    "section": "",
    "text": "MuZero\nLets define our alphabet soup: * h - calculates latent representation of state s from the past observation (board state, or previous frames) * s - state, latent representation of the environment * f - calculates p(policy) and v(value function) from s(state) * p - policy value for each action * v - value function, based on the reward. For atari n-step reward, final reward for board games. * a - some action, sampled from π/p when interacting with the environment, sampled from replay buffer during training. * g - calculates next s(state) and immediate reward(r), recieves previous state and an action as input * r - immediate reward * π - policy, approximately p\n\n[0.1,0.5,0.6,0.7,...]\n\nmuzero can learn rules of game, doesn’t need to be provided with rules.\n\nup,down,left,right\n1=(0.2,0.1,0.1,0.7)\n\n\n\n\nmu.png\n\n\nThe models are trained to predict p(policy),v(value function),r(immediate reward) from the REPLAY buffer.\n\n\nEfficientZero\n\nImprovement 1\n\nshow_image(Image.open('efficient_similarity.png'),figsize=[7,7])\n\n<AxesSubplot:>\n\n\n\n\n\nThe general idea for this one is from this paper: https://arxiv.org/abs/2011.10566\n\nThe “representation” is generated using h from before.\ng then creates s_(t+1) using s_t and a_t for “next state”\nThe “projector” and “predictor” seem to be both thrown away. The projector mapping s_t+1 to a lower deminsional space. This seems to be because we want the predictor to have very few parameters. (lower deminsional space just has less numbers)\nThe “predictor” seems to “bridge” the gap between both branches and allows for smaller batch size training and more stable training. “Exploring Simple Siamese Representation Learning” suggests that the predictor should model the Expected value of what was before the projector, including the difference between O_t and O_(t+1)\nSince the projector and representation is on both sides, and the predictor is sufficiently small, The hidden states for the two s_(t+1) must be similar.\n\n\n\nImprovement 2\nPredicting when a player will lose a point at a particular timestep is hard. Though it is much easier to predict “if” a player will lose a point.\n\n\n\nefficient_future.png\n\n\nPredicting the immediate reward(r) is hard, how do we know 20 steps ahead of time, exactly which time step we will get the reward? Instead they use a LSTM to predict the total reward up until the current time, and train to predict that value instead.\n\n\nImprovement 3\n\n\n\nefficient_replay.png\n\n\nTo use or not to use the replay buffer? - Green, MuZero uses the replayer buffer as is - Yellow, on more recent examples, Efficient Zero will use its policy to predict 1 state for training. - Blue, on less recent examples, Efficient Zero will us its policy for part of the future states. - Red, Efficient Zero can dream up the majority of the policy if observations in the replay buffer are very old.\n\n\nResults\nResults where inconsistent across different applications, so it may be better to think of this as a handful of techniques as opposed to something to always use together. More abalations in the other environments should help to determine what techniques generalize outside of atari and board games.\n\n\n\nefficient_inconsistent.png\n\n\n\n\n\nReferences\n\nMu zero\nEfficient Zero\nSimple Siamese Representations\nYanic Muzero\nYanic Efficient Zero\nImages are from their respective papers."
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nBackground\nFirst, a bit of review. Each \\(x_t\\) is dependent on some \\(x_{t-1}\\) in a Markov chain defined below. This we can also define in terms of \\(a_t\\) and \\(x_0\\), so that we can calculate the amount of noise at each step without the calculation being Markovian(or dependent on the previous step). This essentially gives us a formula that is the original image \\(x_0\\) plus some noise \\(\\epsilon\\)\n\\(q(x_t|x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\textbf{I})\\)\n\\(\\epsilon\\sim\\mathcal{N}(0,\\textbf{I})\\)\n\\(a_t := \\prod_{s=1}^t{1-\\beta_s}\\)\n\\(x_t = \\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon\\)\nBoth \\(\\alpha_t\\) and \\(\\beta_t\\) are of interest to us, so we define the code here. We have 1000 values, one value for each time step \\(t\\).\n\ndef at(Bt): return torch.cumprod(1-Bt,-1)\nBt=torch.linspace(1e-4,0.02,1000) #schedule used in DDPM paper\nat(Bt).shape\n\ntorch.Size([1000])\n\n\n\n\nSignal-to-Noise Ratio\nSignal-to-noise ratio is very important for this paper. We if we put the below two formulas close together the relationship should be clear, remembers that \\(x_0\\) is the original image, and \\(\\epsilon\\) is our noise.\n\\(x_t = \\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon\\)\n\\(SNR(t) = \\frac{\\alpha_t}{1-\\alpha_t}\\)\n\ndef snr(at): return at/(1-at)\n\nAnother noise schedule that is investigated is the cosine noise schedule, we define the code for it below.\n\ns=0.008 #from Improved Denoising Diffusion Probabilistic Models\ndef cos_sched(at): \n    at = torch.linspace(0,1,at.shape[0])\n    return torch.cos((at+s)/(1+s)*torch.pi/2)**2\n\nWe can now take a look at the noise schedule per diffusion step \\(t\\). Remeber low \\(\\alpha_t\\) means more noise \\(\\epsilon\\).\n\nplt.plot(at(Bt))\nplt.plot(cos_sched(Bt))\nplt.ylabel('Alphas a_t')\nplt.xlabel('Diffusion Steps (t)')\nplt.title('Noise Schedules')\nplt.legend(['Cosine Schedule','Linear Schedule'])\n\n<matplotlib.legend.Legend at 0x7fd7e2d69870>\n\n\n\n\n\nNow we can graph the signal to noise ratio for both the cosine and linear schedules.\n\nplt.yscale('log')\nplt.ylim(top=1e4,bottom=1e-8)\nplt.plot(snr(cos_sched(Bt)))\nplt.plot(snr(at(Bt)))\nplt.ylabel('Signal-to-Noise Ratio (SNR)')\nplt.xlabel('Diffusion Steps (t)')\nplt.title('Signal-to-Noise Ratio (SNR)')\nplt.legend(['Cosine Schedule','Linear Schedule'])\n\n<matplotlib.legend.Legend at 0x7fd8e86670a0>\n\n\n\n\n\nNotice that we get very similar results to the paper as seen below.\n\n\n\n161203299-8b02d76b-9c51-4529-8329-3ac08e9f3bc8.png\n\n\n\n\nContinuous Weights\nOkay, now we need to determine the weights above. This is the goal of the paper. First we look at the first contribution, a way to calculate the weights in terms of the signal-to-weight ratio, which is a continuous version of the weighting scheme introduced in the DDPM paper. The derivation of this is in the paper’s appendix.\n\ndef Ho_weights(Bt,at):\n    return (1-Bt)*(1-at)/Bt\ndef continuous_weights(at):\n    weights = -snr(at[1:])/(snr(at[1:])-snr(at[:-1]))\n    return torch.cat((weights[0:1],weights)) #we just make a copy of the first to get same shape\n\nWe can now compare the unnormalized weights of the continuous weight schedule and the one in the DDPM paper. They are fairly close.\n\nplt.plot(Ho_weights(Bt,at(Bt)))\nplt.plot(continuous_weights(at(Bt)))\nplt.ylabel('Weights(λ_t)')\nplt.xlabel('Diffusion Steps(t)')\nplt.title('Comparison of Ho and Continuous Weights')\nplt.legend(['Ho','Continuous'])\n\n<matplotlib.legend.Legend at 0x7fd7e2e0eb60>\n\n\n\n\n\n\n\nPrioritized Weight Schedule\nNext we can look at the prioritized weight schedule. The main contribution of the paper. \\(\\lambda_t\\) is our continuous weights from above, k is a constant set to \\(1\\). \\(\\gamma\\) is a hyperparameter that we can control, but it doesn’t work so well at over 2, because “We empirically observed that γ over 2 suffers noise artifacts in the sample because it assigns almost zero weight to the clean-up stage” (quoting paper).\n\\(\\lambda_t^\\prime = \\frac{\\lambda_t}{(k+SNR(t))^\\gamma}\\)\nAnd, here is it in code. \\(\\gamma=0\\) essentially turns the prioritized weighting mechanism off, and gives us the same result as the weighting mechanism in the DDPM paper.\n\nk=1. #set for nice math reasons\ndef prioritized_weights(l,t,g=0.):\n    return l/(k + snr(t))**g\n\nHere we go ahead and generate weights based on linear and cosine noise schedules for different values of \\(\\gamma\\). Notice how it is similar to the results in the image from the paper above.\n\n\n\nplt.xscale('log')\nplt.plot(snr(at(Bt)),F.normalize(continuous_weights(at(Bt)),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(at(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Linear Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb'])\n\n<matplotlib.legend.Legend at 0x7fd7e2a625c0>\n\n\n\n\n\n\n\nplt.xscale('log')\nplt.xlim(left=1e-8,right=1e4)\nplt.plot(snr(cos_sched(Bt)),F.normalize(continuous_weights(cos_sched(Bt)),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Cosine Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb'])\n\n<matplotlib.legend.Legend at 0x7fd7e2904d00>\n\n\n\n\n\n\n\n\n\nResults\nBelow you can see various results where the models performed better. Note, on the right, the this paper’s model is named P2. For the middle table the schedule makes the most difference when a model is missing attention, suggesting the weighting introduced helps with global features. For the images, notice that the samples generated have better clobal features, though both are going well at smaller details. The authors believe this is because the weights help the model focus more on global features.\n\n\n\npaper_results.png\n\n\n\n\nReferences\nhttps://arxiv.org/abs/2204.00227"
  },
  {
    "objectID": "posts/misc/Untitled.html",
    "href": "posts/misc/Untitled.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\ndown_sized= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(down_sized),down_sized.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg.transpose(2,0,1).shape,array([1/3,1/3,1/3]).shape\n\n((3, 320, 463), (3,))\n\n\n\ngray_scale= array([1/3,1/3,1/3])@img[...,None]\nshow_image(gray_scale,cmap='gray') #have to use cmap otherwise uses \"heatmap\" like coloring. \n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[img.shape[0]//4:-img.shape[0]//4,img.shape[1]//4:-img.shape[1]//4])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2:-1,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nimagenet_stats\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/misc/Untitled-Copy1.html",
    "href": "posts/misc/Untitled-Copy1.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\nSchedule: Question, This notebook, How to read a research paper, Presentation/Less formal\n\nnp.random.choice(np.arange(14),8,replace=False)\n\narray([2, 5, 1, 0, 9, 8, 6, 7])\n\n\n\nimg.shape\n\n(320, 463, 3)\n\n\n\nimg.shape[0]//2,img.shape[1]//2\n\n(160, 231)\n\n\n\nex=img[:img.shape[0]//2,:img.shape[1]//2]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nimg.shape[0]//4,-img.shape[0]//4\n\n(80, -80)\n\n\n\nex=img[img.shape[0]//4:-img.shape[0]//4,\n       img.shape[1]//4:-img.shape[1]//4]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nex.shape,img.shape\n\n((160, 232, 3), (320, 463, 3))\n\n\n\nshow_image(img)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlist(range(10,0,-1))\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\nshow_image(img[:-1:2,:-1:2]),img[:-1:2,:-1:2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nshow_image(img[1::2,1::2]),img[1::2,1::2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg[:-1:2,:-1:2]//2+img[1::2,1::2]//2\n\n\nimg[::2,::2]//2+img[1::2,1::2]//2\n\nValueError: operands could not be broadcast together with shapes (160,232,3) (160,231,3) \n\n\n\nimg[1::2,1::2].shape\n\n(160, 231, 3)\n\n\n\nex= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(ex),ex.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\narray([1/3,1/3,1/3])\n\narray([0.33333333, 0.33333333, 0.33333333])\n\n\n\n(array([1/3,1/3,1/3])@img[...,None]).shape\n\n(320, 463, 1)\n\n\n\nex1.max(),ex2.max()\n\n(765, 255.0)\n\n\n\nnp.\n\n\nex1= (array([1/2,1/2,1/2])@img[...,None]).clip(0,255)\nex2= (array([1/3,1/3,1/3])@img[...,None]).clip(0,255)\nshow_image(ex1,cmap='gray'),show_image(ex2,cmap='gray')\n\n(<AxesSubplot:>, <AxesSubplot:>)\n\n\n\n\n\n\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\n(As)**.5*norm_img\n\n\n(1-As**.5).squeeze()\n\ntensor([1.0000, 0.6985, 0.5736, 0.4778, 0.3970, 0.3258, 0.2615, 0.2023, 0.1472,\n        0.0955, 0.0465, 0.0000])\n\n\n\n((1-As)**.5).squeeze()\n\ntensor([1.0000, 0.9535, 0.9045, 0.8528, 0.7977, 0.7385, 0.6742, 0.6030, 0.5222,\n        0.4264, 0.3015, 0.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img+(1-As**.5)*noise)\n\n\n\n\n\nAs.squeeze(),As.shape\n\n(tensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n         0.8182, 0.9091, 1.0000]),\n torch.Size([12, 1, 1, 1]))\n\n\n\nnorm_img.shape\n\ntorch.Size([1, 3, 320, 463])\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\n1-(As)**.5\n\ntensor([[[[1.0000]]],\n\n\n        [[[0.6985]]],\n\n\n        [[[0.5736]]],\n\n\n        [[[0.4778]]],\n\n\n        [[[0.3970]]],\n\n\n        [[[0.3258]]],\n\n\n        [[[0.2615]]],\n\n\n        [[[0.2023]]],\n\n\n        [[[0.1472]]],\n\n\n        [[[0.0955]]],\n\n\n        [[[0.0465]]],\n\n\n        [[[0.0000]]]])\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "Prioritized Weight Schedule",
    "text": "Prioritized Weight Schedule\n\n\n\\(\\lambda_t^\\prime = \\frac{\\lambda_t}{(k+SNR(t))^\\gamma}\\)\n\n\nk=1. #set for nice math reasons\ndef prioritized_weights(l,t,g=0.):\n    return l/(k + snr(t))**g"
  }
]