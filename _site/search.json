[
  {
    "objectID": "posts/diffusion/Refactor.html",
    "href": "posts/diffusion/Refactor.html",
    "title": "DDPM",
    "section": "",
    "text": "We create two new Tensor Types, one for our noise, and one for our timestep.\n\nclass TensorNoise(TensorImageBase):pass\nclass TensorStep(TensorBase): pass\n\nWe would like normalize to denormalize our noise before showing it. This is so the noise in our image looks similar to the noise in our noised image.\n\n@Normalize\ndef decodes(self, x:TensorNoise):\n    f = to_cpu if x.device.type=='cpu' else noop\n    return (x*f(self.std) + f(self.mean))\n\n\nnorm = Normalize.from_stats(*imagenet_stats)\nshow_images(norm.decode(img))\n\n\n\n\nI patch ItemTransform here, so that it can work off of TypedTuples. Essentially if we have a DiffusionTuple, the transform will apply to that if it should apply to that type of tuple.\n\nclass ItemTransform(Transform):\n    \"A transform that always take tuples as items\"\n    _retain = True\n    # Only showing important code\n    def _call1(self:ItemTransform, x, name, **kwargs):\n        if not _is_tuple(x): return getattr(super(), name)(x, **kwargs)\n        y=self._call_tuple(name,x,**kwargs)\n        if not self._retain: return y\n        if is_listy(y) and not isinstance(y, tuple): y = tuple(y)\n        return retain_type(y, x)\n    def _call_tuple(self:ItemTransform, name, x, split_idx=None, **kwargs):\n        f = getattr(super(), name)\n        f2name='encodes' if name == '__call__' else 'decodes' if name == 'decode' else name\n        f2 = getattr(self, f2name)\n        if isinstance(f2,TypeDispatch) and f2[type(x)] is not None:\n            if split_idx!=self.split_idx and self.split_idx is not None: return x\n            y = f2(x, **kwargs)\n        else:\n            y = f(list(x), **kwargs)\n        return y\n\nThe general idea is to implement a named tuple, and use duck typing. In the future, we should look at the named tuple class and do something more similar to that.\n\nclass DiffusionTuple(fastuple):\n    def __new__(cls, *rest):\n        self=super().__new__(cls, *rest)\n        i=0\n        self.x=self[i]\n        if(isinstance(self[i+1],TensorImage)): self.x0=self[i:=i+1]\n        self.t=self[i:=i+1]\n        if(len(self)>i+1): self.y=self[i:=i+1]\n        if(len(self)>i+1): self.pred=self[i:=i+1]\n        if(len(self)>i+1): self.sampled_pred=self[i:=i+1]\n        return self\n\nA little transform to make our tuple a DiffusionTuple\n\nclass ToDiffusionTuple(ItemTransform):\n    order=100\n    def encodes(self,xy):\n        return DiffusionTuple(*xy[:-1],TensorNoise(xy[-1]))\n\nThis Transform expects y to contain an image, and just replaces it with noise. Our model tries to predict the noise in an image.\n\nclass LabelToNoise(ItemTransform):\n    order=101\n    def encodes(self,xy:DiffusionTuple):\n        y=xy.y\n        xy.y[:]=TensorNoise(torch.randn_like(y))\n        return xy\n\n\ndiff_tuple=LabelToNoise.encodes(DiffusionTuple(img[0].detach().clone(),TensorStep(torch.tensor([[200]])),TensorNoise(img[0].clone())))\n\nWe can access tuple elements by attributes. This is useful when you don’t know what index a particular value is located.\n\ndiff_tuple.x.shape,diff_tuple.t.shape,diff_tuple.y.shape\n\n(torch.Size([3, 320, 480]), torch.Size([1, 1]), torch.Size([3, 320, 480]))\n\n\nNow we have a way to create an image, and convert the label to noise.\n\nnorm.decode(diff_tuple).show(show_noise=True)\n\n<AxesSubplot:title={'center':'TensorStep([[200]])'}>\n\n\n\n\n\nNext, we need to go create a noised image, to pass to our model.\nFirst, how much noise to apply to each step?\n\nclass LinearNoiseSchedule:\n    \"Schedule like used in DDPM\"\n    def __init__(self,betas=None,n_steps=None,device='cuda'):\n        if betas is not None: self.n_steps=betas.shape[0]\n        if n_steps is None: self.n_steps=1000\n        if betas is None: self.betas = torch.linspace(0.0001, 0.02, self.n_steps,device=device)\n        self.alphas = 1. - self.betas\n        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n\nLets graph the various values here, in order to see what happens. Pay particularly close attention to alpha_bar as that controls the balance betwen our signal(image) and our noise.\n\nlns=LinearNoiseSchedule()\nplt.plot((lns.betas).cpu())\nplt.plot((lns.alphas).cpu())\nplt.plot((lns.alpha_bar).cpu())\nplt.legend(['betas', 'alphas','alpha_bar'])\n\n<matplotlib.legend.Legend at 0x7f89721ee710>\n\n\n\n\n\nNext is DDPM-style Q-sampling. This is pretty much used for all diffusion models, and is the process that takes us from and image to noise.\n\nclass DDPM_Q_Sampling():\n    def __init__(self,predicts_x=False,noise_schedule=LinearNoiseSchedule(),n_steps=1000,device='cuda'):\n        self.device=device\n        self.ns=noise_schedule\n        self.n_steps=n_steps\n        self.t_sched=torch.linspace(0,len(self.ns.alpha_bar)-1,n_steps,dtype=torch.long)[...,None,None,None]\n    def __call__(self,x,es,t):\n        t=self.t_sched[t]\n        a=self.ns.alpha_bar[t].to(device=x.device)\n        signal = (a ** .5)*x\n        noise = (1-a)**.5 * es\n        return signal + noise\n    def undo(self,z,es,t):\n        \"Goes back to the original image given noise. Only works if es is the original noise. If es is a TensorImage, assumes it is the original.\"\n        if(isinstance(es,TensorImage)): \n            return es \n        t=self.t_sched[t]\n        a=TensorBase(self.ns.alpha_bar[t].to(device=z.device))\n        noise=TensorBase((1-a)**.5 * es)\n        return TensorImage((z-noise)/(a ** .5))\n\n\ndiff_trans = DiffusionSamplingTransform(DDPM_Q_Sampling(),lambda x:x)\n\n\nnorm.decode(diff_trans(diff_tuple)).show()\n\n<AxesSubplot:title={'center':'TensorStep([[200]])'}>\n\n\n\n\n\nLets now test so make sure our noise is being generated correctly.\n\nnoise_tuple=LabelToNoise.encodes(DiffusionTuple(img[0].detach().clone(),TensorStep(torch.tensor([[999]])),TensorNoise(img[0].clone())))\n\n\nnorm.decode(diff_trans(noise_tuple)).show(show_noise=True)\n\n<AxesSubplot:title={'center':'TensorStep([[999]])'}>\n\n\n\n\n\nThese are not exactly the same as it is one noising step, but they are fairly close.\n\nis_close(norm.decode(diff_trans(noise_tuple))[0],TensorImage(norm.decode(diff_trans(noise_tuple))[2]),eps=1e-02)\n\nTensorImage(True, device='cuda:0')\n\n\n\nGoing from noise to and image, p_sampling\nP sampling is going from noise to image!\n\nclass Diffusion_P_Sampler():\n    def __init__(self,model,sampling_function):\n        self.device=sampling_function.device\n        self.model=model\n        self.sampling_function=sampling_function\n    # __call__ implemented, but not shown.\n    def iter_noise(self,x_t,ts,t_start):\n        i=0\n        while((ts>0).any()):\n            x,t=x_t[ts>0],ts[ts>0]\n            with autocast(device_type=self.device, dtype=x.dtype):\n                with torch.no_grad(): \n                    e = self.model(x,self.deconvert(t) if i!=0 else t_start)\n                x_t[ts>0]=self.sampling_function(x,e,t,t=t_start if i==0 else None)\n            ts[ts>0]-=1\n            i+=1\n            yield x_t\n\nNotice here that we generate noise as random numbers during our sampling process. This makes DDPM sampling a stocastic process.\n\n@patch\ndef __call__(self:DDPM_P_Sampling,x,es,ns_t,t=None):\n    t= self.t_sched[ns_t] if(t is None) else t[...,None,None,None]\n    n=torch.randn_like(x)\n    e,a,b=self._noise_at_t(es,t),self.ns.alphas[t],self.ns.betas[t]\n    signal = (x - e) / (a ** 0.5)\n    noise = b**.5 * n\n    return signal + noise\n@patch\ndef _noise_at_t(self:DDPM_P_Sampling,es,t):\n    eps_coef = (1 - self.ns.alphas[t]) / (1 - self.ns.alpha_bar[t]) ** .5 \n    return eps_coef* es\n\nWe implement DDIM sampling here, as it drastically reduces sampling time from 1000 steps to 50. DDIM is also a deterministic sampler, meaning we do not have random numbers generated as part of our sampling process. Just generally helps us keep our sanity when trying to show our results. https://arxiv.org/abs/2010.02502\n\n@patch\ndef __call__(self:DDIM_P_Sampling,z,es,ns_t,t=None):\n    if(t is None): t=self.t_sched[ns_t]\n    tp1=self.t_sched[ns_t-1]\n    a,a_tp1=self.ns.alpha_bar[t][...,None,None,None],self.ns.alpha_bar[tp1][...,None,None,None]\n    if self.predicts_x: \n        xs=es\n        es=(z - (a)**.5 * xs)/(1-a)**.5\n    else: xs=(z - (1-a)**.5 * es)/ (a ** .5)\n    signal = a_tp1**.5*(xs) \n    noise = (1-a_tp1)**.5*es\n    return signal + noise\n\n\n\nTraining a model\n\npath = untar_data(URLs.CIFAR)\n\n\nm=Unet(dim=192+192//8,channels=3,).cuda()\n\n\nbs=128\nn_steps=1000\ndiffusion_transform = DiffusionSamplingTransform(DDPM_Q_Sampling(),Diffusion_P_Sampler(m,DDIM_P_Sampling()))\ndls=DataBlock((ImageBlock(),\n               TransformBlock(type_tfms=[Transform(enc=lambda o: TensorStep(o))]),\n               ImageBlock()),\n          n_inp=2,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(*cifar_stats),ToDiffusionTuple,LabelToNoise,diffusion_transform),\n          get_items=get_image_files,\n          get_x=[lambda x:x,\n                 lambda x: torch.randint(1, n_steps, (1,), dtype=torch.long)],\n          splitter=IndexSplitter(range(bs)),\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n/home/molly/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/_tensor.py:1121: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  ret = func(*args, **kwargs)\n\n\n\n\n\n\ndef mse_loss_weighted(ys,targ):\n    return torch.mean(targ.w_sched[...,None] * ((ys - targ).flatten(start_dim=1) ** 2))\n\n\ndef snr(at): return at/(1-at)\n\nTaken from a paper called “Perception Prioritized Training of Diffusion Models, this is a continuous version of the weights. Which becomes our signal=to-noise ration, over the change in our signal to noise ratio.”https://arxiv.org/abs/2204.00227\n\ndef continuous_weights(at):\n    weights = -snr(at[1:])/(snr(at[1:])-snr(at[:-1]))\n    return torch.cat((weights[0:1],weights))\n\nThis is what the weights look like. Notice I clip the weights at 1.\n\nplt.plot(continuous_weights(LinearNoiseSchedule().alpha_bar).cpu().clip(min=1))\n\n\n\n\n\nclass WeightedLinSched(Callback):\n    def after_pred(self):\n        if(not hasattr(self,'ws')):\n            self.ws = continuous_weights(LinearNoiseSchedule().alpha_bar).clip(min=1)\n            self.ws /= self.ws.mean()\n        ts=self.learn.xb[1].flatten()\n        self.learn.yb[0].w_sched=self.ws[ts]\n\n\nlearn = Learner(dls,m,mse_loss_weighted,opt_func=Adam,cbs=[FlattenCallback,WeightedLinSched])\nlearn = learn.to_fp16()\nlearn.fit_flat_cos(10,lr=2e-4,wd=1e-4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.049204\n      0.042475\n      04:47\n    \n    \n      1\n      0.043398\n      0.044983\n      04:48\n    \n    \n      2\n      0.042191\n      0.034068\n      04:49\n    \n    \n      3\n      0.040477\n      0.038086\n      04:48\n    \n    \n      4\n      0.039220\n      0.038858\n      04:49\n    \n    \n      5\n      0.038735\n      0.032376\n      04:49\n    \n    \n      6\n      0.038864\n      0.029669\n      04:49\n    \n    \n      7\n      0.038684\n      0.041698\n      04:49\n    \n    \n      8\n      0.037839\n      0.031916\n      04:50\n    \n    \n      9\n      0.037525\n      0.041718\n      04:50\n    \n  \n\n\n\nnext check show_results\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.show_results(show_noise=True)\n\n\n\n\n\n\n\n\n\n\n\nchannels last, fused optimizers(foreach benjamin’s), jit model."
  },
  {
    "objectID": "posts/diffusion/index.html",
    "href": "posts/diffusion/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.vision.all import *\nfrom fastai.torch_basics import *\nfrom denoising_diffusion_pytorch import Unet\n\n\nimport wandb\nwandb.init(reinit=True)\nfrom fastai.callback.wandb import *\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: marii. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /home/molly/Projects/quatro-blog/posts/diffusion/wandb/run-20221006_011034-nhpww4wt\n\n\nSyncing run super-universe-63 to Weights & Biases (docs)\n\n\n\ndef gather(consts: torch.Tensor, t: torch.Tensor):\n    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n    c = consts.gather(-1, t)\n    return c.reshape(-1, 1, 1, 1)\n\n\nclass DenoiseDiffusion:\n    \"\"\"\n    ## Denoise Diffusion\n    \"\"\"\n\n    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n        \"\"\"\n        * `eps_model` is $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n        * `n_steps` is $t$\n        * `device` is the device to place constants on\n        \"\"\"\n        super().__init__()\n        self.eps_model = eps_model\n\n        # Create $\\beta_1, \\dots, \\beta_T$ linearly increasing variance schedule\n        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n\n        # $\\alpha_t = 1 - \\beta_t$\n        self.alpha = 1. - self.beta\n        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n        # $T$\n        self.n_steps = n_steps\n        # $\\sigma^2 = \\beta$\n        self.sigma2 = self.beta\n\n    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        #### Get $q(x_t|x_0)$ distribution\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # [gather](utils.html) $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n        mean = gather(self.alpha_bar, t) ** 0.5 * tensor(x0)\n        # $(1-\\bar\\alpha_t) \\mathbf{I}$\n        var = 1 - gather(self.alpha_bar, t)\n        #\n        return mean, var\n\n    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n        \"\"\"\n        #### Sample from $q(x_t|x_0)$\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        if eps is None:\n            eps = torch.randn_like(x0)\n\n        # get $q(x_t|x_0)$\n        mean, var = self.q_xt_x0(x0, t)\n        # Sample from $q(x_t|x_0)$\n        return mean + (var ** 0.5) * eps\n\n    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n        \\begin{align}\n        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n        \n        # NOTEDDDD REMOVED t\n        \n        eps_theta = self.eps_model(xt,t)\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n        # $\\alpha_t$\n        alpha = gather(self.alpha, t)\n        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n        # $\\sigma^2$\n        var = gather(self.sigma2, t)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        eps = torch.randn(xt.shape, device=xt.device)\n        # Sample\n        return mean + (var ** .5) * eps\n\n\nclass Q_sample(ItemTransform):\n    order=101\n    def __init__(self,diffusion):\n        self.diffusion=diffusion\n    def encodes(self,xy):\n        x=xy[0]\n        y=xy[-1]\n        ts = xy[2][:,0]#torch.randint(0, self.diffusion.n_steps, (x.shape[0],), device=x.device, dtype=torch.long)\n        x_type=type(x)\n        x=self.diffusion.q_sample(x, x_type(ts), eps=y)\n        return (x,*xy[1:-1],y)\n\n\nclass LabelToNoise(ItemTransform):\n    order=100\n    def encodes(self,xy):\n        y=xy[-1]\n        return (*xy[:-1],retain_type(torch.randn(y.shape,device=y.device),old=y))\n\n\ndef sample():\n    \"\"\"\n    ### Sample images\n    \"\"\"\n    with torch.no_grad():\n        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n        x = torch.randn([n_samples, image_channels, 32, 32],\n                        device=device)\n\n        # Remove noise for $T$ steps\n        for t_ in range(n_steps):\n            # $t$\n            t = n_steps - t_ - 1\n            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n            x = diffusion.p_sample(x, x.new_full((n_samples,), t, dtype=torch.long))\n        return x\n\n\nn_steps=1000\n\n\npath = untar_data(URLs.MNIST)\npath = untar_data(URLs.CIFAR)\n\n\nm=Unet(dim=32,channels=3)#UnetTime(img_channels=1,dims=[32, 64, 128, 256, 256],ks=3,stem_stride=2).cuda()\n\n\n@typedispatch\ndef show_batch(x:tuple, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(3*min(len(samples), max_n), nrows=nrows, ncols=3, figsize=figsize, title='Input/Original/Target')\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::3],range(max_n))]\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::3],range(max_n))]\n    ctxs[1::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::3],range(max_n))]\n    ctxs[2::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[2::3],range(max_n))]\n    return ctxs\n\n\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(cls=PILImageBW),\n               ImageBlock(cls=PILImageBW),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock(cls=PILImageBW)),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n).dataloaders(path,bs=128,val_bs=2*128)\ndls.show_batch()\n\nIndexError: list index out of range\n\n\n\npath.ls()\n\n(#3) [Path('/home/molly/data/cifar10/labels.txt'),Path('/home/molly/data/cifar10/test'),Path('/home/molly/data/cifar10/train')]\n\n\n\nbs=128\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(),\n               ImageBlock(),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock()),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=IndexSplitter(range(bs)),\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n\n\n\n\nclass FlattenCallback(Callback):\n    def before_batch(self):\n        self.learn.xb=(self.xb[0],self.xb[-1].view(self.xb[-1].shape[::2]),)\n\n\nlearn = Learner(dls,m,MSELossFlat(),opt_func=Lamb,cbs=[FlattenCallback,WandbCallback(log_preds_every_epoch=True)])\n\ninp=m.layers0:0 inp.seq_dict[‘t’]=torch.tensor([5]).cuda() m.layers1:4.shape\n\nlearn.fit_flat_cos(6,lr=1e-4,wd=0.)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.699252\n      0.648226\n      01:58\n    \n    \n      1\n      0.375051\n      0.354264\n      01:54\n    \n    \n      2\n      0.185453\n      0.168021\n      01:52\n    \n    \n      3\n      0.102715\n      0.080112\n      01:53\n    \n    \n      4\n      0.064207\n      0.043586\n      01:56\n    \n    \n      5\n      0.055305\n      0.053669\n      01:52\n    \n  \n\n\n\nWandbCallback was not able to get prediction samples -> Match length mismatch\n\n\n\n@typedispatch\ndef show_results(x:tuple, y:TensorImage, samples, outs, ctxs=None, max_n=10, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(6*min(len(samples), max_n), ncols=6, figsize=figsize, title='Input/Original/DenoisedImage/Target/Prediction/Diff')\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::6],range(max_n))]\n    ctxs[1::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::6],range(max_n))]\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::6],range(max_n))]\n    ctxs[2::6] = [(b-o).show(ctx=c, **kwargs) for b,o,c,_ in zip(samples.itemgot(0),outs.itemgot(0),ctxs[2::6],range(max_n))]\n    ctxs[3::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[3::6],range(max_n))]\n    ctxs[4::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs[4::6],range(max_n))]\n    ctxs[5::6] = [(b-targ).show(ctx=c, **kwargs) for b,targ,c,_ in zip(outs.itemgot(0),samples.itemgot(3),ctxs[5::6],range(max_n))]\n    return ctxs\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nn_samples=12\nimage_channels=3\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndevice=torch.device(0)\nxs = sample()\n\n\nshow_images((logit((xs.repeat(1,3,1,1)-xs.repeat(1,3,1,1).mean())/xs.repeat(1,3,1,1).std()).sigmoid()),nrows=4)\n\n\nshow_images((logit((xs-xs.mean())/xs.std()).sigmoid()),nrows=4)\n\n\n\n\n\nxs.min()\n\n\nshow_images(xs)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nwith learn.removed_cbs(WandbCallback):\n    show_images(dls.one_batch()[0][:4])\n\n\nlearn.show_results??\n\n\ndls.show_batch(show=False)[2]\n\n\n@typedispatch\ndef wandb_process(x:tuple, y, samples, outs, preds):\n    \"Process `sample` and `out` depending on the type of `x/y`\"\n    res_input, res_pred, res_label = [],[],[]\n    for s,o in zip(samples, outs):\n        img = s[0].permute(1,2,0)\n        res_input.append(wandb.Image(img, caption='Input_data'))\n        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground_Truth\", res_label)):\n            fig, ax = _make_plt(img)\n            # Superimpose label or prediction to input image\n            ax = img.show(ctx=ax)\n            ax = t.show(ctx=ax)\n            res.append(wandb.Image(fig, caption=capt))\n            plt.close(fig)\n    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground_Truth\":res_label}\n\n\nlearn.show_results()\n\n\n%debug"
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=1}\n:::"
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "Prioritized Weight Schedule",
    "text": "Prioritized Weight Schedule\nNext we can look at the prioritized weight schedule. The main contribution of the paper. \\(\\lambda_t\\) is our continuous weights from above, k is a constant set to \\(1\\). \\(\\gamma\\) is a hyperparameter that we can control, but it doesn’t work so well at over 2, because “We empirically observed that γ over 2 suffers noise artifacts in the sample because it assigns almost zero weight to the clean-up stage” (quoting paper).\n\n\n\\(\\lambda_t^\\prime = \\frac{\\lambda_t}{(k+SNR(t))^\\gamma}\\)\n\nAnd, here is it in code. \\(\\gamma=0\\) essentially turns the prioritized weighting mechanism off, and gives us the same result as the weighting mechanism in the DDPM paper.\n\nk=1. #set for nice math reasons\ndef prioritized_weights(l,t,g=0.):\n    return l/(k + snr(t))**g\n\n\n\nHere we go ahead and generate weights based on linear and cosine noise schedules for different values of \\(\\gamma\\). Notice how it is similar to the results in the image from the paper above.\n\n\n\nplt.xscale('log')\nplt.plot(snr(at(Bt)),F.normalize(continuous_weights(at(Bt)),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(at(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Linear Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb']);\n\n\n\n\n\n\nplt.xscale('log')\nplt.xlim(left=1e-8,right=1e4)\nplt.plot(snr(cos_sched(Bt)),F.normalize(continuous_weights(cos_sched(Bt)),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Cosine Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb']);"
  },
  {
    "objectID": "posts/misc/Backprop.html",
    "href": "posts/misc/Backprop.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\npath.ls()\n\n(#2) [Path('/home/molly/data/mnist_png/testing'),Path('/home/molly/data/mnist_png/training')]\n\n\n\ndls = DataBlock((ImageBlock(cls=PILImageBW),CategoryBlock),\n                splitter=GrandparentSplitter('training','testing'),\n                item_tfms=Resize(32),\n                get_y=parent_label,\n                get_items=get_image_files).dataloaders(path)\n\n\nxs,ys=dls.one_batch()\n\nFirst, what is a prediction? A prediction is some “guess” that ranges from 0-1, or 0-100%. We define that here as something that ranges from 0 to 1. (we leave out actual 0 for math reasons).\n\npreds=np.linspace(0.001,1.)\n\nLets look at log likelihood. \\(p(x)\\) here is our label, and can be thought of as either 1 or 0.\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{q(x)}} \\]\nLets graph this, notice near 0, or a completely wrong prediction the error explodes.\n\nF.nll_loss(pt,torch.ones(50).long(),reduction='none').shape\n\ntorch.Size([50])\n\n\n\npt = torch.linspace(0.001,1.,50)[None].repeat(50,1)\nplt.plot(torch.linspace(0.001,1.,50),F.nll_loss(pt,torch.ones(50).long(),reduction='none'))\n\n\n\n\n\nplt.plot(preds,-np.log(preds))\n\n\n\n\nA quick note, cross entropy loss is also the sum of entropy and the KL divergence, KL divergence you will probably see in some diffusion papers. So whenever you see people minimizing the KL divergence, it is a proxy to attempting to minimize the log likelihood as well.\n\\[ H(p,q) =  H(p)+D_{KL}(P\\parallel Q)\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{p(x)\\log{p(x)}}+\\sum_{x\\in X}{p(x)\\log{\\frac{p(x)}{q(x)}}}\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{p(x)\\log{p(x)}}+\\sum_{x\\in X}{p(x)\\log{p(x)}}-\\sum_{x\\in X}{p(x)\\log{q(x)}}\\]\n\\[p(x)=1\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{1\\log{1}}+\\sum_{x\\in X}{1\\log{1}}-\\sum_{x\\in X}{1\\log{q(x)}}\\]\n\\[ H(p,q) =  -\\sum_{x\\in X}{p(x)\\log{q(x)}}\\]\n\ndef entropy_plus_kl(p,q):\n    return p*np.log(p)+p*np.log(p/q)\n\n\nplt.plot(preds,-np.log(preds))\nplt.plot(preds,entropy_plus_kl(1,preds),linewidth=6,linestyle=':')\n\n\n\n\nOkay, now we know why we want to use cross-entropy. How do we generate values from 0 to 1? Well… lets use softmax.\n\\[ Softmax(x_i)=\\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\]\n\nxss=array([-2,-1,0,1,2])\nnp.exp(xss)/np.exp(xss).sum()\n\narray([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865])\n\n\n\nxss=array([-2,-1,0,1,2])\nnp.exp(xss),np.exp(xss)/np.exp(xss).sum()\n\n(array([0.13533528, 0.36787944, 1.        , 2.71828183, 7.3890561 ]),\n array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]))\n\n\n\nplt.plot(np.exp(np.linspace(0,10)))\n\n\n\n\n\nxss=array([100,0,0,0,0])\nnp.exp(xss[0])/np.exp(xss).sum()\n\n1.0\n\n\n\ndef relu(xss): return xss*(xss>0)\nrelu(xss)\n\narray([0, 0, 0, 1, 2])\n\n\n\n\n\n3\n\n\n\nsoftmax(xss).sum()\n\n1.0\n\n\n\ndef softmax(x): return np.exp(x)/np.exp(x).sum()\n\n\nplt.plot(np.linspace(-5,5),softmax(np.linspace(-5,5)))\n\n\n\n\nSoftmax can be applied to the entire real numberline, giving our model flexibility in its output. It pushes up the highest values to be a significant portion of the final probability.\nIt is probably important to pause here and look at what happens when crossentropy and softmax are combined.\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{q(x)}} \\]\n\\[ Softmax(x_i)=\\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{\\frac{e^{x_i}}{\\sum_j{e^{x_j}}}}} \\]\n\\[ \\log\\frac{x}{y} = \\log x-\\log y\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{e^{x_i}}}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j}})}}\\]\nNow, here comes the logsumexp trick. We can subtract a constant a from the exponent…\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^ae^{x_j-a}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(e^a\\sum_j{e^{x_j-a}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)a}+\\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j-a}})}}\\]\nor… in the case of \\(p(x)=1\\)\n\\[ H(p,q) = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\ndef cross_soft(x):\n    a=x.max()\n    return -x+a+torch.log(torch.exp(x-a).sum())\n\n\nplt.plot(torch.linspace(-2,2,50),cross_soft(torch.linspace(-2,2,50)))\nplt.plot(torch.linspace(-2,2,50),F.cross_entropy(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50),reduction='none'),linewidth=6,linestyle=':')\n\n\n\n\nNow, lets rewrite cross_soft so it can take a target, and take a mean.\n\ndef cross_soft2(x,targ,reduction='mean'):\n    x=x[range(targ.shape[0]),targ]\n    a=2\n    if(reduction=='mean'):\n        return (-x+a+torch.log(torch.exp(x-a).sum())).mean()\n    else:\n        return -x+a+torch.log(torch.exp(x-a).sum())\n\n\ncross_soft2(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))\n\ntensor(4.5290)\n\n\n\nF.cross_entropy(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50),)\n\ntensor(4.5290)\n\n\nOkay lets try a derivative …\n\\[ H(p,q) = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\ntorch.linspace(-2,2,50)[None].repeat(50,1)\n\ntensor([[-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        ...,\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000]])\n\n\n\\[ \\frac{H(p,q)}{dX} = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\\[ \\frac{dH(p,q)}{dx_k} = -1+ 0+\\frac{e^{x_k-a}}{\\sum_j{e^{x_j-a}}}\\]\nj does not equal i\n\\[ \\frac{dH(p,q)}{dx_k} = 0+ 0+\\frac{e^{x_k-a}}{\\sum_j{e^{x_j-a}}}\\]\n\\[ \\frac{dH(p,q)}{dx_k} = -y+softmax(x)\\]\n\ndef cross_soft2_grad(x,targ,reduction='mean'):\n    a=x.max()\n    return ((torch.exp(x-a)/(torch.exp(x-a).sum(1)))-F.one_hot(targ))/targ.shape[0]\n\n\ncross_soft2_grad(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))\n\ntensor([[-1.9971e-02,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05, -1.9968e-02,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05, -1.9966e-02,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        ...,\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ..., -1.8646e-02,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n         -1.8530e-02,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03, -1.8405e-02]])\n\n\n\nxs=torch.linspace(-2,2,50)[None].repeat(50,1).requires_grad_()\nloss=F.cross_entropy(xs,torch.arange(50),reduction='mean')\nloss.backward()\n((xs.grad-cross_soft2_grad(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))).abs()<0.00000001).all()\n\ntensor(True)\n\n\n\\[ y_{ik}=\\sum_j{x_{ij}w_{jk}+b_k} \\]\n\\[ \\frac{dY}{dX} =X\\frac{dW}{dX}+\\frac{dX}{dX}W^T + \\frac{dB}{dX}\\]\n\\[ \\frac{dY}{dX} =W^T \\]\n\\[ \\frac{dY}{dW} = X \\]\n\\[ \\frac{dY}{dB} =1 \\]\n\\[ y_{ik}=\\sum_j{w_{ij}x_{jk}} \\]\n\\[ \\frac{dy_{ik}}{dx_{ij}}=\\sum_j{0x_{jk}+w_{ij}\\frac{dx_{jk}}{dx_{jk}}} \\]"
  },
  {
    "objectID": "posts/misc/LSH with random projection.html",
    "href": "posts/misc/LSH with random projection.html",
    "title": "blog",
    "section": "",
    "text": "Problem setup\n\npath = untar_data(URLs.IMAGEWOOF_320)\n\n\nmodel = resnet50(pretrained=True).cuda()\n\n/home/molly/miniconda3/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/molly/miniconda3/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\ndls = DataBlock((ImageBlock,CategoryBlock),\n                splitter=IndexSplitter([0,2]),\n                item_tfms=Resize(320),\n                batch_tfms=aug_transforms(),\n                get_items=get_image_files).dataloaders(path)\n\n\ndef model_call(model,x):\n    #same as model.forward\n        x = model.conv1(x)\n        x = model.bn1(x)\n        x = model.relu(x)\n        x = model.maxpool(x)\n\n        x = model.layer1(x)\n        x = model.layer2(x)\n        x = model.layer3(x)\n        x = model.layer4(x)\n\n        #x = model.avgpool(x)\n        #x = torch.flatten(x, 1)\n        #x = model.fc(x)\n        return x\n\n\nmodel.fc\n\nLinear(in_features=2048, out_features=1000, bias=True)\n\n\n\n#model_call(model,dls.one_batch()[0])\n\nGoing to use a relatively small number of buckets, could use much more if we wanted to have less total collisions.\n\nnbits=8\nbuckets=2**nbits\nbits=torch.arange(0,8,device='cuda')\nnbits,buckets,2.**bits\n\n(8,\n 256,\n tensor([  1.,   2.,   4.,   8.,  16.,  32.,  64., 128.], device='cuda:0'))\n\n\n\nbits\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n\n\n\ntorch.sum(2**bits)\n\ntensor(255, device='cuda:0')\n\n\n\n\nHow would we calculate the maximum value that we can represent?\n\n[64, 2048, 10, 10]\n\n[64, 2048, 10, 10]\n\n\n\n[64, 8, 10, 10]\n\n[64, 8, 10, 10]\n\n\n\n(2.**bits).sum()\n\ntensor(255., device='cuda:0')\n\n\n\n#dividing by nbits gets us close to 1\nprojector=torch.randn(2048,nbits).cuda()/nbits**0.5\ntorch.linalg.norm(projector,dim=1)\n\ntensor([1.0337, 1.0348, 1.0620,  ..., 1.0139, 1.1947, 1.1155], device='cuda:0')\n\n\n\nnbits\n\n8\n\n\n\n[0,1,0,0,0], [0,0,1,0,0],[1,0,0,0,0]\n\n([0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0])\n\n\n\nb=dls.one_batch()[0]\n\n\ndls.show_batch()\n\n\n\n\n\nys=model_call(model,b)\n\n\n(ys[:,:,None]*projector[None,...,None,None]).shape\n\ntorch.Size([64, 2048, 8, 10, 10])\n\n\n\nys.shape,projector.shape\n\n(torch.Size([64, 2048, 10, 10]), torch.Size([2048, 8]))\n\n\n\ntorch.einsum('ijkl,jm->imkl',ys,projector).shape\n\ntorch.Size([64, 8, 10, 10])\n\n\n\n(torch.einsum('ijkl,jm->ijmkl',ys,projector)==(ys[:,:,None]*projector[None,...,None,None])).any()\n\nTensorImage(True, device='cuda:0')\n\n\n\n\nHow would you calculate the dot product for the second dim and first dim? (2048,10,10)@(2048,8)=(8,10,10)\n\nUsing Einstein notation?\n\nprojection=torch.einsum('bcxy,cz->bzxy',ys,projector)\n\n\nprojection\n\nTensorImage([[[[-6.2792e+00,  1.3167e-01, -9.4779e-02,  ..., -2.4656e+00,\n                -5.0584e+00, -1.3296e+00],\n               [ 8.1754e+00,  9.6036e+00, -4.4357e+00,  ..., -4.6396e+00,\n                -7.8996e+00, -2.8259e+00],\n               [ 4.4467e+00, -2.1758e+00, -6.7790e+00,  ...,  4.6684e+00,\n                 1.3650e+01,  1.0253e+01],\n               ...,\n               [ 6.3236e-01, -3.8342e+00, -1.3020e+01,  ..., -4.5836e+00,\n                -3.6719e+00,  4.8875e+00],\n               [-9.5392e+00, -4.3794e+00, -4.9069e+00,  ...,  4.2450e+00,\n                -7.0259e+00, -6.6205e+00],\n               [-2.8886e+00, -2.2777e+00, -3.5640e+00,  ..., -1.4655e+00,\n                -5.7932e+00, -2.0072e+00]],\n\n              [[ 9.2835e+00,  7.7462e+00,  6.1908e+00,  ...,  2.9067e+00,\n                 8.0073e+00,  1.1350e+01],\n               [ 1.1556e+01,  1.3565e+01,  9.3358e+00,  ...,  3.8952e+00,\n                 8.9265e+00,  1.1539e+01],\n               [ 9.3274e+00,  7.3811e+00,  1.1513e+01,  ...,  1.6263e+01,\n                 2.4063e+01,  1.9001e+01],\n               ...,\n               [ 6.8415e+00,  5.9298e+00,  7.7832e+00,  ...,  2.8651e+01,\n                 4.1376e+00, -4.5388e+00],\n               [ 5.3508e+00,  1.1588e+01,  1.2607e+01,  ...,  1.5089e+01,\n                 7.7975e+00,  5.8521e+00],\n               [ 9.5215e+00,  6.9118e+00,  1.0100e+01,  ...,  8.9737e+00,\n                 6.7271e+00,  8.5211e+00]],\n\n              [[-3.1688e+00, -6.4222e+00, -2.8567e+00,  ..., -3.6577e+00,\n                -5.9065e+00, -2.9208e+00],\n               [-6.0068e+00, -9.4032e+00, -2.8770e+00,  ..., -6.6112e-01,\n                -5.0611e+00, -8.4874e+00],\n               [-4.4515e+00, -1.0154e+01, -1.2417e+00,  ..., -1.5574e-01,\n                -1.1492e+00,  7.6063e+00],\n               ...,\n               [ 1.5988e+01,  1.3046e+01,  6.4920e+00,  ..., -4.4126e+00,\n                 1.4175e+00,  1.2447e+01],\n               [ 1.9409e+01,  1.7188e+01,  6.8644e+00,  ..., -3.7321e+00,\n                -2.8769e+00,  1.6388e+00],\n               [ 1.6584e+01,  1.4844e+01,  9.3730e+00,  ..., -8.5079e-01,\n                -6.0974e+00, -3.7285e+00]],\n\n              ...,\n\n              [[-7.1310e+00,  6.5308e-01,  2.6390e+00,  ..., -4.4481e+00,\n                -7.0668e+00, -9.1038e+00],\n               [ 2.2496e+00,  5.2074e+00,  1.0821e+01,  ..., -3.1807e+00,\n                 8.5318e-01, -1.1862e+00],\n               [-5.3933e+00,  3.0677e+00,  6.6602e+00,  ..., -4.9860e+00,\n                 1.2720e+00,  1.7180e+01],\n               ...,\n               [-3.8841e+00, -1.0031e+00,  1.4554e+01,  ...,  1.7657e+01,\n                 8.1946e+00,  8.0810e+00],\n               [-7.9001e+00, -9.9230e+00,  1.9062e+00,  ...,  2.0380e+00,\n                -1.1232e+00,  2.4761e+00],\n               [-1.1327e+01, -1.5734e+01, -1.2547e+01,  ..., -1.1905e+00,\n                 3.4819e+00,  1.7690e+00]],\n\n              [[-5.7783e+00,  1.5963e-01,  3.8980e+00,  ...,  1.9761e+00,\n                 1.8847e+00,  2.4780e-01],\n               [-1.7248e+00, -3.0383e+00, -6.9456e+00,  ...,  8.2464e+00,\n                -2.3109e+00, -2.6400e+00],\n               [ 3.1076e-01,  1.4344e+00, -1.1225e+01,  ...,  5.7751e+00,\n                 1.9494e+00,  3.9672e+00],\n               ...,\n               [-1.1729e+01, -1.0031e+01, -2.1186e+01,  ..., -7.7848e+00,\n                -1.1644e+01, -1.2071e+01],\n               [-1.5569e+01, -2.1363e+01, -2.6422e+01,  ..., -1.6087e+01,\n                -1.9381e+01, -2.5736e+01],\n               [-2.8877e+01, -2.3298e+01, -1.9927e+01,  ..., -1.1429e+01,\n                -1.3280e+01, -2.2741e+01]],\n\n              [[-9.2639e+00, -3.3929e+00,  9.7048e+00,  ..., -3.8852e+00,\n                 8.4170e-01,  1.7012e+00],\n               [-2.6492e+00,  3.2599e+00,  3.2146e+01,  ...,  2.9210e+00,\n                 2.3555e-01,  3.9248e-01],\n               [ 1.1340e+01,  1.2774e+01,  2.0421e+01,  ..., -7.9307e+00,\n                -1.6897e+01, -4.7462e+00],\n               ...,\n               [ 9.6164e+00,  1.3184e+01,  1.3542e+01,  ...,  7.6629e+00,\n                 1.3888e+01,  1.5293e+01],\n               [ 1.0481e+01,  1.4385e+01,  1.1060e+01,  ...,  4.2098e+00,\n                 1.1673e+01,  1.5041e+01],\n               [ 4.5687e+00,  1.4530e+01,  1.3015e+01,  ...,  9.6600e+00,\n                 1.2996e+01,  1.3536e+01]]],\n\n\n             [[[ 9.5124e+00,  7.0540e+00,  1.0731e+01,  ..., -1.1898e-01,\n                -4.8526e+00, -1.6273e+01],\n               [ 1.6990e-01, -1.3911e-01,  6.2190e+00,  ..., -6.3489e-01,\n                -1.0237e+01, -8.5378e+00],\n               [-3.7613e+00, -4.9234e+00, -2.4806e+00,  ..., -2.5140e+00,\n                -7.3274e+00, -7.1875e+00],\n               ...,\n               [-6.5095e+00,  1.7462e+00, -1.9302e-01,  ..., -9.5685e+00,\n                -7.6806e+00, -8.3060e-01],\n               [-9.2985e-01, -6.4332e+00,  6.1271e+00,  ..., -1.7758e+01,\n                -1.5903e+01, -1.8803e+01],\n               [ 9.8972e+00,  1.4579e+01,  8.9024e+00,  ..., -2.1343e+01,\n                -3.3487e+01, -2.7978e+01]],\n\n              [[ 9.7840e+00,  1.3461e+01,  2.7833e+01,  ...,  5.9922e+00,\n                 1.1757e+01,  2.2186e+01],\n               [ 6.5224e+00,  6.7404e+00,  1.3893e+01,  ...,  9.1900e+00,\n                 1.8095e+01,  1.9578e+01],\n               [ 8.7155e-01,  1.9546e+00,  8.3535e+00,  ...,  1.5843e+01,\n                 1.7649e+01,  1.1515e+01],\n               ...,\n               [-1.0384e+01, -1.4986e+01,  5.2173e+00,  ...,  3.0871e+01,\n                 4.3548e+01,  4.0713e+01],\n               [ 1.8919e+01,  1.2526e+01,  2.4128e+00,  ...,  2.9831e+01,\n                 4.2634e+01,  3.9587e+01],\n               [ 3.8178e+01,  3.2932e+01,  4.6763e+00,  ...,  3.2836e+01,\n                 3.7258e+01,  4.4807e+01]],\n\n              [[-6.7811e+00, -1.3298e+01, -2.0355e+00,  ..., -4.4833e+00,\n                -6.0731e+00, -5.2660e+00],\n               [-4.5190e+00, -4.7397e+00, -4.3055e+00,  ..., -8.3795e+00,\n                -8.6657e+00, -3.1813e+00],\n               [-4.0083e+00, -5.4050e+00, -8.3738e+00,  ..., -8.1771e+00,\n                -5.7712e+00,  3.3252e+00],\n               ...,\n               [-2.0961e+01, -1.7522e+01, -1.9436e+01,  ..., -3.9490e+00,\n                 3.5565e+00,  4.4478e+00],\n               [-2.4070e+01, -2.2667e+01, -7.1590e+00,  ..., -3.1348e+00,\n                 5.0762e+00,  1.2960e+01],\n               [-7.6899e+00, -8.6212e+00,  6.7536e-01,  ..., -2.6530e-01,\n                 1.1745e+01,  1.5273e+01]],\n\n              ...,\n\n              [[ 9.2374e-01,  3.2793e+00, -6.5183e+00,  ...,  4.7281e-01,\n                -2.2870e+00,  1.3027e+00],\n               [ 4.0763e+00,  3.0901e+00,  4.1876e+00,  ...,  2.0954e+00,\n                -9.8625e-01, -3.1275e+00],\n               [ 9.6910e-01,  2.8828e-01,  7.4595e-01,  ...,  1.0479e+01,\n                 8.2900e+00,  4.0798e+00],\n               ...,\n               [ 1.2553e+01,  1.4372e+01,  1.3448e+01,  ...,  1.2941e+01,\n                 7.6967e+00,  1.1426e+01],\n               [ 1.1466e+01,  1.4855e+01,  2.0851e+01,  ...,  3.2110e+01,\n                 4.5123e+01,  4.3206e+01],\n               [ 1.0600e+01,  7.3636e+00,  1.0523e+01,  ...,  1.6592e+01,\n                 2.7320e+01,  5.5086e+01]],\n\n              [[ 2.4515e+00,  1.0092e+01,  1.2336e+01,  ...,  6.2933e+00,\n                 4.9184e+00,  9.3574e+00],\n               [ 1.2444e+00,  3.0530e+00,  7.1421e+00,  ...,  5.0260e+00,\n                 6.4202e+00, -1.7965e+00],\n               [-2.0972e+00,  3.1371e-01,  8.9068e-01,  ...,  2.6291e+00,\n                 3.4665e+00, -1.9193e+00],\n               ...,\n               [ 1.5958e+00, -6.7022e+00, -5.6058e+00,  ...,  6.6774e+00,\n                 4.8079e+00,  3.4565e+00],\n               [-4.5736e-02, -1.8359e+01, -7.8809e+00,  ...,  6.7820e+00,\n                 3.7120e+00,  4.0987e+00],\n               [ 1.4360e+00, -5.5607e+00,  6.2634e-01,  ..., -1.5635e+00,\n                -1.0811e+01, -2.0711e+00]],\n\n              [[ 2.8068e+00,  3.3115e+00,  7.0982e+00,  ...,  2.5611e+00,\n                -1.5446e+00, -7.0554e+00],\n               [ 3.2848e+00, -2.6821e+00,  4.3762e+00,  ...,  1.6894e+00,\n                -1.6904e+00, -6.2212e+00],\n               [-4.2415e-01, -3.8746e+00, -4.9796e-01,  ..., -3.5530e-01,\n                 2.7174e+00,  1.3833e+01],\n               ...,\n               [ 6.1781e+00,  3.1638e+00,  7.9841e+00,  ..., -9.4526e+00,\n                -3.6841e+00, -1.4651e+01],\n               [-6.8839e+00, -3.6912e+00, -3.6544e+00,  ..., -1.6174e+01,\n                -2.0845e+01, -2.3711e+01],\n               [-3.4264e+00, -5.4144e+00, -1.1610e-01,  ..., -1.2922e+01,\n                -1.8294e+01, -2.1884e+01]]],\n\n\n             [[[ 1.6726e+01,  1.7428e+01,  4.2822e+00,  ...,  1.3308e+01,\n                 7.7680e+00,  1.4643e-01],\n               [ 1.0972e+01,  1.4134e+01,  4.4387e+00,  ...,  1.5983e+01,\n                 7.5606e+00,  2.1118e+00],\n               [ 8.8190e+00,  7.4110e+00,  1.1246e+01,  ...,  1.3696e+01,\n                 3.8566e+00,  6.2899e+00],\n               ...,\n               [ 1.2071e+01,  1.0601e+01,  1.6731e+00,  ..., -1.3564e+01,\n                -1.3250e+01,  3.3810e-01],\n               [ 1.8464e+00,  7.9721e+00,  3.5149e+00,  ..., -1.6235e+01,\n                -1.0623e+01,  8.7157e+00],\n               [ 8.7276e+00, -9.2056e-01,  7.5868e-01,  ..., -1.1893e+01,\n                 2.7289e+00,  2.7667e+01]],\n\n              [[ 6.6531e+00,  1.0010e+01,  2.2753e+01,  ...,  1.4925e+01,\n                 1.3476e+01,  1.6543e+01],\n               [ 4.5088e+00,  1.9536e+01,  3.0127e+01,  ...,  1.1684e+01,\n                 9.8105e+00,  3.2875e+00],\n               [ 4.2340e+00,  5.5834e+00,  1.8127e+01,  ..., -3.0256e+00,\n                 2.6275e+00, -7.1714e-01],\n               ...,\n               [ 1.6779e+01,  2.9478e+00,  1.5335e+01,  ...,  3.1999e+01,\n                 1.1919e+01,  8.4015e+00],\n               [ 1.5926e+01,  2.0441e+00,  7.7098e+00,  ...,  1.2695e+01,\n                 1.0315e+01,  5.9256e+00],\n               [ 4.5396e+00, -3.9510e+00,  3.0082e+00,  ...,  4.2423e+00,\n                 1.4620e+00, -8.1835e-01]],\n\n              [[-1.7676e+01, -1.7258e+01, -1.1684e+01,  ..., -1.0038e+01,\n                -7.8505e-02,  5.6030e-01],\n               [-1.2038e+01, -1.0079e+01,  3.9319e-01,  ..., -1.3313e+00,\n                -2.6005e+00,  6.5192e+00],\n               [-5.7970e+00, -1.0169e+01, -1.4580e+01,  ...,  3.7920e+00,\n                 2.1885e+00, -2.9092e+00],\n               ...,\n               [-9.1141e+00, -1.7944e+01, -2.5657e+00,  ..., -2.0504e+01,\n                -8.4545e+00,  3.7092e+00],\n               [-4.9958e+00, -9.3027e+00, -7.6127e+00,  ..., -1.3733e+01,\n                -7.3914e+00,  1.0248e+01],\n               [-2.4956e+00, -4.2638e+00, -3.6001e+00,  ..., -5.5006e-01,\n                 2.5708e+00,  5.7056e+00]],\n\n              ...,\n\n              [[ 4.2114e-01, -7.9980e+00, -8.6686e+00,  ..., -4.2628e+00,\n                 8.8063e+00, -9.2941e-02],\n               [-8.4785e+00, -1.7714e+01, -5.5977e-01,  ...,  9.8289e-01,\n                 1.2428e+01,  9.1162e+00],\n               [-9.5216e+00, -2.6300e+01, -1.7297e+01,  ...,  4.7637e+00,\n                 6.2845e+00,  4.5398e+00],\n               ...,\n               [ 1.2768e+00, -6.3151e+00, -1.0408e+00,  ...,  7.4808e-01,\n                -1.0534e+00,  4.4392e+00],\n               [ 3.0409e+00, -1.3620e+00,  3.0350e+00,  ..., -9.6885e+00,\n                -2.5018e+00, -3.8327e+00],\n               [-1.6695e+01, -1.1960e+01, -6.9895e+00,  ..., -8.2594e+00,\n                -1.4068e+01, -2.0667e+01]],\n\n              [[ 5.1652e+00,  1.0395e+01,  7.4535e+00,  ...,  8.6804e+00,\n                 4.5424e+00,  1.4589e+01],\n               [ 4.9551e+00,  1.8278e+01,  2.8135e+01,  ...,  2.0059e+01,\n                 1.2688e+01,  1.1655e+01],\n               [ 8.6485e+00,  2.4904e+01,  4.3337e+01,  ...,  1.4686e+01,\n                 1.6079e+01,  2.0526e+01],\n               ...,\n               [ 3.6101e+00, -3.5364e+00,  7.2125e+00,  ..., -8.2165e-01,\n                -7.4522e+00,  6.1776e+00],\n               [-3.6109e+00, -6.7193e+00,  1.2664e+01,  ..., -5.5758e+00,\n                -7.2992e+00,  3.7080e+00],\n               [-9.0229e-01, -2.1786e+00,  7.6342e+00,  ..., -7.0221e+00,\n                -9.6212e+00,  5.1720e+00]],\n\n              [[-1.1960e+01, -1.1723e+01, -2.1839e+01,  ..., -7.9723e+00,\n                 9.2299e+00,  5.2175e+00],\n               [-1.4355e+01, -2.1408e+01, -3.0463e+01,  ...,  3.5703e+00,\n                 4.7135e+00,  7.2011e+00],\n               [-1.3361e+01, -2.6255e+01, -3.8794e+01,  ...,  2.9431e+00,\n                -4.4089e+00,  1.5832e-01],\n               ...,\n               [-4.0332e+00, -1.0877e+01, -1.0693e+01,  ..., -7.4630e+00,\n                -1.3602e-01,  1.2742e+00],\n               [-3.7040e+00, -2.8553e+00, -8.7539e+00,  ...,  6.7736e-01,\n                 2.3848e+00, -1.8044e-01],\n               [-8.3037e+00, -1.0215e+01, -8.9308e+00,  ...,  1.6606e+01,\n                 1.4776e+00, -7.1054e+00]]],\n\n\n             ...,\n\n\n             [[[ 3.0742e+01,  2.0617e+01,  3.4883e+00,  ..., -9.0017e+00,\n                -1.6916e+00,  3.9473e-01],\n               [ 3.5029e+01,  2.8505e+01,  1.3211e+01,  ..., -5.5471e+00,\n                -6.9257e+00, -2.2475e+00],\n               [ 1.4213e+01,  2.3229e+01,  1.6051e+01,  ..., -1.8002e-01,\n                -3.7035e-01,  6.3750e+00],\n               ...,\n               [ 8.4374e-01,  4.3312e+00, -3.1439e-01,  ..., -4.4065e+00,\n                -1.1842e+00,  3.8927e+00],\n               [-1.3196e+00, -4.4678e-01, -2.1221e+00,  ..., -2.8409e+00,\n                -4.0868e+00,  2.5064e+00],\n               [-3.6652e-01, -9.4469e-01, -9.2601e+00,  ..., -3.7837e+00,\n                -1.7367e+00,  8.7911e+00]],\n\n              [[ 2.1170e+01,  1.9605e+01,  8.4797e+00,  ...,  1.7911e+01,\n                 8.9692e+00,  6.8814e+00],\n               [ 1.5301e+01,  1.3120e+01,  1.9084e+00,  ...,  1.5672e+01,\n                 1.0492e+01,  9.9011e+00],\n               [ 4.6515e+00, -1.0032e+00,  1.7698e+00,  ...,  9.0264e+00,\n                 7.3372e+00,  2.9377e+00],\n               ...,\n               [ 9.4733e+00,  1.7173e+01,  1.2714e+01,  ...,  2.1929e+01,\n                 1.6661e+01,  1.2190e+01],\n               [ 4.9162e+00,  1.5067e+01,  1.7427e+01,  ...,  1.6764e+01,\n                 1.1263e+01,  1.2861e+01],\n               [ 3.4370e+00,  1.0124e+01,  1.2676e+01,  ...,  2.0788e+01,\n                 1.6364e+01,  2.2770e+01]],\n\n              [[ 1.7073e+01,  1.2411e+01,  3.9940e+00,  ..., -2.5094e-01,\n                 1.0745e+00,  2.0860e+00],\n               [ 9.7372e+00,  2.9299e+00, -5.5394e+00,  ...,  1.9889e+00,\n                 3.3939e+00, -1.5617e+00],\n               [-2.5642e-01, -1.1352e+00, -8.8535e+00,  ..., -4.5452e+00,\n                -1.9397e+00, -7.2823e+00],\n               ...,\n               [-2.0316e+00, -2.9228e+00, -4.4708e+00,  ..., -3.9330e+00,\n                -1.1184e+01, -9.1596e+00],\n               [ 4.0511e-01, -5.1108e-01, -5.8130e+00,  ..., -3.4118e+00,\n                -4.2866e+00, -2.3001e+00],\n               [-3.2900e+00, -7.9968e+00, -4.4886e+00,  ..., -4.5846e+00,\n                -5.1580e+00, -5.1870e+00]],\n\n              ...,\n\n              [[-1.0441e+01,  3.6222e-01,  5.7083e+00,  ..., -8.7860e-01,\n                -5.9626e+00, -2.4280e+00],\n               [-3.2910e-01,  3.1008e+00,  1.1946e+01,  ...,  2.7110e+00,\n                -5.6768e+00, -6.1909e+00],\n               [-5.4021e+00,  1.0762e+01,  2.7438e+01,  ...,  5.8677e+00,\n                -1.0364e-01, -2.4942e+00],\n               ...,\n               [ 7.0567e-01, -3.1743e+00, -1.8512e+00,  ...,  4.5055e+00,\n                 1.3033e+00, -8.2645e+00],\n               [-3.0940e+00,  2.9549e+00,  1.0386e+01,  ..., -4.2258e+00,\n                -8.3496e+00, -6.9864e+00],\n               [-5.8167e+00,  5.5948e+00,  2.3419e+01,  ..., -6.6175e+00,\n                -1.2134e+01, -7.1784e+00]],\n\n              [[-1.5039e+01, -5.6994e+00,  6.0646e+00,  ...,  4.8761e+00,\n                -1.6647e+00, -3.3653e+00],\n               [-1.2232e+01,  3.2679e-01,  2.1025e+00,  ...,  4.5207e+00,\n                -6.3115e-01,  4.4318e-01],\n               [-1.6360e+01, -4.2183e-01, -2.9451e+00,  ..., -3.2819e+00,\n                -7.8153e+00, -3.9869e+00],\n               ...,\n               [-9.9416e+00,  1.5021e+01,  1.2003e+01,  ..., -5.2349e+00,\n                 2.6565e+00, -2.1975e+00],\n               [-1.3382e+01,  2.7150e+00,  7.4891e+00,  ..., -5.7495e+00,\n                 8.4802e+00,  6.6035e+00],\n               [-1.3810e+01,  7.6381e+00,  5.1958e+00,  ..., -1.8646e+00,\n                 3.0005e+00,  9.3771e-01]],\n\n              [[ 1.3810e+01,  1.7808e+01,  8.6061e+00,  ...,  1.1546e+01,\n                 8.8377e+00,  8.1417e+00],\n               [ 1.1785e+01,  1.7363e+01, -4.3577e+00,  ...,  3.6378e+00,\n                 1.6090e+01,  1.5553e+01],\n               [ 1.0965e+01,  2.3075e+00, -2.3853e+00,  ..., -8.7184e+00,\n                 1.7130e+01,  1.8968e+01],\n               ...,\n               [ 2.2361e+00,  2.7280e+00,  6.5013e+00,  ...,  5.6273e+00,\n                 8.9686e+00,  9.0201e+00],\n               [ 5.5273e+00,  3.4220e+00, -9.4277e+00,  ...,  8.6742e-01,\n                 7.3521e+00,  1.1186e+01],\n               [ 4.3832e+00, -6.7473e-01, -1.1554e+01,  ...,  6.7014e+00,\n                 8.0967e+00,  7.0652e+00]]],\n\n\n             [[[-2.6913e+01, -2.9568e+01, -8.9120e+00,  ..., -3.6777e+00,\n                -9.5993e+00, -1.3907e+01],\n               [-3.3114e+01, -4.0868e+01, -3.1856e+01,  ..., -2.1573e+00,\n                -7.5814e+00, -1.1474e+01],\n               [-2.7869e+01, -3.9983e+01, -2.6464e+01,  ...,  6.0545e+00,\n                -6.4322e+00, -6.3355e+00],\n               ...,\n               [-1.6259e+01, -1.7102e+01, -9.8119e+00,  ..., -2.6094e+00,\n                -9.1046e+00, -4.5698e+00],\n               [-4.2792e+00, -1.4512e+01, -7.5014e+00,  ..., -3.2948e-01,\n                -8.0459e+00, -5.2389e+00],\n               [-2.2665e-03, -1.0965e+01, -7.7879e+00,  ..., -9.6266e+00,\n                -1.0545e+01, -7.5697e-01]],\n\n              [[ 1.5671e+00,  1.2056e+01,  1.2069e+01,  ...,  2.4591e+01,\n                 3.2034e+01,  3.6939e+01],\n               [-3.8125e+00,  3.2298e+00,  1.2571e+01,  ...,  2.2087e+01,\n                 2.9005e+01,  1.8920e+01],\n               [ 7.9042e-01,  2.6120e+01,  3.5221e+01,  ...,  3.8826e+00,\n                 1.6314e+01,  2.7712e+01],\n               ...,\n               [-2.7849e+00, -1.0391e+01, -5.8262e+00,  ...,  6.0313e+00,\n                 6.1854e+00,  7.3942e+00],\n               [ 8.9106e+00,  6.6072e+00,  3.7478e+00,  ...,  4.4233e+00,\n                -1.8716e-01, -3.8388e-01],\n               [ 2.1626e+01,  1.3852e+01,  1.0201e+01,  ...,  3.2672e+00,\n                -1.1369e+00,  3.3530e+00]],\n\n              [[ 8.4911e+00,  8.4341e+00,  4.6288e+00,  ...,  1.3576e+01,\n                -7.2182e+00, -1.2758e+01],\n               [ 2.1120e+01,  1.2484e+01,  8.6090e-02,  ..., -1.1351e+00,\n                -1.4208e+01, -1.1975e+01],\n               [-3.1700e+00, -4.0954e+00, -1.6492e+00,  ..., -1.0604e+01,\n                -9.4700e+00,  6.6750e-01],\n               ...,\n               [ 2.9052e+00,  6.3958e+00,  1.3706e-01,  ...,  5.7390e+00,\n                 3.1581e+00, -6.0765e-01],\n               [-3.8417e+00, -4.2816e+00, -5.9917e-01,  ...,  6.7638e+00,\n                 5.0139e+00,  5.6882e+00],\n               [-1.8144e+01, -1.2465e+01, -4.2393e+00,  ...,  4.2922e+00,\n                 6.0590e+00,  9.7644e+00]],\n\n              ...,\n\n              [[-7.3576e+00,  1.0356e+01,  3.1724e-01,  ...,  1.8843e+01,\n                 1.8283e+01,  1.0810e+01],\n               [-7.3645e+00,  5.2899e+00,  1.3292e+00,  ...,  1.3994e+01,\n                 8.5479e+00, -4.9072e+00],\n               [-9.9293e+00,  2.7770e+00, -8.8405e+00,  ...,  3.1427e+00,\n                 1.9661e+00,  8.4572e+00],\n               ...,\n               [-8.4440e+00, -5.7747e+00, -7.7229e+00,  ...,  8.1245e+00,\n                -3.8143e+00, -9.9999e+00],\n               [ 6.4093e-01,  5.9969e+00, -6.3158e+00,  ...,  1.4593e+00,\n                -5.8362e+00, -1.3630e+01],\n               [ 4.7849e+00,  4.6974e+00,  3.1298e+00,  ..., -4.9041e+00,\n                -1.0216e+01, -1.2580e+01]],\n\n              [[-5.8014e+00, -1.0372e+01,  1.0254e+01,  ..., -4.6294e-01,\n                 1.8217e+01,  1.0038e+01],\n               [ 5.7797e+00,  9.2750e+00,  2.1096e+01,  ...,  8.8768e+00,\n                 2.2236e+01,  2.0851e+01],\n               [ 1.4236e+01,  3.3656e+01,  3.3081e+01,  ...,  2.2807e+01,\n                 3.4478e+01,  1.7018e+01],\n               ...,\n               [ 1.9373e+01,  1.7202e+01,  7.0146e-01,  ...,  9.4769e+00,\n                 5.3170e+00,  7.5832e+00],\n               [ 2.3346e+01,  2.6482e+01,  1.0779e+01,  ...,  1.0170e+00,\n                 9.9077e+00,  6.9110e+00],\n               [ 1.7564e+01,  2.1025e+01,  1.2639e+01,  ...,  4.6908e+00,\n                 5.8073e+00,  3.1757e+00]],\n\n              [[-5.2906e+00, -2.7127e+00,  1.5336e+01,  ..., -3.3235e+00,\n                 3.0431e+00,  4.4982e+00],\n               [-3.8740e+00, -2.7825e-01,  1.8444e+01,  ..., -6.8304e+00,\n                -9.7458e+00, -2.2539e+00],\n               [-2.3341e+00,  1.1108e+00,  9.1996e+00,  ...,  1.4541e+00,\n                 4.0044e+00,  5.3693e+00],\n               ...,\n               [-5.5670e+00, -3.2424e+00, -1.0724e+01,  ..., -2.3431e+00,\n                 3.1732e+00,  2.2250e+00],\n               [ 4.7924e+00,  2.9135e+00,  1.0695e+00,  ..., -3.1337e+00,\n                 3.6863e+00,  9.4232e+00],\n               [ 1.3064e+01,  1.4679e+00,  4.2512e+00,  ...,  2.0431e+00,\n                 5.5197e+00,  9.0792e+00]]],\n\n\n             [[[-4.1837e+00,  4.4228e+00,  7.5466e+00,  ...,  9.0294e+00,\n                 1.8985e+00,  1.3472e+00],\n               [ 3.4190e+00,  7.7751e+00,  1.4765e+01,  ...,  1.0512e+01,\n                 1.2134e+01,  1.3603e+01],\n               [ 1.1120e+01,  1.9633e+01,  1.5074e+01,  ...,  7.3059e+00,\n                 5.4364e+00,  1.3671e+01],\n               ...,\n               [ 7.7573e+00,  6.5959e+00,  2.4744e+00,  ...,  1.3797e+01,\n                 2.7515e+00,  2.0304e+00],\n               [ 5.0850e+00,  9.8228e+00,  7.2073e+00,  ...,  3.0873e-01,\n                -2.5682e+01, -1.1913e+01],\n               [-1.9177e+00,  1.0239e+00,  2.1603e+00,  ...,  7.6998e-02,\n                -1.0977e+01, -1.0928e+01]],\n\n              [[ 9.6061e+00,  1.5370e+00,  2.8735e+00,  ...,  5.0786e+00,\n                 1.1083e+01,  9.2034e+00],\n               [ 3.0860e+00,  7.1579e+00,  6.0781e+00,  ...,  7.9867e+00,\n                 3.1649e+00,  5.9685e+00],\n               [ 2.0483e+00,  1.4068e+01,  9.0135e+00,  ...,  4.9285e+00,\n                 6.5073e+00, -3.5669e+00],\n               ...,\n               [-1.0659e+00,  4.2513e+00,  3.0158e+00,  ...,  2.1836e+00,\n                -7.4681e+00, -1.0047e+01],\n               [-6.4479e+00, -1.6330e+00,  7.5435e+00,  ...,  1.3588e+01,\n                 1.1536e+00, -1.1312e+01],\n               [-4.2848e+00,  4.4101e+00,  9.2249e+00,  ...,  1.7328e+01,\n                 1.4831e+01, -7.3449e-01]],\n\n              [[-1.3682e+00, -5.1753e+00, -4.2238e+00,  ..., -1.3184e+01,\n                -1.0707e+01, -9.5371e+00],\n               [-1.6860e+00, -3.3730e+00,  3.3757e+00,  ..., -9.6586e+00,\n                -4.3802e+00, -4.1719e-02],\n               [-4.7046e-01, -4.2123e-01, -3.8035e+00,  ..., -9.6757e+00,\n                -1.5742e+01, -1.8032e+00],\n               ...,\n               [-4.2253e+00, -3.7856e+00, -4.7338e+00,  ...,  6.0688e+00,\n                -2.2821e+00,  6.7935e+00],\n               [-8.7673e+00, -2.8977e-01, -1.8588e+00,  ...,  7.1490e+00,\n                -4.2642e+00, -4.3403e-01],\n               [-1.3027e+01, -9.8119e+00, -4.1386e+00,  ..., -3.9124e+00,\n                -1.8475e+01, -5.6218e+00]],\n\n              ...,\n\n              [[-1.3048e+01, -6.4990e+00,  3.7771e+00,  ..., -3.5662e-01,\n                -2.1649e+01, -1.9933e+01],\n               [-1.3308e+01,  1.0137e+00,  4.0163e+00,  ...,  1.5178e+01,\n                -1.8908e+01, -2.6649e+01],\n               [-6.0838e+00,  4.4509e+00,  9.2461e+00,  ...,  1.2517e+01,\n                -6.9830e+00, -1.0261e+01],\n               ...,\n               [ 3.8149e+00,  1.0815e+01,  3.1839e+00,  ...,  9.5991e+00,\n                 3.8434e+00,  2.1540e+00],\n               [-2.1296e+00, -1.8112e+00, -2.8965e+00,  ..., -6.4432e+00,\n                -6.9710e+00, -5.0703e-01],\n               [ 7.2261e+00, -2.4904e+00, -6.8260e+00,  ..., -8.1620e+00,\n                -4.7305e+00, -1.3990e+00]],\n\n              [[ 6.4019e+00,  1.4235e+01,  1.1706e+01,  ...,  8.1696e+00,\n                 1.3590e+01,  1.1112e+01],\n               [ 9.0817e+00,  1.8450e+01,  1.8520e+00,  ...,  2.9886e+00,\n                 1.0353e+01,  1.9502e+01],\n               [ 1.2688e+01,  1.2391e+01,  7.0384e+00,  ..., -4.8993e+00,\n                 4.6831e+00,  1.3654e+01],\n               ...,\n               [ 6.8772e+00,  4.8467e+00,  3.6678e+00,  ..., -3.3305e+00,\n                 2.7749e+01,  3.7785e+01],\n               [ 1.4010e+01,  1.1408e+01,  5.3943e+00,  ..., -2.1537e+01,\n                 2.1972e+00,  3.4320e+01],\n               [ 1.4877e+01,  1.7063e+01,  6.9275e+00,  ..., -1.4477e+01,\n                -3.4017e+00,  3.0101e+01]],\n\n              [[-3.0831e+00,  3.0010e+00,  6.9832e+00,  ..., -1.8217e+00,\n                -7.1681e+00, -3.4423e+00],\n               [ 8.3796e+00,  3.6513e+00, -2.5611e+00,  ..., -6.8060e+00,\n                -1.4611e+01, -8.5084e+00],\n               [-6.5332e-01, -1.1911e+01, -1.2325e+01,  ..., -4.0894e+00,\n                -1.0735e+01,  1.7551e+00],\n               ...,\n               [ 3.4406e+00,  1.4224e+00, -3.6680e+00,  ..., -1.0623e+01,\n                -2.8591e+01, -1.4722e+01],\n               [ 3.2616e+00,  2.6747e+00, -1.9402e+00,  ..., -6.7384e+00,\n                -1.2895e+01, -1.5971e+01],\n               [ 1.2498e+01,  2.2431e+00, -3.8494e+00,  ..., -9.0071e+00,\n                -2.1719e+01, -1.8405e+01]]]], device='cuda:0',\n            grad_fn=<AliasBackward0>)\n\n\n\n2**bits, 0-255\n\n(tensor([  1,   2,   4,   8,  16,  32,  64, 128], device='cuda:0'), -255)\n\n\n\n(1*(projection>0)).shape,(2**bits[...,None,None]).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8, 1, 1]))\n\n\n\n\n\ntorch.Size([64, 10, 10])\n\n\n\nprojection.shape,(2.**bits).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8]))\n\n\n\n(1*(projection>0)).shape,(2**bits).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8]))\n\n\n\n2**bits,(1*(projection>0)).shape\n\n(tensor([  1,   2,   4,   8,  16,  32,  64, 128], device='cuda:0'),\n torch.Size([64, 8, 10, 10]))\n\n\n\n((1*(projection>0)[:,:,0,0])*(2**bits))\n\nTensorImage([[  0,   2,   0,   8,   0,   0,   0,   0],\n             [  1,   2,   0,   0,   0,  32,  64, 128],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   0,   0,   0,   0,   0],\n             [  0,   2,   4,   8,  16,   0,   0, 128],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   4,   0,   0,  32,  64, 128],\n             [  0,   2,   0,   0,   0,  32,   0,   0],\n             [  1,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  0,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,   0,  64,   0],\n             [  0,   2,   0,   0,   0,  32,  64, 128],\n             [  1,   2,   0,   0,   0,  32,   0,   0],\n             [  0,   2,   4,   8,   0,   0,   0,   0],\n             [  1,   2,   0,   0,   0,  32,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   4,   0,   0,  32,   0,   0],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  1,   2,   4,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   0,   0,   0,  64,   0],\n             [  1,   2,   4,   0,   0,  32,   0,   0],\n             [  0,   2,   0,   0,  16,   0,   0, 128],\n             [  1,   2,   0,   0,  16,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,  16,  32,   0,   0],\n             [  1,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,   0,   0,   0],\n             [  0,   2,   4,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  1,   2,   0,   0,   0,   0,   0, 128],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   8,   0,  32,  64,   0],\n             [  0,   0,   4,   8,  16,   0,  64, 128],\n             [  0,   0,   4,   8,  16,  32,   0, 128],\n             [  1,   0,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,  16,   0,   0, 128],\n             [  1,   2,   4,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,   0,  32,   0,   0],\n             [  0,   2,   4,   0,   0,   0,   0, 128],\n             [  1,   0,   0,   0,   0,  32,  64,   0],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  0,   2,   4,   0,  16,  32,   0, 128],\n             [  1,   2,   0,   0,  16,   0,  64, 128],\n             [  0,   0,   0,   0,  16,  32,  64,   0],\n             [  1,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   0,   0,   0,   0,  64,   0],\n             [  0,   2,   0,   8,   0,   0,  64,   0],\n             [  0,   2,   0,   0,   0,   0,   0, 128],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  1,   2,   0,   8,   0,   0,  64,   0],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   4,   8,  16,   0,  64,   0],\n             [  1,   2,   4,   8,  16,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,   0,   0],\n             [  0,   2,   0,   8,  16,   0,  64,   0]], device='cuda:0')\n\n\n\ntorch.einsum('ijkl,j->ikl',1.*(projection>0),(2.**bits))\n\nTensorImage([[[ 10.,  99., 226.,  ...,  66., 194., 194.],\n              [ 35., 163., 162.,  ..., 194., 162., 130.],\n              [211., 242., 178.,  ...,  67., 115., 119.],\n              ...,\n              [151., 134., 166.,  ..., 162., 166., 165.],\n              [134., 134., 166.,  ..., 163., 146., 182.],\n              [150., 150., 150.,  ..., 146., 178., 178.]],\n\n             [[227., 243., 211.,  ..., 226.,  66., 106.],\n              [227.,  98., 227.,  ..., 226.,  82.,  18.],\n              [ 34.,  98.,  98.,  ..., 114., 242., 166.],\n              ...,\n              [232., 169., 170.,  ...,  98., 102., 110.],\n              [ 42.,  42.,  43.,  ...,  98., 102., 102.],\n              [ 99.,  35., 111.,  ...,  42.,  38.,  38.]],\n\n             [[ 99.,  83.,  67.,  ...,  67., 227., 215.],\n              [ 67.,  67.,  87.,  ..., 227., 243., 231.],\n              [ 67.,  67.,  83.,  ..., 229., 119., 241.],\n              ...,\n              [ 99.,   3.,  67.,  ...,  34.,   2., 231.],\n              [ 35.,   3., 107.,  ..., 130., 130.,  71.],\n              [  3.,   8.,  75.,  ..., 130., 135.,  69.]],\n\n             ...,\n\n             [[159., 183., 231.,  ..., 194., 134., 151.],\n              [151., 247.,  99.,  ..., 246., 150., 210.],\n              [147., 161.,  35.,  ...,  34., 146., 147.],\n              ...,\n              [179., 195., 194.,  ..., 162., 226., 131.],\n              [150., 226.,  98.,  ..., 130., 194., 195.],\n              [146.,  98.,  98.,  ..., 130., 194., 195.]],\n\n             [[ 22.,  54., 246.,  ...,  38., 226., 226.],\n              [ 68., 102., 246.,  ...,  98.,  98.,  66.],\n              [ 82., 226., 210.,  ..., 227., 226., 230.],\n              ...,\n              [ 68.,  68.,  68.,  ..., 102., 198., 202.],\n              [226., 226., 194.,  ..., 102., 196., 204.],\n              [226., 226., 226.,  ..., 222., 212., 214.]],\n\n             [[ 90., 195., 227.,  ...,  91.,  83.,  83.],\n              [203., 235., 119.,  ..., 115.,  75.,  75.],\n              [ 75., 115., 115.,  ...,  51.,  83., 201.],\n              ...,\n              [249., 243., 115.,  ...,  63., 105., 109.],\n              [193., 193.,  91.,  ...,  31.,  90.,  64.],\n              [232., 203.,  91.,  ...,  27.,  18.,  64.]]], device='cuda:0')\n\n\n\n\nHow would we calculate the bucket for each x,y location?\n\nhash_loc=torch.einsum('bchw,c->bhw',(projection>0).float(),2.**bits).int()\nhash_loc.shape,hash_loc\n\n(torch.Size([64, 10, 10]),\n TensorImage([[[ 10,  99, 226,  ...,  66, 194, 194],\n               [ 35, 163, 162,  ..., 194, 162, 130],\n               [211, 242, 178,  ...,  67, 115, 119],\n               ...,\n               [151, 134, 166,  ..., 162, 166, 165],\n               [134, 134, 166,  ..., 163, 146, 182],\n               [150, 150, 150,  ..., 146, 178, 178]],\n \n              [[227, 243, 211,  ..., 226,  66, 106],\n               [227,  98, 227,  ..., 226,  82,  18],\n               [ 34,  98,  98,  ..., 114, 242, 166],\n               ...,\n               [232, 169, 170,  ...,  98, 102, 110],\n               [ 42,  42,  43,  ...,  98, 102, 102],\n               [ 99,  35, 111,  ...,  42,  38,  38]],\n \n              [[ 99,  83,  67,  ...,  67, 227, 215],\n               [ 67,  67,  87,  ..., 227, 243, 231],\n               [ 67,  67,  83,  ..., 229, 119, 241],\n               ...,\n               [ 99,   3,  67,  ...,  34,   2, 231],\n               [ 35,   3, 107,  ..., 130, 130,  71],\n               [  3,   8,  75,  ..., 130, 135,  69]],\n \n              ...,\n \n              [[159, 183, 231,  ..., 194, 134, 151],\n               [151, 247,  99,  ..., 246, 150, 210],\n               [147, 161,  35,  ...,  34, 146, 147],\n               ...,\n               [179, 195, 194,  ..., 162, 226, 131],\n               [150, 226,  98,  ..., 130, 194, 195],\n               [146,  98,  98,  ..., 130, 194, 195]],\n \n              [[ 22,  54, 246,  ...,  38, 226, 226],\n               [ 68, 102, 246,  ...,  98,  98,  66],\n               [ 82, 226, 210,  ..., 227, 226, 230],\n               ...,\n               [ 68,  68,  68,  ..., 102, 198, 202],\n               [226, 226, 194,  ..., 102, 196, 204],\n               [226, 226, 226,  ..., 222, 212, 214]],\n \n              [[ 90, 195, 227,  ...,  91,  83,  83],\n               [203, 235, 119,  ..., 115,  75,  75],\n               [ 75, 115, 115,  ...,  51,  83, 201],\n               ...,\n               [249, 243, 115,  ...,  63, 105, 109],\n               [193, 193,  91,  ...,  31,  90,  64],\n               [232, 203,  91,  ...,  27,  18,  64]]], device='cuda:0',\n             dtype=torch.int32))\n\n\n\nhash_loc.flatten().mode()\n\ntorch.return_types.mode(\nvalues=TensorImage(99, device='cuda:0', dtype=torch.int32),\nindices=TensorImage(1, device='cuda:0'))\n\n\n\nhash_loc==99\n\nTensorImage([[[False,  True, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [ True, False, False,  ..., False, False, False]],\n\n             [[ True, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [ True, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             ...,\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False,  True,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]]],\n            device='cuda:0')\n\n\n\n\nHow would we generate a mask for the locations that equal the mode(199)?\n\n(hash_loc==199).shape,(hash_loc==199)\n\n(torch.Size([64, 10, 10]),\n TensorImage([[[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ...,  True,  True,  True],\n               [False, False, False,  ...,  True, False, False]],\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]],\n \n              [[False, False, False,  ...,  True,  True,  True],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False,  True,  True],\n               [False, False, False,  ..., False, False,  True],\n               [False, False, False,  ...,  True, False, False]],\n \n              ...,\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ...,  True, False, False],\n               [ True, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]],\n \n              [[False, False, False,  ...,  True, False, False],\n               [False, False, False,  ...,  True, False, False],\n               [False, False, False,  ...,  True, False, False],\n               ...,\n               [False, False,  True,  ..., False, False, False],\n               [False, False,  True,  ..., False, False, False],\n               [False, False,  True,  ..., False, False, False]],\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]]],\n             device='cuda:0'))\n\n\n\nb.shape,hash_loc.shape\n\n(torch.Size([64, 3, 320, 320]), torch.Size([64, 10, 10]))\n\n\n\n%timeit b.view(64, 3, 10,320//10, 10,320//10)\n\n15.4 µs ± 2.08 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\ntiled_img=b.view(64, 3, 10,320//10, 10,320//10).permute(0,2,4,1,3,5)\nb.shape,tiled_img.shape\n\n(torch.Size([64, 3, 320, 320]), torch.Size([64, 10, 10, 3, 32, 32]))\n\n\n\nshow_images(b.view(64, 10, 10, 3, 32, 32)[0].flatten(end_dim=1),nrows=10)\n\n\n\n\n\nb.view(64, 3, 10,320//10, 10,320//10).shape\n\ntorch.Size([64, 3, 10, 32, 10, 32])\n\n\n\nshow_images(tiled_img[0].flatten(end_dim=1),nrows=10)\n\n\n\n\n\ntiled_img[1].shape,(hash_loc==99)[0].shape\n\n(torch.Size([10, 10, 3, 32, 32]), torch.Size([10, 10]))\n\n\n\ntiled_img[8][(hash_loc==99)[8]].shape\n\ntorch.Size([15, 3, 32, 32])\n\n\n\n\nHow would select select the patches of the image that fall into our mode bucket?\n\ntiled_img[1][hash_loc[1]==199].shape\n\ntorch.Size([11, 3, 32, 32])\n\n\n\nshow_image(b[0])\n\n<AxesSubplot:>\n\n\n\n\n\nSeems to detect “objects.” Other images included scales for weight things, and sea shells. Dog nose, might be due to that space being close.\n\nshow_images(tiled_img[0][hash_loc[0]==199][:64],nrows=4)\n\n\n\n\n\nshow_images(tiled_img[8][(hash_loc==99)[8]],nrows=4)\n\n\n\n\n\nshow_image(b[8])\n\n<AxesSubplot:>\n\n\n\n\n\n\nlen(dls.dataset)*10*10\n\n1295200\n\n\n\n1295200/256\n\n5059.375"
  },
  {
    "objectID": "posts/misc/Untitled-Copy1.html",
    "href": "posts/misc/Untitled-Copy1.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\nSchedule: Question, This notebook, How to read a research paper, Presentation/Less formal\n\nnp.random.choice(np.arange(14),8,replace=False)\n\narray([2, 5, 1, 0, 9, 8, 6, 7])\n\n\n\nimg.shape\n\n(320, 463, 3)\n\n\n\nimg.shape[0]//2,img.shape[1]//2\n\n(160, 231)\n\n\n\nex=img[:img.shape[0]//2,:img.shape[1]//2]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nimg.shape[0]//4,-img.shape[0]//4\n\n(80, -80)\n\n\n\nex=img[img.shape[0]//4:-img.shape[0]//4,\n       img.shape[1]//4:-img.shape[1]//4]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nex.shape,img.shape\n\n((160, 232, 3), (320, 463, 3))\n\n\n\nshow_image(img)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlist(range(10,0,-1))\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\nshow_image(img[:-1:2,:-1:2]),img[:-1:2,:-1:2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nshow_image(img[1::2,1::2]),img[1::2,1::2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg[:-1:2,:-1:2]//2+img[1::2,1::2]//2\n\n\nimg[::2,::2]//2+img[1::2,1::2]//2\n\nValueError: operands could not be broadcast together with shapes (160,232,3) (160,231,3) \n\n\n\nimg[1::2,1::2].shape\n\n(160, 231, 3)\n\n\n\nex= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(ex),ex.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\narray([1/3,1/3,1/3])\n\narray([0.33333333, 0.33333333, 0.33333333])\n\n\n\n(array([1/3,1/3,1/3])@img[...,None]).shape\n\n(320, 463, 1)\n\n\n\nex1.max(),ex2.max()\n\n(765, 255.0)\n\n\n\nnp.\n\n\nex1= (array([1/2,1/2,1/2])@img[...,None]).clip(0,255)\nex2= (array([1/3,1/3,1/3])@img[...,None]).clip(0,255)\nshow_image(ex1,cmap='gray'),show_image(ex2,cmap='gray')\n\n(<AxesSubplot:>, <AxesSubplot:>)\n\n\n\n\n\n\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\n(As)**.5*norm_img\n\n\n(1-As**.5).squeeze()\n\ntensor([1.0000, 0.6985, 0.5736, 0.4778, 0.3970, 0.3258, 0.2615, 0.2023, 0.1472,\n        0.0955, 0.0465, 0.0000])\n\n\n\n((1-As)**.5).squeeze()\n\ntensor([1.0000, 0.9535, 0.9045, 0.8528, 0.7977, 0.7385, 0.6742, 0.6030, 0.5222,\n        0.4264, 0.3015, 0.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img+(1-As**.5)*noise)\n\n\n\n\n\nAs.squeeze(),As.shape\n\n(tensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n         0.8182, 0.9091, 1.0000]),\n torch.Size([12, 1, 1, 1]))\n\n\n\nnorm_img.shape\n\ntorch.Size([1, 3, 320, 463])\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\n1-(As)**.5\n\ntensor([[[[1.0000]]],\n\n\n        [[[0.6985]]],\n\n\n        [[[0.5736]]],\n\n\n        [[[0.4778]]],\n\n\n        [[[0.3970]]],\n\n\n        [[[0.3258]]],\n\n\n        [[[0.2615]]],\n\n\n        [[[0.2023]]],\n\n\n        [[[0.1472]]],\n\n\n        [[[0.0955]]],\n\n\n        [[[0.0465]]],\n\n\n        [[[0.0000]]]])\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/misc/Untitled.html",
    "href": "posts/misc/Untitled.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\ndown_sized= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(down_sized),down_sized.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg.transpose(2,0,1).shape,array([1/3,1/3,1/3]).shape\n\n((3, 320, 463), (3,))\n\n\n\ngray_scale= array([1/3,1/3,1/3])@img[...,None]\nshow_image(gray_scale,cmap='gray') #have to use cmap otherwise uses \"heatmap\" like coloring. \n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[img.shape[0]//4:-img.shape[0]//4,img.shape[1]//4:-img.shape[1]//4])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2:-1,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nimagenet_stats\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/misc/image_augs.html",
    "href": "posts/misc/image_augs.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = Image.open(path.ls()[0].ls()[3].ls()[0])\nimg = TensorImage(image2tensor(img)[None]/255.)\n\n\naug_transforms(max_lighting=.99)[1]\n\nBrightness -- {'max_lighting': 0.99, 'p': 1.0, 'draw': None, 'batch': False}:\nencodes: (TensorImage,object) -> encodes\ndecodes: \n\n\n\nshow_images(aug_transforms(max_lighting=.99)[1](img.repeat(12,1,1,1)))\n\n\n\n\n\nRandTransform\nNotice that p, can be set to control the probability of a transform being applied.\n\nclass RandTransform(DisplayedTransform):\n    \"A transform that before_call its state at each `__call__`\"\n    do,nm,supports,split_idx = True,None,[],0\n    def __init__(self, \n        p:float=1., # Probability of applying Transform\n        nm:str=None,\n        before_call:callable=None, # Optional batchwise preprocessing function\n        **kwargs\n    ):\n        store_attr('p')\n        super().__init__(**kwargs)\n        self.before_call = ifnone(before_call,self.before_call)\n\n    def before_call(self, \n        b, \n        split_idx:int, # Index of the train/valid dataset\n    ):\n        \"This function can be overridden. Set `self.do` based on `self.p`\"\n        self.do = self.p==1. or random.random() < self.p\n\n    def __call__(self, \n        b, \n        split_idx:int=None, # Index of the train/valid dataset\n        **kwargs\n    ):\n        self.before_call(b, split_idx=split_idx)\n        return super().__call__(b, split_idx=split_idx, **kwargs) if self.do else b\n\n\n\nAffine\nFastai has many affine transforms. These include crop, zoom, flip etc. Lets go through some now.\n\nxy_grid.shape\n\ntorch.Size([1, 320, 480, 2])\n\n\n\ndef show_grid(xy_grid):\n    neutral_dim=torch.zeros_like(xy_grid)[...,0,None]\n    normal_grid=torch.cat((xy_grid,neutral_dim),dim=3)\n    bad_mask = (normal_grid>1).int() + (normal_grid<-1).int()\n    bad_mask=-bad_mask.sum(-1)*10\n    normal_grid+=bad_mask[...,None]\n    show_images(((normal_grid+1)/2).clip(0,1))\n\n\ny_coords=torch.linspace(-1,1,img.shape[-2])\nx_coords=torch.linspace(-1,1,img.shape[-1])\nxy_grid=torch.meshgrid(x_coords, y_coords, indexing='xy')\nxy_grid=torch.stack(xy_grid,dim=2)[None]\nshow_grid(xy_grid)\n\n\n\n\n\nxy_grid\n\ntensor([[[[-1.0000, -1.0000],\n          [-0.9958, -1.0000],\n          [-0.9916, -1.0000],\n          ...,\n          [ 0.9916, -1.0000],\n          [ 0.9958, -1.0000],\n          [ 1.0000, -1.0000]],\n\n         [[-1.0000, -0.9937],\n          [-0.9958, -0.9937],\n          [-0.9916, -0.9937],\n          ...,\n          [ 0.9916, -0.9937],\n          [ 0.9958, -0.9937],\n          [ 1.0000, -0.9937]],\n\n         [[-1.0000, -0.9875],\n          [-0.9958, -0.9875],\n          [-0.9916, -0.9875],\n          ...,\n          [ 0.9916, -0.9875],\n          [ 0.9958, -0.9875],\n          [ 1.0000, -0.9875]],\n\n         ...,\n\n         [[-1.0000,  0.9875],\n          [-0.9958,  0.9875],\n          [-0.9916,  0.9875],\n          ...,\n          [ 0.9916,  0.9875],\n          [ 0.9958,  0.9875],\n          [ 1.0000,  0.9875]],\n\n         [[-1.0000,  0.9937],\n          [-0.9958,  0.9937],\n          [-0.9916,  0.9937],\n          ...,\n          [ 0.9916,  0.9937],\n          [ 0.9958,  0.9937],\n          [ 1.0000,  0.9937]],\n\n         [[-1.0000,  1.0000],\n          [-0.9958,  1.0000],\n          [-0.9916,  1.0000],\n          ...,\n          [ 0.9916,  1.0000],\n          [ 0.9958,  1.0000],\n          [ 1.0000,  1.0000]]]])\n\n\n\nshow_images(F.grid_sample(img,xy_grid))\n\n\n\n\n\ndef make_grid(x_coords,y_coords):\n    xy_grid=torch.meshgrid(x_coords, y_coords, indexing='xy')\n    xy_grid=torch.stack(xy_grid,dim=2)[None]\n    return xy_grid\n\n\ny_coords=torch.linspace(-1,1,img.shape[-2])\nx_coords=torch.linspace(-1,1,img.shape[-1])\nmake_grid(y_coords)\nshow_grid(xy_grid)\n\n\nSlide Left\n\nshow_grid(make_grid(x_coords+1,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(-1*x_coords,y_coords)))\n\n\n\n\n\n\nFlip\n\nshow_grid(make_grid(-1*x_coords,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(2.*x_coords,y_coords)))\n\n\n\n\n\n\nSquish/Resize x dim\n\nshow_grid(make_grid(2*x_coords,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(2*x_coords,y_coords)))\n\n\n\n\n\ny_coords.shape\n\ntorch.Size([320])\n\n\n\nx_coords.shape\n\ntorch.Size([480])\n\n\n\n\nWhy can’t we use the technique we have used to implement rotate/warp?\nI mostly introduced the previous techniques to make things easy to understand by making x and y indepentdent, but affine transformations can work off of the current x and y values, which takes a bit more code to implement. Lets jump straight into building these like fastai.\n\n\nF.affine_grid\nAffine grids work on much smaller grids.\n\ntranslate_grid=torch.tensor([[1.,0,.5],\n              [0,1,0]])\n\n\ncoords_grid=F.affine_grid(translate_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\nhttps://en.wikipedia.org/wiki/Affine_transformation\n\ntorch.tensor([[1,0,0],\n              [0,1,0]])\n\ntensor([[1, 0, 0],\n        [0, 1, 0]])\n\n\n\ntorch.tensor([[1.,0,0],\n              [0,1,0]])\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.]])\n\n\n\nidentity_grid=torch.tensor([[1.,0,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(identity_grid[None], img.shape)\nshow_grid(coords_grid)\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\n\n\nrot_grid=torch.tensor([[math.cos(.5),-math.sin(.5),0],\n              [math.sin(.5),math.cos(.5),0]])\ncoords_grid=F.affine_grid(rot_grid[None], img.shape)\nshow_grid(coords_grid)\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\n\n\n\n\nRotate\n\nmath.cos(.5)\n\n0.8775825618903728\n\n\n\nrotate_grid=torch.tensor([[math.cos(1),math.sin(1),0],\n              [-math.sin(1),math.cos(1),0]])\ncoords_grid=F.affine_grid(rotate_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\nshear_grid=torch.tensor([[1,.5,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(shear_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\nHow would we warp/skew?\n\nwarp_grid=torch.tensor([[1.,.5,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(warp_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\nCombining affine augmentations\nLets look at the affine grid identity.\n\naffine_grid=torch.tensor([[1.,0,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(affine_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\nDoes this affine grid identity look familiar? Can you think of a way to combine affine transforms?\n\n\nImplementation\n\ndef combine_affines(affines):\n    id_row=lambda a:torch.cat((a,torch.tensor([.0,0,1])[None]))\n    comb_mat=id_row(affines[0])\n    for a in affines:\n        comb_mat@=id_row(a)\n    return comb_mat[:2]\n\n\nwrt_grid=combine_affines([warp_grid,rotate_grid,translate_grid])\n\n\nwrt_grid\n\ntensor([[-0.3012,  1.3818, -0.3012],\n        [-0.8415,  0.5403, -0.8415]])\n\n\n\ncoords_grid=F.affine_grid(wrt_grid[None], img.shape)\n\n\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid)),show_images(img)\n\n(None, None)\n\n\n\n\n\n\n\n\n\n_BrightnessLogit??\n\nObject `_BrightnessLogit` not found.\n\n\n\n\n\nLighting\n\nshow_images((img+.4))\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nx=TensorImage(torch.tensor([.01* i for i in range(0,101)]))\nf_lin= lambda x:(2*(x-0.5)+0.5).clamp(0,1) #blue line\nf_log= lambda x:2*x #red line\nplt.plot(x,f_lin(x),'b',x,torch.sigmoid(f_log(logit(x))),'r');\n\n\n\n\nWhat is special about logit in relationship to sigmoid?\n\nshow_images(img+.5)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nshow_images(torch.sigmoid(logit(img)+logit(torch.tensor(.85))))\n\n\n\n\n\nshow_images(img)\n\n\n\n\n\nlogit??\n\n\nHow to do contrast?\n\nshow_images(torch.sigmoid(logit(img)*4))\n\n\n\n\n\n\n\nNext Section\nOpen other notebook"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/adan-optimizer/index.html",
    "href": "posts/adan-optimizer/index.html",
    "title": "blog",
    "section": "",
    "text": "def update_prev_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n    \"Keeps track of the previous gradient, should be one of last cbs. \"\n    return {'prev_grad': p.grad.data}\n\n\ndef n_avg_grad(p,lr,nmom=None,n_avg=None,prev_grad=None,**kwags):\n    if n_avg is None: \n        prev_grad=torch.zeros_like(p.grad.data)\n        n_avg = p.grad.data-prev_grad\n    else:\n        n_avg = (1-nmom)*n_avg+nmom*(p.grad.data-prev_grad)\n    return {'n_avg': n_avg,'prev_grad':prev_grad}\n\n\ndef n_average_sqr_grad(p,nmom,sqr_mom, prev_grad=None, dampening=True, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    damp = 1-sqr_mom if dampening else 1.\n    grad = (2-nmom)*p.grad.data+(nmom-1)*prev_grad\n    sqr_avg.mul_(sqr_mom).addcmul_(grad,grad, value=damp)\n    return {'sqr_avg': sqr_avg}\n\n\ndef adan_step(p,lr,grad_avg=None,nmom=None,n_avg=None,sqr_avg=None,\n             eps=None,**kwargs):\n    p.data.addcdiv_(grad_avg+(1-nmom)*n_avg, \n                    (sqr_avg).sqrt() + eps, \n                    value = -lr)\n\n\ndef Adan(params, lr, mom=0.9, sqr_mom=0.99,nmom=0.9, eps=1e-5, wd=0.01, decouple_wd=True):\n    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n    cbs = [weight_decay] if decouple_wd else [l2_reg]\n    cbs += [partial(average_grad, dampening=True),n_avg_grad, n_average_sqr_grad,adan_step, update_prev_grad]\n    return Optimizer(params, cbs, lr=lr,nmom=nmom, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n\n\nl=nn.Linear(4,4)\nopt=Adan(l.parameters(),0.01)\nprint(l.weight)\ninp=torch.tensor([.1,.2,.3,.4])\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\n\nParameter containing:\ntensor([[ 0.4984,  0.2108,  0.3309, -0.1065],\n        [-0.4451,  0.3669, -0.2573,  0.1675],\n        [-0.3011, -0.4368, -0.3770,  0.4079],\n        [-0.0182,  0.3828,  0.4397, -0.0060]], requires_grad=True)\n\n\n\nl.weight\n\nParameter containing:\ntensor([[ 0.5296,  0.2421,  0.3622, -0.0752],\n        [-0.4137,  0.3981, -0.2259,  0.1988],\n        [-0.2698, -0.4054, -0.3455,  0.4392],\n        [ 0.0132,  0.4141,  0.4709,  0.0254]], requires_grad=True)"
  },
  {
    "objectID": "posts/efficient-zero/index.html",
    "href": "posts/efficient-zero/index.html",
    "title": "blog",
    "section": "",
    "text": "MuZero\nLets define our alphabet soup: * h - calculates latent representation of state s from the past observation (board state, or previous frames) * s - state, latent representation of the environment * f - calculates p(policy) and v(value function) from s(state) * p - policy value for each action * v - value function, based on the reward. For atari n-step reward, final reward for board games. * a - some action, sampled from π/p when interacting with the environment, sampled from replay buffer during training. * g - calculates next s(state) and immediate reward(r), recieves previous state and an action as input * r - immediate reward * π - policy, approximately p\n\n[0.1,0.5,0.6,0.7,...]\n\nmuzero can learn rules of game, doesn’t need to be provided with rules.\n\nup,down,left,right\n1=(0.2,0.1,0.1,0.7)\n\n\n\n\nmu.png\n\n\nThe models are trained to predict p(policy),v(value function),r(immediate reward) from the REPLAY buffer.\n\n\nEfficientZero\n\nImprovement 1\n\nshow_image(Image.open('efficient_similarity.png'),figsize=[7,7])\n\n<AxesSubplot:>\n\n\n\n\n\nThe general idea for this one is from this paper: https://arxiv.org/abs/2011.10566\n\nThe “representation” is generated using h from before.\ng then creates s_(t+1) using s_t and a_t for “next state”\nThe “projector” and “predictor” seem to be both thrown away. The projector mapping s_t+1 to a lower deminsional space. This seems to be because we want the predictor to have very few parameters. (lower deminsional space just has less numbers)\nThe “predictor” seems to “bridge” the gap between both branches and allows for smaller batch size training and more stable training. “Exploring Simple Siamese Representation Learning” suggests that the predictor should model the Expected value of what was before the projector, including the difference between O_t and O_(t+1)\nSince the projector and representation is on both sides, and the predictor is sufficiently small, The hidden states for the two s_(t+1) must be similar.\n\n\n\nImprovement 2\nPredicting when a player will lose a point at a particular timestep is hard. Though it is much easier to predict “if” a player will lose a point.\n\n\n\nefficient_future.png\n\n\nPredicting the immediate reward(r) is hard, how do we know 20 steps ahead of time, exactly which time step we will get the reward? Instead they use a LSTM to predict the total reward up until the current time, and train to predict that value instead.\n\n\nImprovement 3\n\n\n\nefficient_replay.png\n\n\nTo use or not to use the replay buffer? - Green, MuZero uses the replayer buffer as is - Yellow, on more recent examples, Efficient Zero will use its policy to predict 1 state for training. - Blue, on less recent examples, Efficient Zero will us its policy for part of the future states. - Red, Efficient Zero can dream up the majority of the policy if observations in the replay buffer are very old.\n\n\nResults\nResults where inconsistent across different applications, so it may be better to think of this as a handful of techniques as opposed to something to always use together. More abalations in the other environments should help to determine what techniques generalize outside of atari and board games.\n\n\n\nefficient_inconsistent.png\n\n\n\n\n\nReferences\n\nMu zero\nEfficient Zero\nSimple Siamese Representations\nYanic Muzero\nYanic Efficient Zero\nImages are from their respective papers."
  },
  {
    "objectID": "posts/unet/index.html",
    "href": "posts/unet/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.torch_basics import *\n\n\nclass SequentialExDict(nn.Sequential):\n    \"Like `nn.Sequential`, but has a dictionary passed along with x.\"\n    def __init__(self, *layers,dict_names=['seq_dict']): \n        super().__init__(*layers)\n        self.dict_names=dict_names\n    def forward(self, x,**kwargs):\n        dicts = getattrs(x,*self.dict_names,default=kwargs)\n        for module in self:\n            for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n            x = module(x)\n        for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n        return x\n\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    ### Embeddings for $t$\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n        \"\"\"\n        * `n_channels` is the number of dimensions in the embedding\n        \"\"\"\n        super().__init__()\n        self.n_channels = n_channels\n        # First linear layer\n        self.layers = nn.Sequential(\n            nn.Linear(self.n_channels // 4, self.n_channels),\n            nn.ReLU(True),\n            nn.Linear(self.n_channels, self.n_channels)\n        )\n\n    def forward(self, x):\n        # Create sinusoidal position embeddings\n        # [same as those from the transformer](../../transformers/positional_encoding.html)\n        #\n        # \\begin{align}\n        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n        # \\end{align}\n        #\n        # where $d$ is `half_dim`\n        t=torch.tensor(x.seq_dict['t']) if isinstance(x.seq_dict['t'],int) else x.seq_dict['t']\n        t=t.view(t.shape[0])\n        half_dim = self.n_channels // 8\n        emb = math.log(10_000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n\n        # Transform with the MLP\n        emb = self.layers(emb)\n        x.seq_dict['time']=emb\n        return x\n\n\nclass OnKey(nn.Module):\n    def __init__(self,k_in,module,k_out=None):\n        super().__init__()\n        if(k_out is None): k_out=k_in+'_out'\n        self.k_in=k_in\n        self.k_out=k_out\n        self.f=module\n    def forward(self, x):\n        x.seq_dict[self.k_out]=self.f(x.seq_dict[self.k_in])\n        return x\n\n\nclass Stack(nn.Module):\n    def __init__(self,key,f=lambda x:x):\n        super().__init__()\n        self.key,self.f=key,f\n    def forward(self,x):\n        if(self.key not in x.seq_dict): x.seq_dict[self.key]=[]\n        x.seq_dict[self.key]+=[self.f(x)]\n        return x\n\n\nclass Pop(nn.Module):\n    def __init__(self,key,f,clear=True,**kwargs):\n        super().__init__()\n        self.key,self.clear,self.f,self.kwargs=key,clear,f,kwargs\n    def forward(self, x): \n        o=x.seq_dict[self.key]\n        if(is_listy(o)): \n            o =  x.seq_dict[self.key].pop(-1) if(self.clear) else o[-1]\n        elif(self.clear): x.seq_dict[self.key]=None\n        return self.f(x,o,**self.kwargs)\n\n\ndef merge(x,o,dense=False): return torch.cat((x,o),dim=1) if(dense) else x+o.view(o.shape+(1,)*(x.ndim-o.ndim)) \n\n\nclass UnetTime(nn.Module):\n    \"A little Unet with time embeddings\"\n    def __init__(self,dims=[96, 192, 384, 768, 768],img_channels=3,ks=7,stem_stride=4,t_channels=128):\n        super().__init__()\n        i_d=0\n        h=dims[i_d]\n        self.time_emb=TimeEmbedding(t_channels)\n        # Not putting in for loop for ease of understanding arch\n        self.down=SequentialExDict(\n            nn.Conv2d(img_channels,h,ks,1,ks//2),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,stem_stride,1),\n            nn.GroupNorm(1,h),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n        )\n        self.middle=SequentialExDict(\n            self.basic_block(h,t_channels)\n        )\n        self.up=SequentialExDict(\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.down_sample(h*2,img_channels,5,1,2,bias=True),\n            self.basic_block(img_channels,t_channels,bias=True),\n        )\n        self.layers=SequentialExDict(\n            self.time_emb,\n            self.down,\n            self.middle,\n            self.up\n        )\n    @delegates(nn.Conv2d.__init__)\n    def up_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            Pop('s',lambda x,o:F.interpolate(x, size=[oi+1 for oi in o], mode='bilinear')),\n            self.down_sample(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    @delegates(nn.Conv2d.__init__)\n    def down_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            nn.GroupNorm(1,in_channels),\n            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    def basic_block(self,channels,time_channels,expansion=4,ks=7,stride=1,pad=None,bias=False):\n        if pad is None: pad=ks//2\n        return SequentialExDict(\n            Stack('r'),\n            nn.Conv2d(channels,channels,ks,padding=pad,bias=bias,stride=stride),\n            nn.GroupNorm(1,channels),\n            nn.Conv2d(channels,channels*expansion,1,bias=bias),\n            OnKey('time',nn.Linear(time_channels,channels*expansion)),\n            Pop('time_out',merge),\n            nn.GELU(),\n            nn.Conv2d(channels*expansion,channels,1,bias=bias),\n            Pop('r',merge),\n        )\n    def forward(self,x,x_o,t):\n        return self.layers(x,t=t)\n\nhello"
  },
  {
    "objectID": "posts/fp16/FP16.html",
    "href": "posts/fp16/FP16.html",
    "title": "FP16",
    "section": "",
    "text": "Getting setup\nLets start by looking at our data. We are working with low resolution cifar images.\n\nbs=128\ndls=DataBlock((ImageBlock(),\n               CategoryBlock()),\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(*cifar_stats)),\n          get_items=get_image_files,\n          get_y=parent_label\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n\n\n\nWe create a function to recreate our dataloader, because we will be doing it a lot for repeatability.\n\ndef create_dls():\n    bs=128\n    return DataBlock((ImageBlock(),\n                   CategoryBlock()),\n              item_tfms=[Resize(32)],\n              batch_tfms=(Normalize.from_stats(*cifar_stats)),\n              get_items=get_image_files,\n              get_y=parent_label\n    ).dataloaders(path,bs=bs,val_bs=2*bs)\n\nWe will just use a simple resnet to test our work.\n\nlearn=Learner(dls,resnet18(),opt_func=SGD)\n\nWe can make a tensor fp16 by calling Tensor.half()\n\ntorch.tensor(5.),torch.tensor(5.).half()\n\n(tensor(5.), tensor(5., dtype=torch.float16))\n\n\n\n\nWhole model half precision\nWe start by training our whole model using half precision. This includes converting our input data and out parameters to fp16.\n\nclass fp16Callback(Callback):\n    def before_batch(self):\n        self.learn.xb=(self.xb[0].half(),)\n        self.learn.model=self.model.half()\n    def after_batch(self):\n        #fix loss for logging\n        self.learn.loss=self.learn.loss.float()\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.154748\n      2.054516\n      00:17\n    \n    \n      1\n      1.803467\n      1.779198\n      00:15\n    \n    \n      2\n      1.646736\n      1.664151\n      00:15\n    \n    \n      3\n      1.547711\n      1.596234\n      00:15\n    \n    \n      4\n      1.487926\n      1.549276\n      00:15\n    \n    \n      5\n      1.433233\n      1.512745\n      00:15\n    \n  \n\n\n\nHow does this compare to training with fp32?\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD)\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.084821\n      1.969784\n      00:15\n    \n    \n      1\n      1.742251\n      1.712177\n      00:14\n    \n    \n      2\n      1.575977\n      1.596902\n      00:14\n    \n    \n      3\n      1.465374\n      1.527497\n      00:14\n    \n    \n      4\n      1.398808\n      1.475065\n      00:15\n    \n    \n      5\n      1.337400\n      1.437802\n      00:14\n    \n  \n\n\n\nSeems we are doing a bit worse…\n\n\nLooking at the gradients.\nWe create a callback that collects the data needed to create a colorful deminsion graph of our gradients.\n\n@patch\ndef after_backward(self:GradLogCallback):\n    for n,p in learn.model.named_parameters():\n        if p.numel()>10: \n            if n not in self.log: self.log[n]=[]\n            self.log[n]+=[p.cpu().abs().float().histc(100,0,1)]\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback,GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.154748\n      2.054516\n      00:25\n    \n    \n      1\n      1.803467\n      1.779198\n      00:25\n    \n    \n      2\n      1.646736\n      1.664151\n      00:25\n    \n    \n      3\n      1.547711\n      1.596234\n      00:25\n    \n    \n      4\n      1.487926\n      1.549276\n      00:25\n    \n    \n      5\n      1.433233\n      1.512745\n      00:25\n    \n  \n\n\n\nMost of the gradients are very close to zero. Just something to take note of, as the gradients don’t have a standard deviation of 1 like the activations.\n\nshow_colorful_grid(learn.cbs[-1].log)\n\n\n\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.084821\n      1.969784\n      00:27\n    \n    \n      1\n      1.742251\n      1.712177\n      00:27\n    \n    \n      2\n      1.575977\n      1.596902\n      00:27\n    \n    \n      3\n      1.465374\n      1.527497\n      00:26\n    \n    \n      4\n      1.398808\n      1.475065\n      00:26\n    \n    \n      5\n      1.337400\n      1.437802\n      00:27\n    \n  \n\n\n\nThe fp32 gradients actually look fairly similar.\n\nshow_colorful_grid(learn.cbs[-1].log)\n\n\n\n\n\n\nLooking near 0 for the Gradients\nWe look from 0 to 3 times the standard deviation of the first batch to get a closer look at the gradients\n\n@patch\ndef after_backward(self:GradLogCallback):\n    for n,p in learn.model.named_parameters():\n        if p.numel()>10: \n            if n not in self.log: self.log[n]=[3*p.cpu().abs().float().std()]\n            self.log[n]+=[p.cpu().abs().float().histc(100,0,self.log[n][0].item())]\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback,GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.154748\n      2.054516\n      00:30\n    \n    \n      1\n      1.803467\n      1.779198\n      00:29\n    \n    \n      2\n      1.646736\n      1.664151\n      00:29\n    \n    \n      3\n      1.547711\n      1.596234\n      00:30\n    \n    \n      4\n      1.487926\n      1.549276\n      00:29\n    \n    \n      5\n      1.433233\n      1.512745\n      00:30\n    \n  \n\n\n\n\nfp16_log=learn.grad_log.log\n\nWe can see something now! Pay a little bit of attention to the bn.weight graphs.\n\nshow_colorful_grid(learn.cbs[-1].log)\n\n\n\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.084821\n      1.969784\n      00:29\n    \n    \n      1\n      1.742251\n      1.712177\n      00:32\n    \n    \n      2\n      1.575977\n      1.596902\n      00:32\n    \n    \n      3\n      1.465374\n      1.527497\n      00:31\n    \n    \n      4\n      1.398808\n      1.475065\n      00:32\n    \n    \n      5\n      1.337400\n      1.437802\n      00:32\n    \n  \n\n\n\nhm, these FP32 gradients mostly look the same, though we are not getting a horizontal line for the batchnorm weights.\n\nshow_colorful_grid(learn.cbs[-1].log)\n\n\n\n\n\nfp32_log=learn.grad_log.log\n\nOne thing to note is that the minimum positive value for fp16 is a not as small as fp32.\n\ntorch.tensor(2**-24,dtype=torch.half),torch.tensor(2**-149)\n\n(tensor(5.9605e-08, dtype=torch.float16), tensor(1.4013e-45))\n\n\nhm, the batchnorm weight gradients standard deviations are ZERO for fp16!\n\n[[k,fp16_log[k][0].item(),fp32_log[k][0].item()] for k in fp16_log if 'bn' in k]\n\n[['bn1.weight', 0.0, 0.0060293180868029594],\n ['bn1.bias', 0.004148084670305252, 0.003580566728487611],\n ['layer1.0.bn1.weight', 0.0, 0.005399828776717186],\n ['layer1.0.bn1.bias', 0.004158522468060255, 0.0034905048087239265],\n ['layer1.0.bn2.weight', 0.0, 0.004458598792552948],\n ['layer1.0.bn2.bias', 0.0027511364314705133, 0.0024442975409328938],\n ['layer1.1.bn1.weight', 0.0, 0.0039586215279996395],\n ['layer1.1.bn1.bias', 0.002898486563935876, 0.002269925782456994],\n ['layer1.1.bn2.weight', 0.0, 0.0035925875417888165],\n ['layer1.1.bn2.bias', 0.001713279401883483, 0.0014464608393609524],\n ['layer2.0.bn1.weight', 0.0, 0.003240257501602173],\n ['layer2.0.bn1.bias', 0.0017084497958421707, 0.0014696172438561916],\n ['layer2.0.bn2.weight', 0.0, 0.002757459646090865],\n ['layer2.0.bn2.bias', 0.0018845133017748594, 0.0017825027462095022],\n ['layer2.1.bn1.weight', 0.0, 0.0026580658741295338],\n ['layer2.1.bn1.bias', 0.0015702887903898954, 0.0015195324085652828],\n ['layer2.1.bn2.weight', 0.0, 0.002109265886247158],\n ['layer2.1.bn2.bias', 0.0011786026880145073, 0.001151321455836296],\n ['layer3.0.bn1.weight', 0.0, 0.001799717196263373],\n ['layer3.0.bn1.bias', 0.0011105844751000404, 0.000998087227344513],\n ['layer3.0.bn2.weight', 0.0, 0.0017275793943554163],\n ['layer3.0.bn2.bias', 0.001101263682357967, 0.0009456288535147905],\n ['layer3.1.bn1.weight', 0.0, 0.001411611563526094],\n ['layer3.1.bn1.bias', 0.0008357313927263021, 0.0007302637677639723],\n ['layer3.1.bn2.weight', 0.0, 0.0011884564300999045],\n ['layer3.1.bn2.bias', 0.0006777657545171678, 0.0004925086977891624],\n ['layer4.0.bn1.weight', 0.0, 0.0009125272044911981],\n ['layer4.0.bn1.bias', 0.0005735242739319801, 0.0004978459910489619],\n ['layer4.0.bn2.weight', 0.0, 0.0017549480544403195],\n ['layer4.0.bn2.bias', 0.002376189222559333, 0.002254981081932783],\n ['layer4.1.bn1.weight', 0.0, 0.0008248933590948582],\n ['layer4.1.bn1.bias', 0.0006004928727634251, 0.0004860978224314749],\n ['layer4.1.bn2.weight', 0.0, 0.00195809337310493],\n ['layer4.1.bn2.bias', 0.0034636915661394596, 0.003284711856395006]]\n\n\n\n\nLetting Batchnorm stay in FP32\nWe create a function here that just allows us to apply a funtion to both our module, and one of its parameters.\n\ndef apply_p(f,m):\n    for module in m.children():\n        apply_p(f,module)\n        for n,p in module.named_parameters(recurse=False):\n            f(module,n)\n\nHere we allow BatchNorm weights to stay in fp32.\n\nclass fp16Callback(Callback):\n    def before_fit(self):\n        def f(m,n):\n            getattr(m,n).data=getattr(m,n).data.float() if isinstance(m,nn.BatchNorm2d) else getattr(m,n).data.half()\n        apply_p(f,self.learn.model)\n    def before_batch(self):\n        self.learn.xb=(self.xb[0].half(),)\n    def after_pred(self):\n        #loss should be calculated in fp32 as well\n        self.learn.pred=self.learn.pred.float()\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback,GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.160702\n      2.038649\n      00:29\n    \n    \n      1\n      1.807114\n      1.770107\n      00:29\n    \n    \n      2\n      1.642703\n      1.656874\n      00:28\n    \n    \n      3\n      1.542962\n      1.592482\n      00:29\n    \n    \n      4\n      1.486021\n      1.546489\n      00:29\n    \n    \n      5\n      1.425706\n      1.504864\n      00:29\n    \n  \n\n\n\nhmm, we have slightly better results, but nothing too close to our fp32 model.\n\nfp16_bn_log=learn.grad_log.log\n\nThough, we have gotten the bn weights to have gradients!\n\n[[k,fp16_log[k][0].item(),fp16_bn_log[k][0].item(),fp32_log[k][0].item()] for k in fp16_log if 'bn' in k]\n\n[['bn1.weight', 0.0, 0.006746089085936546, 0.0060293180868029594],\n ['bn1.bias',\n  0.004148084670305252,\n  0.003786720335483551,\n  0.003580566728487611],\n ['layer1.0.bn1.weight', 0.0, 0.005284080747514963, 0.005399828776717186],\n ['layer1.0.bn1.bias',\n  0.004158522468060255,\n  0.003640677547082305,\n  0.0034905048087239265],\n ['layer1.0.bn2.weight', 0.0, 0.005027716979384422, 0.004458598792552948],\n ['layer1.0.bn2.bias',\n  0.0027511364314705133,\n  0.0025208923034369946,\n  0.0024442975409328938],\n ['layer1.1.bn1.weight', 0.0, 0.00424396526068449, 0.0039586215279996395],\n ['layer1.1.bn1.bias',\n  0.002898486563935876,\n  0.002260061912238598,\n  0.002269925782456994],\n ['layer1.1.bn2.weight', 0.0, 0.004007537383586168, 0.0035925875417888165],\n ['layer1.1.bn2.bias',\n  0.001713279401883483,\n  0.0016484896186739206,\n  0.0014464608393609524],\n ['layer2.0.bn1.weight', 0.0, 0.0031620513182133436, 0.003240257501602173],\n ['layer2.0.bn1.bias',\n  0.0017084497958421707,\n  0.0016835747519508004,\n  0.0014696172438561916],\n ['layer2.0.bn2.weight', 0.0, 0.0031509941909462214, 0.002757459646090865],\n ['layer2.0.bn2.bias',\n  0.0018845133017748594,\n  0.0020461969543248415,\n  0.0017825027462095022],\n ['layer2.1.bn1.weight', 0.0, 0.003080494701862335, 0.0026580658741295338],\n ['layer2.1.bn1.bias',\n  0.0015702887903898954,\n  0.0016526294639334083,\n  0.0015195324085652828],\n ['layer2.1.bn2.weight', 0.0, 0.0026030924636870623, 0.002109265886247158],\n ['layer2.1.bn2.bias',\n  0.0011786026880145073,\n  0.0013054630253463984,\n  0.001151321455836296],\n ['layer3.0.bn1.weight', 0.0, 0.0020066993311047554, 0.001799717196263373],\n ['layer3.0.bn1.bias',\n  0.0011105844751000404,\n  0.0010541853262111545,\n  0.000998087227344513],\n ['layer3.0.bn2.weight', 0.0, 0.0021246818359941244, 0.0017275793943554163],\n ['layer3.0.bn2.bias',\n  0.001101263682357967,\n  0.0010443663923069835,\n  0.0009456288535147905],\n ['layer3.1.bn1.weight', 0.0, 0.001690064324066043, 0.001411611563526094],\n ['layer3.1.bn1.bias',\n  0.0008357313927263021,\n  0.0008858887013047934,\n  0.0007302637677639723],\n ['layer3.1.bn2.weight', 0.0, 0.0014081220142543316, 0.0011884564300999045],\n ['layer3.1.bn2.bias',\n  0.0006777657545171678,\n  0.0006280804518610239,\n  0.0004925086977891624],\n ['layer4.0.bn1.weight', 0.0, 0.0010809964733198285, 0.0009125272044911981],\n ['layer4.0.bn1.bias',\n  0.0005735242739319801,\n  0.0005982829607091844,\n  0.0004978459910489619],\n ['layer4.0.bn2.weight', 0.0, 0.0018873803783208132, 0.0017549480544403195],\n ['layer4.0.bn2.bias',\n  0.002376189222559333,\n  0.0023989235050976276,\n  0.002254981081932783],\n ['layer4.1.bn1.weight', 0.0, 0.0010339637519791722, 0.0008248933590948582],\n ['layer4.1.bn1.bias',\n  0.0006004928727634251,\n  0.0006056932033970952,\n  0.0004860978224314749],\n ['layer4.1.bn2.weight', 0.0, 0.002108693355694413, 0.00195809337310493],\n ['layer4.1.bn2.bias',\n  0.0034636915661394596,\n  0.0034976028837263584,\n  0.003284711856395006]]\n\n\n\n\nLooking at the gradients\n\n\n\nnv_grads.png\n\n\nImage taken from here: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html\nAbove we can see that some of the gradients will go to zero in fp16. This will happen more as the model trains and the gradients get smaller throughout training.\nHere we are create seperate FP16 parameters for our model, while our optimizer uses the fp32 weights. We scale up the loss by 128 before our backwards pass, to increase the size of the gradients. We then descale it back 128 when converting back to fp32 for our optimizer step.\n\n@patch\ndef before_fit(self:fp16Callback):\n    self.opt_params= [p for p in self.model.parameters()]\n    def f(m,n):\n        value=getattr(m,n).data.float() if isinstance(m,nn.BatchNorm2d) else getattr(m,n).data.half()\n        setattr(m,n,nn.Parameter(value))\n    apply_p(f,self.learn.model)\n@patch\ndef before_backward(self:fp16Callback):\n    self.learn.loss_grad=128*self.learn.loss_grad\n@patch\ndef after_backward(self:fp16Callback):\n    for mp,op in zip(self.learn.model.parameters(),self.opt_params):\n        op.grad=mp.grad.to(dtype=torch.float32)/128\n@patch\ndef after_step(self:fp16Callback):\n    for mp,op in zip(self.learn.model.parameters(),self.opt_params):\n        mp.data= op.data.to(dtype=torch.float16) if(op.grad.dtype!=mp.grad.dtype) else op.data.clone()\n    self.learn.model.zero_grad()\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback,GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.088479\n      1.967441\n      00:32\n    \n    \n      1\n      1.727627\n      1.706729\n      00:31\n    \n    \n      2\n      1.566641\n      1.595630\n      00:31\n    \n    \n      3\n      1.463253\n      1.530451\n      00:31\n    \n    \n      4\n      1.398891\n      1.483463\n      00:32\n    \n    \n      5\n      1.338582\n      1.453968\n      00:32\n    \n  \n\n\n\nAnd, now we have results comparable to fp32!\n\n\nCan we do this in a non-manual way?\nWell we start at the maximum possible multiple as the loss_scale. We multiply our gradients by this value. Yes, this value is definitely going to be too big so we need a strategy for decreasing the value now.\n\n@patch\ndef before_fit(self:fp16Callback):\n    self.loss_scale=2.**24\n    self.count=0\n    self.opt_params= [p for p in self.model.parameters()]\n    def f(m,n):\n        value=getattr(m,n).data.float() if isinstance(m,nn.BatchNorm2d) else getattr(m,n).data.half()\n        setattr(m,n,nn.Parameter(value))\n    apply_p(f,self.learn.model)\n@patch\ndef before_backward(self:fp16Callback):\n    self.learn.loss_grad=self.loss_scale*self.learn.loss_grad\n\nThis gets really messy, but we decrease by half if we overflow. We then skip the current batch and go to the next batch. This does mean we will hit a lot of skipped batches in the beginning of training. As our model trains, our gradients will probably get smaller, so we will want to increase our loss_scale. For this example we just test every 500 batches to see if we should increase our loss_scale.\n\n@patch\ndef after_backward(self:fp16Callback):\n    for mp in self.learn.model.parameters():\n        if mp.grad is not None and test_overflow(mp.grad.data):\n            self.learn.model.zero_grad()\n            self.loss_scale/=2\n            raise CancelBatchException()\n    for mp,op in zip(self.learn.model.parameters(),self.opt_params):\n        op.grad=mp.grad.to(dtype=torch.float32)/self.loss_scale\n    self.count += 1\n    if self.count == 500:\n        self.count = 0\n        self.loss_scale *= 2\n\n\nwith less_random():\n    dls=create_dls()\n    learn=Learner(dls,resnet18(),opt_func=SGD,cbs=[fp16Callback,GradLogCallback])\n    learn.fit(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.094611\n      1.968921\n      00:33\n    \n    \n      1\n      1.729887\n      1.698773\n      00:33\n    \n    \n      2\n      1.563014\n      1.592412\n      00:33\n    \n    \n      3\n      1.455867\n      1.526811\n      00:34\n    \n    \n      4\n      1.397211\n      1.486772\n      00:34\n    \n    \n      5\n      1.330738\n      1.447847\n      00:34\n    \n  \n\n\n\nYou can look more into all of this by checking out NonNativeMixedPrecision."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "marii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]