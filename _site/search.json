[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "marii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adan-optimizer/index.html",
    "href": "posts/adan-optimizer/index.html",
    "title": "blog",
    "section": "",
    "text": "def update_prev_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n    \"Keeps track of the previous gradient, should be one of last cbs. \"\n    return {'prev_grad': p.grad.data}\n\n\ndef n_avg_grad(p,lr,nmom=None,n_avg=None,prev_grad=None,**kwags):\n    if n_avg is None: \n        prev_grad=torch.zeros_like(p.grad.data)\n        n_avg = p.grad.data-prev_grad\n    else:\n        n_avg = (1-nmom)*n_avg+nmom*(p.grad.data-prev_grad)\n    return {'n_avg': n_avg,'prev_grad':prev_grad}\n\n\ndef n_average_sqr_grad(p,nmom,sqr_mom, prev_grad=None, dampening=True, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    damp = 1-sqr_mom if dampening else 1.\n    grad = (2-nmom)*p.grad.data+(nmom-1)*prev_grad\n    sqr_avg.mul_(sqr_mom).addcmul_(grad,grad, value=damp)\n    return {'sqr_avg': sqr_avg}\n\n\ndef adan_step(p,lr,grad_avg=None,nmom=None,n_avg=None,sqr_avg=None,\n             eps=None,**kwargs):\n    p.data.addcdiv_(grad_avg+(1-nmom)*n_avg, \n                    (sqr_avg).sqrt() + eps, \n                    value = -lr)\n\n\ndef Adan(params, lr, mom=0.9, sqr_mom=0.99,nmom=0.9, eps=1e-5, wd=0.01, decouple_wd=True):\n    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n    cbs = [weight_decay] if decouple_wd else [l2_reg]\n    cbs += [partial(average_grad, dampening=True),n_avg_grad, n_average_sqr_grad,adan_step, update_prev_grad]\n    return Optimizer(params, cbs, lr=lr,nmom=nmom, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n\n\nl=nn.Linear(4,4)\nopt=Adan(l.parameters(),0.01)\nprint(l.weight)\ninp=torch.tensor([.1,.2,.3,.4])\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\n\nParameter containing:\ntensor([[ 0.4984,  0.2108,  0.3309, -0.1065],\n        [-0.4451,  0.3669, -0.2573,  0.1675],\n        [-0.3011, -0.4368, -0.3770,  0.4079],\n        [-0.0182,  0.3828,  0.4397, -0.0060]], requires_grad=True)\n\n\n\nl.weight\n\nParameter containing:\ntensor([[ 0.5296,  0.2421,  0.3622, -0.0752],\n        [-0.4137,  0.3981, -0.2259,  0.1988],\n        [-0.2698, -0.4054, -0.3455,  0.4392],\n        [ 0.0132,  0.4141,  0.4709,  0.0254]], requires_grad=True)"
  },
  {
    "objectID": "posts/diffusion/index.html",
    "href": "posts/diffusion/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.vision.all import *\nfrom fastai.torch_basics import *\nfrom denoising_diffusion_pytorch import Unet\n\n\nimport wandb\nwandb.init(reinit=True)\nfrom fastai.callback.wandb import *\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: marii. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /home/molly/Projects/quatro-blog/posts/diffusion/wandb/run-20221006_011034-nhpww4wt\n\n\nSyncing run super-universe-63 to Weights & Biases (docs)\n\n\n\ndef gather(consts: torch.Tensor, t: torch.Tensor):\n    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n    c = consts.gather(-1, t)\n    return c.reshape(-1, 1, 1, 1)\n\n\nclass DenoiseDiffusion:\n    \"\"\"\n    ## Denoise Diffusion\n    \"\"\"\n\n    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n        \"\"\"\n        * `eps_model` is $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n        * `n_steps` is $t$\n        * `device` is the device to place constants on\n        \"\"\"\n        super().__init__()\n        self.eps_model = eps_model\n\n        # Create $\\beta_1, \\dots, \\beta_T$ linearly increasing variance schedule\n        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n\n        # $\\alpha_t = 1 - \\beta_t$\n        self.alpha = 1. - self.beta\n        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n        # $T$\n        self.n_steps = n_steps\n        # $\\sigma^2 = \\beta$\n        self.sigma2 = self.beta\n\n    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        #### Get $q(x_t|x_0)$ distribution\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # [gather](utils.html) $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n        mean = gather(self.alpha_bar, t) ** 0.5 * tensor(x0)\n        # $(1-\\bar\\alpha_t) \\mathbf{I}$\n        var = 1 - gather(self.alpha_bar, t)\n        #\n        return mean, var\n\n    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n        \"\"\"\n        #### Sample from $q(x_t|x_0)$\n        \\begin{align}\n        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        if eps is None:\n            eps = torch.randn_like(x0)\n\n        # get $q(x_t|x_0)$\n        mean, var = self.q_xt_x0(x0, t)\n        # Sample from $q(x_t|x_0)$\n        return mean + (var ** 0.5) * eps\n\n    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n        \\begin{align}\n        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n        \\end{align}\n        \"\"\"\n\n        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n        \n        # NOTEDDDD REMOVED t\n        \n        eps_theta = self.eps_model(xt,t)\n        # [gather](utils.html) $\\bar\\alpha_t$\n        alpha_bar = gather(self.alpha_bar, t)\n        # $\\alpha_t$\n        alpha = gather(self.alpha, t)\n        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n        # $\\sigma^2$\n        var = gather(self.sigma2, t)\n\n        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n        eps = torch.randn(xt.shape, device=xt.device)\n        # Sample\n        return mean + (var ** .5) * eps\n\n\nclass Q_sample(ItemTransform):\n    order=101\n    def __init__(self,diffusion):\n        self.diffusion=diffusion\n    def encodes(self,xy):\n        x=xy[0]\n        y=xy[-1]\n        ts = xy[2][:,0]#torch.randint(0, self.diffusion.n_steps, (x.shape[0],), device=x.device, dtype=torch.long)\n        x_type=type(x)\n        x=self.diffusion.q_sample(x, x_type(ts), eps=y)\n        return (x,*xy[1:-1],y)\n\n\nclass LabelToNoise(ItemTransform):\n    order=100\n    def encodes(self,xy):\n        y=xy[-1]\n        return (*xy[:-1],retain_type(torch.randn(y.shape,device=y.device),old=y))\n\n\ndef sample():\n    \"\"\"\n    ### Sample images\n    \"\"\"\n    with torch.no_grad():\n        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n        x = torch.randn([n_samples, image_channels, 32, 32],\n                        device=device)\n\n        # Remove noise for $T$ steps\n        for t_ in range(n_steps):\n            # $t$\n            t = n_steps - t_ - 1\n            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n            x = diffusion.p_sample(x, x.new_full((n_samples,), t, dtype=torch.long))\n        return x\n\n\nn_steps=1000\n\n\npath = untar_data(URLs.MNIST)\npath = untar_data(URLs.CIFAR)\n\n\nm=Unet(dim=32,channels=3)#UnetTime(img_channels=1,dims=[32, 64, 128, 256, 256],ks=3,stem_stride=2).cuda()\n\n\n@typedispatch\ndef show_batch(x:tuple, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(3*min(len(samples), max_n), nrows=nrows, ncols=3, figsize=figsize, title='Input/Original/Target')\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::3],range(max_n))]\n    ctxs[0::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::3],range(max_n))]\n    ctxs[1::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::3],range(max_n))]\n    ctxs[2::3] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[2::3],range(max_n))]\n    return ctxs\n\n\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(cls=PILImageBW),\n               ImageBlock(cls=PILImageBW),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock(cls=PILImageBW)),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n).dataloaders(path,bs=128,val_bs=2*128)\ndls.show_batch()\n\nIndexError: list index out of range\n\n\n\npath.ls()\n\n(#3) [Path('/home/molly/data/cifar10/labels.txt'),Path('/home/molly/data/cifar10/test'),Path('/home/molly/data/cifar10/train')]\n\n\n\nbs=128\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndls=DataBlock((ImageBlock(),\n               ImageBlock(),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorCategory(o),dec=Category)]),\n               ImageBlock()),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(0.5,1.),LabelToNoise,Q_sample(diffusion)),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(0, n_steps, (1,), dtype=torch.long)],\n          splitter=IndexSplitter(range(bs)),\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n\n\n\n\nclass FlattenCallback(Callback):\n    def before_batch(self):\n        self.learn.xb=(self.xb[0],self.xb[-1].view(self.xb[-1].shape[::2]),)\n\n\nlearn = Learner(dls,m,MSELossFlat(),opt_func=Lamb,cbs=[FlattenCallback,WandbCallback(log_preds_every_epoch=True)])\n\ninp=m.layers0:0 inp.seq_dict[‘t’]=torch.tensor([5]).cuda() m.layers1:4.shape\n\nlearn.fit_flat_cos(6,lr=1e-4,wd=0.)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.699252\n      0.648226\n      01:58\n    \n    \n      1\n      0.375051\n      0.354264\n      01:54\n    \n    \n      2\n      0.185453\n      0.168021\n      01:52\n    \n    \n      3\n      0.102715\n      0.080112\n      01:53\n    \n    \n      4\n      0.064207\n      0.043586\n      01:56\n    \n    \n      5\n      0.055305\n      0.053669\n      01:52\n    \n  \n\n\n\nWandbCallback was not able to get prediction samples -> Match length mismatch\n\n\n\n@typedispatch\ndef show_results(x:tuple, y:TensorImage, samples, outs, ctxs=None, max_n=10, figsize=None, **kwargs):\n    if ctxs is None: ctxs = get_grid(6*min(len(samples), max_n), ncols=6, figsize=figsize, title='Input/Original/DenoisedImage/Target/Prediction/Diff')\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::6],range(max_n))]\n    ctxs[1::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(1),ctxs[1::6],range(max_n))]\n    ctxs[0::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(2),ctxs[0::6],range(max_n))]\n    ctxs[2::6] = [(b-o).show(ctx=c, **kwargs) for b,o,c,_ in zip(samples.itemgot(0),outs.itemgot(0),ctxs[2::6],range(max_n))]\n    ctxs[3::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(3),ctxs[3::6],range(max_n))]\n    ctxs[4::6] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(0),ctxs[4::6],range(max_n))]\n    ctxs[5::6] = [(b-targ).show(ctx=c, **kwargs) for b,targ,c,_ in zip(outs.itemgot(0),samples.itemgot(3),ctxs[5::6],range(max_n))]\n    return ctxs\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nn_samples=12\nimage_channels=3\ndiffusion = DenoiseDiffusion(m,n_steps,torch.device(0))\ndevice=torch.device(0)\nxs = sample()\n\n\nshow_images((logit((xs.repeat(1,3,1,1)-xs.repeat(1,3,1,1).mean())/xs.repeat(1,3,1,1).std()).sigmoid()),nrows=4)\n\n\nshow_images((logit((xs-xs.mean())/xs.std()).sigmoid()),nrows=4)\n\n\n\n\n\nxs.min()\n\n\nshow_images(xs)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nwith learn.removed_cbs(WandbCallback):\n    show_images(dls.one_batch()[0][:4])\n\n\nlearn.show_results??\n\n\ndls.show_batch(show=False)[2]\n\n\n@typedispatch\ndef wandb_process(x:tuple, y, samples, outs, preds):\n    \"Process `sample` and `out` depending on the type of `x/y`\"\n    res_input, res_pred, res_label = [],[],[]\n    for s,o in zip(samples, outs):\n        img = s[0].permute(1,2,0)\n        res_input.append(wandb.Image(img, caption='Input_data'))\n        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground_Truth\", res_label)):\n            fig, ax = _make_plt(img)\n            # Superimpose label or prediction to input image\n            ax = img.show(ctx=ax)\n            ax = t.show(ctx=ax)\n            res.append(wandb.Image(fig, caption=capt))\n            plt.close(fig)\n    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground_Truth\":res_label}\n\n\nlearn.show_results()\n\n\n%debug\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev import nbdev_export\nnbdev_export()\n:::"
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nBackground\nFirst, a bit of review. Each \\(x_t\\) is dependent on some \\(x_{t-1}\\) in a Markov chain defined below. This we can also define in terms of \\(a_t\\) and \\(x_0\\), so that we can calculate the amount of noise at each step without the calculation being Markovian(or dependent on the previous step). This essentially gives us a formula that is the original image \\(x_0\\) plus some noise \\(\\epsilon\\)\n\\(q(x_t|x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\textbf{I})\\\\ \\epsilon\\sim\\mathcal{N}(0,\\textbf{I})\\\\ a_t := \\prod_{s=1}^t{1-\\beta_s}\\\\ x_t = \\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon\\)\nBoth \\(\\alpha_t\\) and \\(\\beta_t\\) are of interest to us, so we define the code here. We have 1000 values, one value for each time step \\(t\\).\n\ndef at(Bt): return torch.cumprod(1-Bt,-1)\nBt=torch.linspace(1e-4,0.02,1000) #schedule used in DDPM paper\nat(Bt).shape\n\ntorch.Size([1000])\n\n\n\n\nSignal-to-Noise Ratio\nSignal-to-noise ratio is very important for this paper. We if we put the below two formulas close together the relationship should be clear, remembers that \\(x_0\\) is the original image, and \\(\\epsilon\\) is our noise.\n\\(x_t = \\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon\\\\ SNR(t) = \\frac{\\alpha_t}{1-\\alpha_t}\\)\n\ndef snr(at): return at/(1-at)\n\nAnother noise schedule that is investigated is the cosine noise schedule, we define the code for it below.\n\ns=0.008 #from Improved Denoising Diffusion Probabilistic Models\ndef cos_sched(at): \n    at = torch.linspace(0,1,at.shape[0])\n    return torch.cos((at+s)/(1+s)*torch.pi/2)**2\n\nWe can now take a look at the noise schedule per diffusion step \\(t\\). Remeber low \\(\\alpha_t\\) means more noise \\(\\epsilon\\).\n\nplt.plot(at(Bt))\nplt.plot(cos_sched(Bt))\nplt.ylabel('Alphas a_t')\nplt.xlabel('Diffusion Steps (t)')\nplt.title('Noise Schedules')\nplt.legend(['Cosine Schedule','Linear Schedule'])\n\n<matplotlib.legend.Legend at 0x7fd7e2d69870>\n\n\n\n\n\nNow we can graph the signal to noise ratio for both the cosine and linear schedules.\n\nplt.yscale('log')\nplt.ylim(top=1e4,bottom=1e-8)\nplt.plot(snr(cos_sched(Bt)))\nplt.plot(snr(at(Bt)))\nplt.ylabel('Signal-to-Noise Ratio (SNR)')\nplt.xlabel('Diffusion Steps (t)')\nplt.title('Signal-to-Noise Ratio (SNR)')\nplt.legend(['Cosine Schedule','Linear Schedule'])\n\n<matplotlib.legend.Legend at 0x7fd8e86670a0>\n\n\n\n\n\nNotice that we get very similar results to the paper as seen below.\n\n\n\n161203299-8b02d76b-9c51-4529-8329-3ac08e9f3bc8.png\n\n\n\n\nContinuous Weights\nOkay, now we need to determine the weights above. This is the goal of the paper. First we look at the first contribution, a way to calculate the weights in terms of the signal-to-weight ratio, which is a continuous version of the weighting scheme introduced in the DDPM paper. The derivation of this is in the paper’s appendix.\n\ndef Ho_weights(Bt,at):\n    return (1-Bt)*(1-at)/Bt\ndef continuous_weights(at):\n    weights = -snr(at[1:])/(snr(at[1:])-snr(at[:-1]))\n    return torch.cat((weights[0:1],weights)) #we just make a copy of the first to get same shape\n\nWe can now compare the unnormalized weights of the continuous weight schedule and the one in the DDPM paper. They are fairly close.\n\nplt.plot(Ho_weights(Bt,at(Bt)))\nplt.plot(continuous_weights(at(Bt)))\nplt.ylabel('Weights(λ_t)')\nplt.xlabel('Diffusion Steps(t)')\nplt.title('Comparison of Ho and Continuous Weights')\nplt.legend(['Ho','Continuous'])\n\n<matplotlib.legend.Legend at 0x7fd7e2e0eb60>\n\n\n\n\n\n\n\nPrioritized Weight Schedule\nNext we can look at the prioritized weight schedule. The main contribution of the paper. \\(\\lambda_t\\) is our continuous weights from above, k is a constant set to \\(1\\). \\(\\gamma\\) is a hyperparameter that we can control, but it doesn’t work so well at over 2, because “We empirically observed that γ over 2 suffers noise artifacts in the sample because it assigns almost zero weight to the clean-up stage” (quoting paper).\n\\(\\lambda_t^\\prime = \\frac{\\lambda_t}{(k+SNR(t))^\\gamma}\\)\nAnd, here is it in code. \\(\\gamma=0\\) essentially turns the prioritized weighting mechanism off, and gives us the same result as the weighting mechanism in the DDPM paper.\n\nk=1. #set for nice math reasons\ndef prioritized_weights(l,t,g=0.):\n    return l/(k + snr(t))**g\n\nHere we go ahead and generate weights based on a linear noise schedule for different values of \\(\\gamma\\). Notice how it is similar to the results from the paper.\n\n\n\nplt.xscale('log')\nplt.plot(snr(at(Bt)),F.normalize(continuous_weights(at(Bt)),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(at(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Linear Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb'])\n\n<matplotlib.legend.Legend at 0x7fd7e2a625c0>\n\n\n\n\n\nHere we go ahead and generate weights based on a cosine noise schedule for different values of \\(\\gamma\\). Notice how it is similar to the results from the paper.\n\nplt.xscale('log')\nplt.xlim(left=1e-8,right=1e4)\nplt.plot(snr(cos_sched(Bt)),F.normalize(continuous_weights(cos_sched(Bt)),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Cosine Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb'])\n\n<matplotlib.legend.Legend at 0x7fd7e2904d00>\n\n\n\n\n\n\n\n\n\nResults\nBelow you can see various results where the models performed better. Note, on the right, the this paper’s model is named P2. For the middle table the schedule makes the most difference when a model is missing attention, suggesting the weighting introduced helps with global features. For the images, notice that the samples generated have better clobal features, though both are going well at smaller details. The authors believe this is because the weights help the model focus more on global features.\n\n\n\npaper_results.png\n\n\n\n\nReferences\nhttps://arxiv.org/abs/2204.00227"
  }
]