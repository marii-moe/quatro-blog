[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "marii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarii\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adan-optimizer/index.html",
    "href": "posts/adan-optimizer/index.html",
    "title": "blog",
    "section": "",
    "text": "def update_prev_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n    \"Keeps track of the previous gradient, should be one of last cbs. \"\n    return {'prev_grad': p.grad.data}\n\n\ndef n_avg_grad(p,lr,nmom=None,n_avg=None,prev_grad=None,**kwags):\n    if n_avg is None: \n        prev_grad=torch.zeros_like(p.grad.data)\n        n_avg = p.grad.data-prev_grad\n    else:\n        n_avg = (1-nmom)*n_avg+nmom*(p.grad.data-prev_grad)\n    return {'n_avg': n_avg,'prev_grad':prev_grad}\n\n\ndef n_average_sqr_grad(p,nmom,sqr_mom, prev_grad=None, dampening=True, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    damp = 1-sqr_mom if dampening else 1.\n    grad = (2-nmom)*p.grad.data+(nmom-1)*prev_grad\n    sqr_avg.mul_(sqr_mom).addcmul_(grad,grad, value=damp)\n    return {'sqr_avg': sqr_avg}\n\n\ndef adan_step(p,lr,grad_avg=None,nmom=None,n_avg=None,sqr_avg=None,\n             eps=None,**kwargs):\n    p.data.addcdiv_(grad_avg+(1-nmom)*n_avg, \n                    (sqr_avg).sqrt() + eps, \n                    value = -lr)\n\n\ndef Adan(params, lr, mom=0.9, sqr_mom=0.99,nmom=0.9, eps=1e-5, wd=0.01, decouple_wd=True):\n    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n    cbs = [weight_decay] if decouple_wd else [l2_reg]\n    cbs += [partial(average_grad, dampening=True),n_avg_grad, n_average_sqr_grad,adan_step, update_prev_grad]\n    return Optimizer(params, cbs, lr=lr,nmom=nmom, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n\n\nl=nn.Linear(4,4)\nopt=Adan(l.parameters(),0.01)\nprint(l.weight)\ninp=torch.tensor([.1,.2,.3,.4])\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\nF.mse_loss(l(inp),torch.tensor([1.,2.,3.,4.])).backward()\nopt.step()\n\nParameter containing:\ntensor([[ 0.4984,  0.2108,  0.3309, -0.1065],\n        [-0.4451,  0.3669, -0.2573,  0.1675],\n        [-0.3011, -0.4368, -0.3770,  0.4079],\n        [-0.0182,  0.3828,  0.4397, -0.0060]], requires_grad=True)\n\n\n\nl.weight\n\nParameter containing:\ntensor([[ 0.5296,  0.2421,  0.3622, -0.0752],\n        [-0.4137,  0.3981, -0.2259,  0.1988],\n        [-0.2698, -0.4054, -0.3455,  0.4392],\n        [ 0.0132,  0.4141,  0.4709,  0.0254]], requires_grad=True)"
  },
  {
    "objectID": "posts/diffusion/Refactor.html",
    "href": "posts/diffusion/Refactor.html",
    "title": "DDPM",
    "section": "",
    "text": "class TensorNoise(TensorImageBase):pass\nclass TensorStep(TensorBase): pass\n\nWe would like normalize to denormalize our noise before showing it. This is so the noise in our image looks similar to the noise in our noised image.\n\n@Normalize\ndef decodes(self, x:TensorNoise):\n    f = to_cpu if x.device.type=='cpu' else noop\n    return (x*f(self.std) + f(self.mean))\n\n\nnorm = Normalize.from_stats(*imagenet_stats)\nshow_images(norm.decode(img))\n\n\n\n\nI patch ItemTransform here, so that it can work off of TypedTuples. Essentially if we have a DiffusionTuple, the transform will apply to that if it should apply to that type of tuple.\n\nclass ItemTransform(Transform):\n    \"A transform that always take tuples as items\"\n    _retain = True\n    # Only showing important code\n    def _call_tuple(self:ItemTransform, name, x, split_idx=None, **kwargs):\n        f = getattr(super(), name)\n        f2name='encodes' if name == '__call__' else 'decodes' if name == 'decode' else name\n        f2 = getattr(self, f2name)\n        if isinstance(f2,TypeDispatch) and f2[type(x)] is not None:\n            if split_idx!=self.split_idx and self.split_idx is not None: return x\n            y = f2(x, **kwargs)\n        else:\n            y = f(list(x), **kwargs)\n        return y\n\nThe general idea is to implement a named tuple, and use duck typing. In the future, we should look at the named tuple class and do something more similar to that.\n\nclass DiffusionTuple(fastuple):\n    def __new__(cls, *rest):\n        self=super().__new__(cls, *rest)\n        i=0\n        self.x=self[i]\n        if(isinstance(self[i+1],TensorImage)): self.x0=self[i:=i+1]\n        self.t=self[i:=i+1]\n        if(len(self)>i+1): self.y=self[i:=i+1]\n        if(len(self)>i+1): self.pred=self[i:=i+1]\n        return self\n\nA little transform to make our tuple a DiffusionTuple\n\nclass ToDiffusionTuple(ItemTransform):\n    order=100\n    def encodes(self,xy):\n        return DiffusionTuple(*xy[:-1],TensorNoise(xy[-1]))\n\nThis Transform expects y to contain an image, and just replaces it with noise. Our model tries to predict the noise in an image.\n\nclass LabelToNoise(ItemTransform):\n    order=101\n    def encodes(self,xy:DiffusionTuple):\n        y=xy.y\n        xy.y[:]=TensorNoise(torch.randn_like(y))\n        return xy\n\n\ndiff_tuple=LabelToNoise.encodes(DiffusionTuple(img[0].detach().clone(),TensorStep(torch.tensor([[200]])),TensorNoise(img[0].clone())))\n\nWe can access tuple elements by attributes. This is useful when you don’t know what index a particular value is located.\n\ndiff_tuple.x.shape,diff_tuple.t.shape,diff_tuple.y.shape\n\n(torch.Size([3, 320, 480]), torch.Size([1, 1]), torch.Size([3, 320, 480]))\n\n\nNow we have a way to create an image, and convert the label to noise.\n\nnorm.decode(diff_tuple).show(show_noise=True)\n\n<AxesSubplot:title={'center':'TensorStep([[200]])'}>\n\n\n\n\n\nNext, we need to go create a noised image, to pass to our model.\n\nclass Diffusion_P_Sampler():\n    def __init__(self,model,sampling_function):\n        self.device=sampling_function.device\n        self.model=model\n        self.sampling_function=sampling_function\n    # __call__ implemented, but not shown.\n    def iter_noise(self,x_t,ts,t_start):\n        i=0\n        while((ts>0).any()):\n            x,t=x_t[ts>0],ts[ts>0]\n            with autocast(device_type=self.device, dtype=x.dtype):\n                with torch.no_grad(): \n                    e = self.model(x,self.deconvert(t) if i!=0 else t_start)\n                x_t[ts>0]=self.sampling_function(x,e,t,t=t_start if i==0 else None)\n            ts[ts>0]-=1\n            i+=1\n            yield x_t\n\nNext we need to know how much noise to apply to each step.\n\nclass LinearNoiseSchedule:\n    \"Schedule like used in DDPM\"\n    def __init__(self,betas=None,n_steps=None,device='cuda'):\n        if betas is not None: self.n_steps=betas.shape[0]\n        if n_steps is None: self.n_steps=1000\n        if betas is None: self.betas = torch.linspace(0.0001, 0.02, self.n_steps,device=device)\n        self.alphas = 1. - self.betas\n        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n\nLets graph the various values here, in order to see what happens. Pay particularly close attention to alpha_bar as that controls the balance betwen our signal(image) and our noise.\n\nlns=LinearNoiseSchedule()\nplt.plot((lns.betas).cpu())\nplt.plot((lns.alphas).cpu())\nplt.plot((lns.alpha_bar).cpu())\nplt.legend(['betas', 'alphas','alpha_bar'])\n\n<matplotlib.legend.Legend at 0x7f74f2a22e00>\n\n\n\n\n\nNext is DDPM-style Q-sampling. This is pretty much used for all diffusion models, and is the process that takes us from and image to noise.\n\nclass DDPM_Q_Sampling():\n    def __init__(self,predicts_x=False,noise_schedule=LinearNoiseSchedule(),n_steps=1000,device='cuda'):\n        self.device=device\n        self.ns=noise_schedule\n        self.n_steps=n_steps\n        self.t_sched=torch.linspace(0,len(self.ns.alpha_bar)-1,n_steps,dtype=torch.long)[...,None,None,None]\n    def __call__(self,x,es,t):\n        t=self.t_sched[t]\n        a=self.ns.alpha_bar[t].to(device=x.device)\n        signal = (a ** .5)*x\n        noise = (1-a)**.5 * es\n        return signal + noise\n\n\ndiff_trans = DiffusionSamplingTransform(DDPM_Q_Sampling(),lambda x:x)\n\n\nnorm.decode(diff_trans(diff_tuple)).show()\n\n<AxesSubplot:title={'center':'TensorStep([[200]])'}>\n\n\n\n\n\nLets now test so make sure our noise is being generated correctly.\n\nnoise_tuple=LabelToNoise.encodes(DiffusionTuple(img[0].detach().clone(),TensorStep(torch.tensor([[999]])),TensorNoise(img[0].clone())))\n\n\nnorm.decode(diff_trans(noise_tuple)).show(show_noise=True)\n\n<AxesSubplot:title={'center':'TensorStep([[999]])'}>\n\n\n\n\n\nThese are not exactly the same as it is one noising step, but they are fairly close.\n\nis_close(norm.decode(diff_trans(noise_tuple))[0],TensorImage(norm.decode(diff_trans(noise_tuple))[2]),eps=1e-02)\n\nTensorImage(True, device='cuda:0')\n\n\n\nGoing from noise to and image, p_sampling\n\n@patch\ndef __call__(self:DDPM_P_Sampling,x,es,ns_t,t=None):\n    t= self.t_sched[ns_t] if(t is None) else t[...,None,None,None]\n    n=torch.randn_like(x)\n    e,a,b=self._noise_at_t(es,t),self.ns.alphas[t],self.ns.betas[t]\n    signal = (x - e) / (a ** 0.5)\n    noise = b**.5 * n\n    return signal + noise\n@patch\ndef _noise_at_t(self:DDPM_P_Sampling,es,t):\n    eps_coef = (1 - self.ns.alphas[t]) / (1 - self.ns.alpha_bar[t]) ** .5 \n    return eps_coef* es\n\nWe implement DDIM sampling here, as it drastically reduces sampling time from 1000 steps to 50. Just generally helps us keep our sanity when trying to show our results.\n\n@patch\ndef __call__(self:DDIM_P_Sampling,z,es,ns_t,t=None):\n    if(t is None): t=self.t_sched[ns_t]\n    tp1=self.t_sched[ns_t-1]\n    a,a_tp1=self.ns.alpha_bar[t][...,None,None,None],self.ns.alpha_bar[tp1][...,None,None,None]\n    if self.predicts_x: \n        xs=es\n        es=(z - (a)**.5 * xs)/(1-a)**.5\n    else: xs=(z - (1-a)**.5 * es)/ (a ** .5)\n    signal = a_tp1**.5*(xs) \n    noise = (1-a_tp1)**.5*es\n    return signal + noise\n\n\n\nTraining a model\n\npath = untar_data(URLs.CIFAR)\n\n\nm=Unet(dim=192+192//8,channels=3,).cuda()\n\n\nbs=128\nn_steps=1000\ndiffusion_transform = DiffusionSamplingTransform(DDPM_Q_Sampling(),Diffusion_P_Sampler(m,DDPM_P_Sampling()))\ndls=DataBlock((ImageBlock(),\n               ImageBlock(),\n               TransformBlock(type_tfms=[DisplayedTransform(enc=lambda o: TensorStep(o))]),\n               ImageBlock()),\n          n_inp=3,\n          item_tfms=[Resize(32)],\n          batch_tfms=(Normalize.from_stats(*cifar_stats),ToDiffusionTuple,LabelToNoise,diffusion_transform),\n          get_items=get_image_files,\n          get_x=[lambda x:x,lambda x:x,\n                 lambda x: torch.randint(1, n_steps, (1,), dtype=torch.long)],\n          splitter=IndexSplitter(range(bs)),\n).dataloaders(path,bs=bs,val_bs=2*bs)\ndls.show_batch()\n\n\n\n\n\ndef mse_loss_weighted(ys,targ):\n    return torch.mean(targ.w_sched[...,None] * ((ys - targ).flatten(start_dim=1) ** 2))\n\n\ndef snr(at): return at/(1-at)\n\n\ndef continuous_weights(at):\n    weights = -snr(at[1:])/(snr(at[1:])-snr(at[:-1]))\n    return torch.cat((weights[0:1],weights))\n\n\nclass WeightedLinSched(Callback):\n    def after_pred(self):\n        if(not hasattr(self,'ws')):\n            self.ws = continuous_weights(LinearNoiseSchedule().alpha_bar).clip(min=1)\n            self.ws /= self.ws.mean()\n        ts=self.learn.xb[1].flatten()\n        self.learn.yb[0].w_sched=self.ws[ts]\n\n\nlearn = Learner(dls,m,mse_loss_weighted,opt_func=Adam,cbs=[FlattenCallback,WeightedLinSched])\nlearn = learn.to_fp16()\nlearn.fit_flat_cos(10,lr=2e-4,wd=0.)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.049789\n      0.042322\n      04:39\n    \n    \n      1\n      0.044212\n      0.039284\n      04:39\n    \n    \n      2\n      0.041315\n      0.045253\n      04:40\n    \n    \n      3\n      0.040457\n      0.038427\n      04:39\n    \n    \n      4\n      0.039394\n      0.035517\n      04:40\n    \n    \n      5\n      0.039198\n      0.039416\n      04:39\n    \n    \n      6\n      0.039298\n      0.037537\n      04:39\n    \n    \n      7\n      0.038256\n      0.040432\n      04:39\n    \n    \n      8\n      0.038232\n      0.025152\n      04:40\n    \n    \n      9\n      nan\n      0.036510\n      04:38\n    \n  \n\n\n\nnext check show_results\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/unet/index.html",
    "href": "posts/unet/index.html",
    "title": "blog",
    "section": "",
    "text": "from fastai.basics import *\nfrom fastai.vision.models.unet import *\nfrom fastai.torch_basics import *\n\n\nclass SequentialExDict(nn.Sequential):\n    \"Like `nn.Sequential`, but has a dictionary passed along with x.\"\n    def __init__(self, *layers,dict_names=['seq_dict']): \n        super().__init__(*layers)\n        self.dict_names=dict_names\n    def forward(self, x,**kwargs):\n        dicts = getattrs(x,*self.dict_names,default=kwargs)\n        for module in self:\n            for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n            x = module(x)\n        for k,v in zip(self.dict_names,dicts): setattr(x,k,v)\n        return x\n\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    ### Embeddings for $t$\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n        \"\"\"\n        * `n_channels` is the number of dimensions in the embedding\n        \"\"\"\n        super().__init__()\n        self.n_channels = n_channels\n        # First linear layer\n        self.layers = nn.Sequential(\n            nn.Linear(self.n_channels // 4, self.n_channels),\n            nn.ReLU(True),\n            nn.Linear(self.n_channels, self.n_channels)\n        )\n\n    def forward(self, x):\n        # Create sinusoidal position embeddings\n        # [same as those from the transformer](../../transformers/positional_encoding.html)\n        #\n        # \\begin{align}\n        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n        # \\end{align}\n        #\n        # where $d$ is `half_dim`\n        t=torch.tensor(x.seq_dict['t']) if isinstance(x.seq_dict['t'],int) else x.seq_dict['t']\n        t=t.view(t.shape[0])\n        half_dim = self.n_channels // 8\n        emb = math.log(10_000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n\n        # Transform with the MLP\n        emb = self.layers(emb)\n        x.seq_dict['time']=emb\n        return x\n\n\nclass OnKey(nn.Module):\n    def __init__(self,k_in,module,k_out=None):\n        super().__init__()\n        if(k_out is None): k_out=k_in+'_out'\n        self.k_in=k_in\n        self.k_out=k_out\n        self.f=module\n    def forward(self, x):\n        x.seq_dict[self.k_out]=self.f(x.seq_dict[self.k_in])\n        return x\n\n\nclass Stack(nn.Module):\n    def __init__(self,key,f=lambda x:x):\n        super().__init__()\n        self.key,self.f=key,f\n    def forward(self,x):\n        if(self.key not in x.seq_dict): x.seq_dict[self.key]=[]\n        x.seq_dict[self.key]+=[self.f(x)]\n        return x\n\n\nclass Pop(nn.Module):\n    def __init__(self,key,f,clear=True,**kwargs):\n        super().__init__()\n        self.key,self.clear,self.f,self.kwargs=key,clear,f,kwargs\n    def forward(self, x): \n        o=x.seq_dict[self.key]\n        if(is_listy(o)): \n            o =  x.seq_dict[self.key].pop(-1) if(self.clear) else o[-1]\n        elif(self.clear): x.seq_dict[self.key]=None\n        return self.f(x,o,**self.kwargs)\n\n\ndef merge(x,o,dense=False): return torch.cat((x,o),dim=1) if(dense) else x+o.view(o.shape+(1,)*(x.ndim-o.ndim)) \n\n\nclass UnetTime(nn.Module):\n    \"A little Unet with time embeddings\"\n    def __init__(self,dims=[96, 192, 384, 768, 768],img_channels=3,ks=7,stem_stride=4,t_channels=128):\n        super().__init__()\n        i_d=0\n        h=dims[i_d]\n        self.time_emb=TimeEmbedding(t_channels)\n        # Not putting in for loop for ease of understanding arch\n        self.down=SequentialExDict(\n            nn.Conv2d(img_channels,h,ks,1,ks//2),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,stem_stride,1),\n            nn.GroupNorm(1,h),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n            Stack('s',lambda x:x.shape[-2:]),\n            self.basic_block(h,t_channels,ks=ks),\n            self.down_sample(h,(h:=dims[(i_d:=i_d+1)]),2,2,1),\n            Stack('u'),\n        )\n        self.middle=SequentialExDict(\n            self.basic_block(h,t_channels)\n        )\n        self.up=SequentialExDict(\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.up_sample(h*2,(h:=dims[(i_d:=i_d-1)]),4,1,1),\n            self.basic_block(h,t_channels),\n            Pop('u',merge,dense=True),\n            self.down_sample(h*2,img_channels,5,1,2,bias=True),\n            self.basic_block(img_channels,t_channels,bias=True),\n        )\n        self.layers=SequentialExDict(\n            self.time_emb,\n            self.down,\n            self.middle,\n            self.up\n        )\n    @delegates(nn.Conv2d.__init__)\n    def up_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            Pop('s',lambda x,o:F.interpolate(x, size=[oi+1 for oi in o], mode='bilinear')),\n            self.down_sample(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    @delegates(nn.Conv2d.__init__)\n    def down_sample(self,in_channels,out_channels,kernel_size,stride,padding,**kwargs):\n        return SequentialExDict(\n            nn.GroupNorm(1,in_channels),\n            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,**kwargs),\n        )\n    def basic_block(self,channels,time_channels,expansion=4,ks=7,stride=1,pad=None,bias=False):\n        if pad is None: pad=ks//2\n        return SequentialExDict(\n            Stack('r'),\n            nn.Conv2d(channels,channels,ks,padding=pad,bias=bias,stride=stride),\n            nn.GroupNorm(1,channels),\n            nn.Conv2d(channels,channels*expansion,1,bias=bias),\n            OnKey('time',nn.Linear(time_channels,channels*expansion)),\n            Pop('time_out',merge),\n            nn.GELU(),\n            nn.Conv2d(channels*expansion,channels,1,bias=bias),\n            Pop('r',merge),\n        )\n    def forward(self,x,x_o,t):\n        return self.layers(x,t=t)\n\nhello"
  },
  {
    "objectID": "posts/efficient-zero/index.html",
    "href": "posts/efficient-zero/index.html",
    "title": "blog",
    "section": "",
    "text": "MuZero\nLets define our alphabet soup: * h - calculates latent representation of state s from the past observation (board state, or previous frames) * s - state, latent representation of the environment * f - calculates p(policy) and v(value function) from s(state) * p - policy value for each action * v - value function, based on the reward. For atari n-step reward, final reward for board games. * a - some action, sampled from π/p when interacting with the environment, sampled from replay buffer during training. * g - calculates next s(state) and immediate reward(r), recieves previous state and an action as input * r - immediate reward * π - policy, approximately p\n\n[0.1,0.5,0.6,0.7,...]\n\nmuzero can learn rules of game, doesn’t need to be provided with rules.\n\nup,down,left,right\n1=(0.2,0.1,0.1,0.7)\n\n\n\n\nmu.png\n\n\nThe models are trained to predict p(policy),v(value function),r(immediate reward) from the REPLAY buffer.\n\n\nEfficientZero\n\nImprovement 1\n\nshow_image(Image.open('efficient_similarity.png'),figsize=[7,7])\n\n<AxesSubplot:>\n\n\n\n\n\nThe general idea for this one is from this paper: https://arxiv.org/abs/2011.10566\n\nThe “representation” is generated using h from before.\ng then creates s_(t+1) using s_t and a_t for “next state”\nThe “projector” and “predictor” seem to be both thrown away. The projector mapping s_t+1 to a lower deminsional space. This seems to be because we want the predictor to have very few parameters. (lower deminsional space just has less numbers)\nThe “predictor” seems to “bridge” the gap between both branches and allows for smaller batch size training and more stable training. “Exploring Simple Siamese Representation Learning” suggests that the predictor should model the Expected value of what was before the projector, including the difference between O_t and O_(t+1)\nSince the projector and representation is on both sides, and the predictor is sufficiently small, The hidden states for the two s_(t+1) must be similar.\n\n\n\nImprovement 2\nPredicting when a player will lose a point at a particular timestep is hard. Though it is much easier to predict “if” a player will lose a point.\n\n\n\nefficient_future.png\n\n\nPredicting the immediate reward(r) is hard, how do we know 20 steps ahead of time, exactly which time step we will get the reward? Instead they use a LSTM to predict the total reward up until the current time, and train to predict that value instead.\n\n\nImprovement 3\n\n\n\nefficient_replay.png\n\n\nTo use or not to use the replay buffer? - Green, MuZero uses the replayer buffer as is - Yellow, on more recent examples, Efficient Zero will use its policy to predict 1 state for training. - Blue, on less recent examples, Efficient Zero will us its policy for part of the future states. - Red, Efficient Zero can dream up the majority of the policy if observations in the replay buffer are very old.\n\n\nResults\nResults where inconsistent across different applications, so it may be better to think of this as a handful of techniques as opposed to something to always use together. More abalations in the other environments should help to determine what techniques generalize outside of atari and board games.\n\n\n\nefficient_inconsistent.png\n\n\n\n\n\nReferences\n\nMu zero\nEfficient Zero\nSimple Siamese Representations\nYanic Muzero\nYanic Efficient Zero\nImages are from their respective papers."
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "",
    "text": "These are a few questions to think about as you read. Come back and try to answer them at the end to check your understanding.\n\nWhat is the Signal to Noise ratio?\nHow does this paper reformulate the original weighting scheme in terms of SNR?\nAt what signal to noise ratios, is the content of the image developed?\nWhat is P2 weighting?"
  },
  {
    "objectID": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "href": "posts/perception-prioritized-training-of-diffusion-models/Perception Prioritized Training of Diffusion Models.html#prioritized-weight-schedule",
    "title": "Perception Prioritized Training of Diffusion Models",
    "section": "Prioritized Weight Schedule",
    "text": "Prioritized Weight Schedule\nNext we can look at the prioritized weight schedule. The main contribution of the paper. \\(\\lambda_t\\) is our continuous weights from above, k is a constant set to \\(1\\). \\(\\gamma\\) is a hyperparameter that we can control, but it doesn’t work so well at over 2, because “We empirically observed that γ over 2 suffers noise artifacts in the sample because it assigns almost zero weight to the clean-up stage” (quoting paper).\n\\(\\lambda_t^\\prime = \\frac{\\lambda_t}{(k+SNR(t))^\\gamma}\\)\nAnd, here is it in code. \\(\\gamma=0\\) essentially turns the prioritized weighting mechanism off, and gives us the same result as the weighting mechanism in the DDPM paper.\n\nk=1. #set for nice math reasons\ndef prioritized_weights(l,t,g=0.):\n    return l/(k + snr(t))**g\n\nHere we go ahead and generate weights based on linear and cosine noise schedules for different values of \\(\\gamma\\). Notice how it is similar to the results in the image from the paper above.\n\n\n\nplt.xscale('log')\n\nplt.plot(snr(at(Bt)),F.normalize(continuous_weights(at(Bt)),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(at(Bt)),F.normalize(prioritized_weights(continuous_weights(at(Bt)),at(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(at(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Linear Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb']);\n\n\n\n\n\n\nplt.xscale('log')\nplt.xlim(left=1e-8,right=1e4)\nplt.plot(snr(cos_sched(Bt)),F.normalize(continuous_weights(cos_sched(Bt)),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=0.5),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),F.normalize(prioritized_weights(continuous_weights(cos_sched(Bt)),cos_sched(Bt),g=1.),p=1.,dim=0))\nplt.plot(snr(cos_sched(Bt)),torch.full_like(Bt,0.001))\nplt.ylabel('Weights(λ′_t)')\nplt.xlabel('Signal-to-Noise Ratio (SNR)')\nplt.title('Cosine Schedules Weights')\nplt.legend(['Baseline','γ=0.5','γ=1','vlb']);"
  },
  {
    "objectID": "posts/misc/Untitled.html",
    "href": "posts/misc/Untitled.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\ndown_sized= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(down_sized),down_sized.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg.transpose(2,0,1).shape,array([1/3,1/3,1/3]).shape\n\n((3, 320, 463), (3,))\n\n\n\ngray_scale= array([1/3,1/3,1/3])@img[...,None]\nshow_image(gray_scale,cmap='gray') #have to use cmap otherwise uses \"heatmap\" like coloring. \n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[img.shape[0]//4:-img.shape[0]//4,img.shape[1]//4:-img.shape[1]//4])\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(img[:img.shape[0]//2:-1,:img.shape[1]//2])\n\n<AxesSubplot:>\n\n\n\n\n\n\nimagenet_stats\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/misc/Untitled-Copy1.html",
    "href": "posts/misc/Untitled-Copy1.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = array(Image.open((path/'train').ls()[0].ls()[0]))\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\nSchedule: Question, This notebook, How to read a research paper, Presentation/Less formal\n\nnp.random.choice(np.arange(14),8,replace=False)\n\narray([2, 5, 1, 0, 9, 8, 6, 7])\n\n\n\nimg.shape\n\n(320, 463, 3)\n\n\n\nimg.shape[0]//2,img.shape[1]//2\n\n(160, 231)\n\n\n\nex=img[:img.shape[0]//2,:img.shape[1]//2]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nimg.shape[0]//4,-img.shape[0]//4\n\n(80, -80)\n\n\n\nex=img[img.shape[0]//4:-img.shape[0]//4,\n       img.shape[1]//4:-img.shape[1]//4]\nshow_image(ex)\n\n<AxesSubplot:>\n\n\n\n\n\n\nex.shape,img.shape\n\n((160, 232, 3), (320, 463, 3))\n\n\n\nshow_image(img)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlist(range(10,0,-1))\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\n\nshow_image(img),img.shape\n\n(<AxesSubplot:>, (320, 463, 3))\n\n\n\n\n\n\nshow_image(img[:-1:2,:-1:2]),img[:-1:2,:-1:2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nshow_image(img[1::2,1::2]),img[1::2,1::2].shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\nimg[:-1:2,:-1:2]//2+img[1::2,1::2]//2\n\n\nimg[::2,::2]//2+img[1::2,1::2]//2\n\nValueError: operands could not be broadcast together with shapes (160,232,3) (160,231,3) \n\n\n\nimg[1::2,1::2].shape\n\n(160, 231, 3)\n\n\n\nex= img[:-1:2,:-1:2]//2+img[1::2,1::2]//2\nshow_image(ex),ex.shape\n\n(<AxesSubplot:>, (160, 231, 3))\n\n\n\n\n\n\narray([1/3,1/3,1/3])\n\narray([0.33333333, 0.33333333, 0.33333333])\n\n\n\n(array([1/3,1/3,1/3])@img[...,None]).shape\n\n(320, 463, 1)\n\n\n\nex1.max(),ex2.max()\n\n(765, 255.0)\n\n\n\nnp.\n\n\nex1= (array([1/2,1/2,1/2])@img[...,None]).clip(0,255)\nex2= (array([1/3,1/3,1/3])@img[...,None]).clip(0,255)\nshow_image(ex1,cmap='gray'),show_image(ex2,cmap='gray')\n\n(<AxesSubplot:>, <AxesSubplot:>)\n\n\n\n\n\n\n\n\n\nnorm_tfm=Normalize.from_stats(*imagenet_stats,cuda=False)\ndef show_norm(img): show_images((norm_tfm.decode(img).clamp(0,1)),nrows=3)\n\n\nnorm_img = norm_tfm(TensorImage(img.transpose(2,0,1)).float()[None]/255)\n\n\nnoise= torch.randn_like(norm_img)\n\n\nAs = torch.linspace(0,1,12)[...,None,None,None]; As.squeeze()\n\ntensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n        0.8182, 0.9091, 1.0000])\n\n\n\n(As)**.5*norm_img\n\n\n(1-As**.5).squeeze()\n\ntensor([1.0000, 0.6985, 0.5736, 0.4778, 0.3970, 0.3258, 0.2615, 0.2023, 0.1472,\n        0.0955, 0.0465, 0.0000])\n\n\n\n((1-As)**.5).squeeze()\n\ntensor([1.0000, 0.9535, 0.9045, 0.8528, 0.7977, 0.7385, 0.6742, 0.6030, 0.5222,\n        0.4264, 0.3015, 0.0000])\n\n\n\nshow_norm((As)**.5*norm_img+(1-As)**.5*noise)\n\n\n\n\n\nshow_norm((As)**.5*norm_img+(1-As**.5)*noise)\n\n\n\n\n\nAs.squeeze(),As.shape\n\n(tensor([0.0000, 0.0909, 0.1818, 0.2727, 0.3636, 0.4545, 0.5455, 0.6364, 0.7273,\n         0.8182, 0.9091, 1.0000]),\n torch.Size([12, 1, 1, 1]))\n\n\n\nnorm_img.shape\n\ntorch.Size([1, 3, 320, 463])\n\n\n\nshow_norm((As)**.5*norm_img)\n\n\n\n\n\n1-(As)**.5\n\ntensor([[[[1.0000]]],\n\n\n        [[[0.6985]]],\n\n\n        [[[0.5736]]],\n\n\n        [[[0.4778]]],\n\n\n        [[[0.3970]]],\n\n\n        [[[0.3258]]],\n\n\n        [[[0.2615]]],\n\n\n        [[[0.2023]]],\n\n\n        [[[0.1472]]],\n\n\n        [[[0.0955]]],\n\n\n        [[[0.0465]]],\n\n\n        [[[0.0000]]]])\n\n\n\nshow_norm((1-As)**.5*noise)"
  },
  {
    "objectID": "posts/misc/LSH with random projection.html",
    "href": "posts/misc/LSH with random projection.html",
    "title": "blog",
    "section": "",
    "text": "Problem setup\n\npath = untar_data(URLs.IMAGEWOOF_320)\n\n\nmodel = resnet50(pretrained=True).cuda()\n\n/home/molly/miniconda3/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/molly/miniconda3/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\ndls = DataBlock((ImageBlock,CategoryBlock),\n                splitter=IndexSplitter([0,2]),\n                item_tfms=Resize(320),\n                batch_tfms=aug_transforms(),\n                get_items=get_image_files).dataloaders(path)\n\n\ndef model_call(model,x):\n    #same as model.forward\n        x = model.conv1(x)\n        x = model.bn1(x)\n        x = model.relu(x)\n        x = model.maxpool(x)\n\n        x = model.layer1(x)\n        x = model.layer2(x)\n        x = model.layer3(x)\n        x = model.layer4(x)\n\n        #x = model.avgpool(x)\n        #x = torch.flatten(x, 1)\n        #x = model.fc(x)\n        return x\n\n\nmodel.fc\n\nLinear(in_features=2048, out_features=1000, bias=True)\n\n\n\n#model_call(model,dls.one_batch()[0])\n\nGoing to use a relatively small number of buckets, could use much more if we wanted to have less total collisions.\n\nnbits=8\nbuckets=2**nbits\nbits=torch.arange(0,8,device='cuda')\nnbits,buckets,2.**bits\n\n(8,\n 256,\n tensor([  1.,   2.,   4.,   8.,  16.,  32.,  64., 128.], device='cuda:0'))\n\n\n\nbits\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n\n\n\ntorch.sum(2**bits)\n\ntensor(255, device='cuda:0')\n\n\n\n\nHow would we calculate the maximum value that we can represent?\n\n[64, 2048, 10, 10]\n\n[64, 2048, 10, 10]\n\n\n\n[64, 8, 10, 10]\n\n[64, 8, 10, 10]\n\n\n\n(2.**bits).sum()\n\ntensor(255., device='cuda:0')\n\n\n\n#dividing by nbits gets us close to 1\nprojector=torch.randn(2048,nbits).cuda()/nbits**0.5\ntorch.linalg.norm(projector,dim=1)\n\ntensor([1.0337, 1.0348, 1.0620,  ..., 1.0139, 1.1947, 1.1155], device='cuda:0')\n\n\n\nnbits\n\n8\n\n\n\n[0,1,0,0,0], [0,0,1,0,0],[1,0,0,0,0]\n\n([0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0])\n\n\n\nb=dls.one_batch()[0]\n\n\ndls.show_batch()\n\n\n\n\n\nys=model_call(model,b)\n\n\n(ys[:,:,None]*projector[None,...,None,None]).shape\n\ntorch.Size([64, 2048, 8, 10, 10])\n\n\n\nys.shape,projector.shape\n\n(torch.Size([64, 2048, 10, 10]), torch.Size([2048, 8]))\n\n\n\ntorch.einsum('ijkl,jm->imkl',ys,projector).shape\n\ntorch.Size([64, 8, 10, 10])\n\n\n\n(torch.einsum('ijkl,jm->ijmkl',ys,projector)==(ys[:,:,None]*projector[None,...,None,None])).any()\n\nTensorImage(True, device='cuda:0')\n\n\n\n\nHow would you calculate the dot product for the second dim and first dim? (2048,10,10)@(2048,8)=(8,10,10)\n\nUsing Einstein notation?\n\nprojection=torch.einsum('bcxy,cz->bzxy',ys,projector)\n\n\nprojection\n\nTensorImage([[[[-6.2792e+00,  1.3167e-01, -9.4779e-02,  ..., -2.4656e+00,\n                -5.0584e+00, -1.3296e+00],\n               [ 8.1754e+00,  9.6036e+00, -4.4357e+00,  ..., -4.6396e+00,\n                -7.8996e+00, -2.8259e+00],\n               [ 4.4467e+00, -2.1758e+00, -6.7790e+00,  ...,  4.6684e+00,\n                 1.3650e+01,  1.0253e+01],\n               ...,\n               [ 6.3236e-01, -3.8342e+00, -1.3020e+01,  ..., -4.5836e+00,\n                -3.6719e+00,  4.8875e+00],\n               [-9.5392e+00, -4.3794e+00, -4.9069e+00,  ...,  4.2450e+00,\n                -7.0259e+00, -6.6205e+00],\n               [-2.8886e+00, -2.2777e+00, -3.5640e+00,  ..., -1.4655e+00,\n                -5.7932e+00, -2.0072e+00]],\n\n              [[ 9.2835e+00,  7.7462e+00,  6.1908e+00,  ...,  2.9067e+00,\n                 8.0073e+00,  1.1350e+01],\n               [ 1.1556e+01,  1.3565e+01,  9.3358e+00,  ...,  3.8952e+00,\n                 8.9265e+00,  1.1539e+01],\n               [ 9.3274e+00,  7.3811e+00,  1.1513e+01,  ...,  1.6263e+01,\n                 2.4063e+01,  1.9001e+01],\n               ...,\n               [ 6.8415e+00,  5.9298e+00,  7.7832e+00,  ...,  2.8651e+01,\n                 4.1376e+00, -4.5388e+00],\n               [ 5.3508e+00,  1.1588e+01,  1.2607e+01,  ...,  1.5089e+01,\n                 7.7975e+00,  5.8521e+00],\n               [ 9.5215e+00,  6.9118e+00,  1.0100e+01,  ...,  8.9737e+00,\n                 6.7271e+00,  8.5211e+00]],\n\n              [[-3.1688e+00, -6.4222e+00, -2.8567e+00,  ..., -3.6577e+00,\n                -5.9065e+00, -2.9208e+00],\n               [-6.0068e+00, -9.4032e+00, -2.8770e+00,  ..., -6.6112e-01,\n                -5.0611e+00, -8.4874e+00],\n               [-4.4515e+00, -1.0154e+01, -1.2417e+00,  ..., -1.5574e-01,\n                -1.1492e+00,  7.6063e+00],\n               ...,\n               [ 1.5988e+01,  1.3046e+01,  6.4920e+00,  ..., -4.4126e+00,\n                 1.4175e+00,  1.2447e+01],\n               [ 1.9409e+01,  1.7188e+01,  6.8644e+00,  ..., -3.7321e+00,\n                -2.8769e+00,  1.6388e+00],\n               [ 1.6584e+01,  1.4844e+01,  9.3730e+00,  ..., -8.5079e-01,\n                -6.0974e+00, -3.7285e+00]],\n\n              ...,\n\n              [[-7.1310e+00,  6.5308e-01,  2.6390e+00,  ..., -4.4481e+00,\n                -7.0668e+00, -9.1038e+00],\n               [ 2.2496e+00,  5.2074e+00,  1.0821e+01,  ..., -3.1807e+00,\n                 8.5318e-01, -1.1862e+00],\n               [-5.3933e+00,  3.0677e+00,  6.6602e+00,  ..., -4.9860e+00,\n                 1.2720e+00,  1.7180e+01],\n               ...,\n               [-3.8841e+00, -1.0031e+00,  1.4554e+01,  ...,  1.7657e+01,\n                 8.1946e+00,  8.0810e+00],\n               [-7.9001e+00, -9.9230e+00,  1.9062e+00,  ...,  2.0380e+00,\n                -1.1232e+00,  2.4761e+00],\n               [-1.1327e+01, -1.5734e+01, -1.2547e+01,  ..., -1.1905e+00,\n                 3.4819e+00,  1.7690e+00]],\n\n              [[-5.7783e+00,  1.5963e-01,  3.8980e+00,  ...,  1.9761e+00,\n                 1.8847e+00,  2.4780e-01],\n               [-1.7248e+00, -3.0383e+00, -6.9456e+00,  ...,  8.2464e+00,\n                -2.3109e+00, -2.6400e+00],\n               [ 3.1076e-01,  1.4344e+00, -1.1225e+01,  ...,  5.7751e+00,\n                 1.9494e+00,  3.9672e+00],\n               ...,\n               [-1.1729e+01, -1.0031e+01, -2.1186e+01,  ..., -7.7848e+00,\n                -1.1644e+01, -1.2071e+01],\n               [-1.5569e+01, -2.1363e+01, -2.6422e+01,  ..., -1.6087e+01,\n                -1.9381e+01, -2.5736e+01],\n               [-2.8877e+01, -2.3298e+01, -1.9927e+01,  ..., -1.1429e+01,\n                -1.3280e+01, -2.2741e+01]],\n\n              [[-9.2639e+00, -3.3929e+00,  9.7048e+00,  ..., -3.8852e+00,\n                 8.4170e-01,  1.7012e+00],\n               [-2.6492e+00,  3.2599e+00,  3.2146e+01,  ...,  2.9210e+00,\n                 2.3555e-01,  3.9248e-01],\n               [ 1.1340e+01,  1.2774e+01,  2.0421e+01,  ..., -7.9307e+00,\n                -1.6897e+01, -4.7462e+00],\n               ...,\n               [ 9.6164e+00,  1.3184e+01,  1.3542e+01,  ...,  7.6629e+00,\n                 1.3888e+01,  1.5293e+01],\n               [ 1.0481e+01,  1.4385e+01,  1.1060e+01,  ...,  4.2098e+00,\n                 1.1673e+01,  1.5041e+01],\n               [ 4.5687e+00,  1.4530e+01,  1.3015e+01,  ...,  9.6600e+00,\n                 1.2996e+01,  1.3536e+01]]],\n\n\n             [[[ 9.5124e+00,  7.0540e+00,  1.0731e+01,  ..., -1.1898e-01,\n                -4.8526e+00, -1.6273e+01],\n               [ 1.6990e-01, -1.3911e-01,  6.2190e+00,  ..., -6.3489e-01,\n                -1.0237e+01, -8.5378e+00],\n               [-3.7613e+00, -4.9234e+00, -2.4806e+00,  ..., -2.5140e+00,\n                -7.3274e+00, -7.1875e+00],\n               ...,\n               [-6.5095e+00,  1.7462e+00, -1.9302e-01,  ..., -9.5685e+00,\n                -7.6806e+00, -8.3060e-01],\n               [-9.2985e-01, -6.4332e+00,  6.1271e+00,  ..., -1.7758e+01,\n                -1.5903e+01, -1.8803e+01],\n               [ 9.8972e+00,  1.4579e+01,  8.9024e+00,  ..., -2.1343e+01,\n                -3.3487e+01, -2.7978e+01]],\n\n              [[ 9.7840e+00,  1.3461e+01,  2.7833e+01,  ...,  5.9922e+00,\n                 1.1757e+01,  2.2186e+01],\n               [ 6.5224e+00,  6.7404e+00,  1.3893e+01,  ...,  9.1900e+00,\n                 1.8095e+01,  1.9578e+01],\n               [ 8.7155e-01,  1.9546e+00,  8.3535e+00,  ...,  1.5843e+01,\n                 1.7649e+01,  1.1515e+01],\n               ...,\n               [-1.0384e+01, -1.4986e+01,  5.2173e+00,  ...,  3.0871e+01,\n                 4.3548e+01,  4.0713e+01],\n               [ 1.8919e+01,  1.2526e+01,  2.4128e+00,  ...,  2.9831e+01,\n                 4.2634e+01,  3.9587e+01],\n               [ 3.8178e+01,  3.2932e+01,  4.6763e+00,  ...,  3.2836e+01,\n                 3.7258e+01,  4.4807e+01]],\n\n              [[-6.7811e+00, -1.3298e+01, -2.0355e+00,  ..., -4.4833e+00,\n                -6.0731e+00, -5.2660e+00],\n               [-4.5190e+00, -4.7397e+00, -4.3055e+00,  ..., -8.3795e+00,\n                -8.6657e+00, -3.1813e+00],\n               [-4.0083e+00, -5.4050e+00, -8.3738e+00,  ..., -8.1771e+00,\n                -5.7712e+00,  3.3252e+00],\n               ...,\n               [-2.0961e+01, -1.7522e+01, -1.9436e+01,  ..., -3.9490e+00,\n                 3.5565e+00,  4.4478e+00],\n               [-2.4070e+01, -2.2667e+01, -7.1590e+00,  ..., -3.1348e+00,\n                 5.0762e+00,  1.2960e+01],\n               [-7.6899e+00, -8.6212e+00,  6.7536e-01,  ..., -2.6530e-01,\n                 1.1745e+01,  1.5273e+01]],\n\n              ...,\n\n              [[ 9.2374e-01,  3.2793e+00, -6.5183e+00,  ...,  4.7281e-01,\n                -2.2870e+00,  1.3027e+00],\n               [ 4.0763e+00,  3.0901e+00,  4.1876e+00,  ...,  2.0954e+00,\n                -9.8625e-01, -3.1275e+00],\n               [ 9.6910e-01,  2.8828e-01,  7.4595e-01,  ...,  1.0479e+01,\n                 8.2900e+00,  4.0798e+00],\n               ...,\n               [ 1.2553e+01,  1.4372e+01,  1.3448e+01,  ...,  1.2941e+01,\n                 7.6967e+00,  1.1426e+01],\n               [ 1.1466e+01,  1.4855e+01,  2.0851e+01,  ...,  3.2110e+01,\n                 4.5123e+01,  4.3206e+01],\n               [ 1.0600e+01,  7.3636e+00,  1.0523e+01,  ...,  1.6592e+01,\n                 2.7320e+01,  5.5086e+01]],\n\n              [[ 2.4515e+00,  1.0092e+01,  1.2336e+01,  ...,  6.2933e+00,\n                 4.9184e+00,  9.3574e+00],\n               [ 1.2444e+00,  3.0530e+00,  7.1421e+00,  ...,  5.0260e+00,\n                 6.4202e+00, -1.7965e+00],\n               [-2.0972e+00,  3.1371e-01,  8.9068e-01,  ...,  2.6291e+00,\n                 3.4665e+00, -1.9193e+00],\n               ...,\n               [ 1.5958e+00, -6.7022e+00, -5.6058e+00,  ...,  6.6774e+00,\n                 4.8079e+00,  3.4565e+00],\n               [-4.5736e-02, -1.8359e+01, -7.8809e+00,  ...,  6.7820e+00,\n                 3.7120e+00,  4.0987e+00],\n               [ 1.4360e+00, -5.5607e+00,  6.2634e-01,  ..., -1.5635e+00,\n                -1.0811e+01, -2.0711e+00]],\n\n              [[ 2.8068e+00,  3.3115e+00,  7.0982e+00,  ...,  2.5611e+00,\n                -1.5446e+00, -7.0554e+00],\n               [ 3.2848e+00, -2.6821e+00,  4.3762e+00,  ...,  1.6894e+00,\n                -1.6904e+00, -6.2212e+00],\n               [-4.2415e-01, -3.8746e+00, -4.9796e-01,  ..., -3.5530e-01,\n                 2.7174e+00,  1.3833e+01],\n               ...,\n               [ 6.1781e+00,  3.1638e+00,  7.9841e+00,  ..., -9.4526e+00,\n                -3.6841e+00, -1.4651e+01],\n               [-6.8839e+00, -3.6912e+00, -3.6544e+00,  ..., -1.6174e+01,\n                -2.0845e+01, -2.3711e+01],\n               [-3.4264e+00, -5.4144e+00, -1.1610e-01,  ..., -1.2922e+01,\n                -1.8294e+01, -2.1884e+01]]],\n\n\n             [[[ 1.6726e+01,  1.7428e+01,  4.2822e+00,  ...,  1.3308e+01,\n                 7.7680e+00,  1.4643e-01],\n               [ 1.0972e+01,  1.4134e+01,  4.4387e+00,  ...,  1.5983e+01,\n                 7.5606e+00,  2.1118e+00],\n               [ 8.8190e+00,  7.4110e+00,  1.1246e+01,  ...,  1.3696e+01,\n                 3.8566e+00,  6.2899e+00],\n               ...,\n               [ 1.2071e+01,  1.0601e+01,  1.6731e+00,  ..., -1.3564e+01,\n                -1.3250e+01,  3.3810e-01],\n               [ 1.8464e+00,  7.9721e+00,  3.5149e+00,  ..., -1.6235e+01,\n                -1.0623e+01,  8.7157e+00],\n               [ 8.7276e+00, -9.2056e-01,  7.5868e-01,  ..., -1.1893e+01,\n                 2.7289e+00,  2.7667e+01]],\n\n              [[ 6.6531e+00,  1.0010e+01,  2.2753e+01,  ...,  1.4925e+01,\n                 1.3476e+01,  1.6543e+01],\n               [ 4.5088e+00,  1.9536e+01,  3.0127e+01,  ...,  1.1684e+01,\n                 9.8105e+00,  3.2875e+00],\n               [ 4.2340e+00,  5.5834e+00,  1.8127e+01,  ..., -3.0256e+00,\n                 2.6275e+00, -7.1714e-01],\n               ...,\n               [ 1.6779e+01,  2.9478e+00,  1.5335e+01,  ...,  3.1999e+01,\n                 1.1919e+01,  8.4015e+00],\n               [ 1.5926e+01,  2.0441e+00,  7.7098e+00,  ...,  1.2695e+01,\n                 1.0315e+01,  5.9256e+00],\n               [ 4.5396e+00, -3.9510e+00,  3.0082e+00,  ...,  4.2423e+00,\n                 1.4620e+00, -8.1835e-01]],\n\n              [[-1.7676e+01, -1.7258e+01, -1.1684e+01,  ..., -1.0038e+01,\n                -7.8505e-02,  5.6030e-01],\n               [-1.2038e+01, -1.0079e+01,  3.9319e-01,  ..., -1.3313e+00,\n                -2.6005e+00,  6.5192e+00],\n               [-5.7970e+00, -1.0169e+01, -1.4580e+01,  ...,  3.7920e+00,\n                 2.1885e+00, -2.9092e+00],\n               ...,\n               [-9.1141e+00, -1.7944e+01, -2.5657e+00,  ..., -2.0504e+01,\n                -8.4545e+00,  3.7092e+00],\n               [-4.9958e+00, -9.3027e+00, -7.6127e+00,  ..., -1.3733e+01,\n                -7.3914e+00,  1.0248e+01],\n               [-2.4956e+00, -4.2638e+00, -3.6001e+00,  ..., -5.5006e-01,\n                 2.5708e+00,  5.7056e+00]],\n\n              ...,\n\n              [[ 4.2114e-01, -7.9980e+00, -8.6686e+00,  ..., -4.2628e+00,\n                 8.8063e+00, -9.2941e-02],\n               [-8.4785e+00, -1.7714e+01, -5.5977e-01,  ...,  9.8289e-01,\n                 1.2428e+01,  9.1162e+00],\n               [-9.5216e+00, -2.6300e+01, -1.7297e+01,  ...,  4.7637e+00,\n                 6.2845e+00,  4.5398e+00],\n               ...,\n               [ 1.2768e+00, -6.3151e+00, -1.0408e+00,  ...,  7.4808e-01,\n                -1.0534e+00,  4.4392e+00],\n               [ 3.0409e+00, -1.3620e+00,  3.0350e+00,  ..., -9.6885e+00,\n                -2.5018e+00, -3.8327e+00],\n               [-1.6695e+01, -1.1960e+01, -6.9895e+00,  ..., -8.2594e+00,\n                -1.4068e+01, -2.0667e+01]],\n\n              [[ 5.1652e+00,  1.0395e+01,  7.4535e+00,  ...,  8.6804e+00,\n                 4.5424e+00,  1.4589e+01],\n               [ 4.9551e+00,  1.8278e+01,  2.8135e+01,  ...,  2.0059e+01,\n                 1.2688e+01,  1.1655e+01],\n               [ 8.6485e+00,  2.4904e+01,  4.3337e+01,  ...,  1.4686e+01,\n                 1.6079e+01,  2.0526e+01],\n               ...,\n               [ 3.6101e+00, -3.5364e+00,  7.2125e+00,  ..., -8.2165e-01,\n                -7.4522e+00,  6.1776e+00],\n               [-3.6109e+00, -6.7193e+00,  1.2664e+01,  ..., -5.5758e+00,\n                -7.2992e+00,  3.7080e+00],\n               [-9.0229e-01, -2.1786e+00,  7.6342e+00,  ..., -7.0221e+00,\n                -9.6212e+00,  5.1720e+00]],\n\n              [[-1.1960e+01, -1.1723e+01, -2.1839e+01,  ..., -7.9723e+00,\n                 9.2299e+00,  5.2175e+00],\n               [-1.4355e+01, -2.1408e+01, -3.0463e+01,  ...,  3.5703e+00,\n                 4.7135e+00,  7.2011e+00],\n               [-1.3361e+01, -2.6255e+01, -3.8794e+01,  ...,  2.9431e+00,\n                -4.4089e+00,  1.5832e-01],\n               ...,\n               [-4.0332e+00, -1.0877e+01, -1.0693e+01,  ..., -7.4630e+00,\n                -1.3602e-01,  1.2742e+00],\n               [-3.7040e+00, -2.8553e+00, -8.7539e+00,  ...,  6.7736e-01,\n                 2.3848e+00, -1.8044e-01],\n               [-8.3037e+00, -1.0215e+01, -8.9308e+00,  ...,  1.6606e+01,\n                 1.4776e+00, -7.1054e+00]]],\n\n\n             ...,\n\n\n             [[[ 3.0742e+01,  2.0617e+01,  3.4883e+00,  ..., -9.0017e+00,\n                -1.6916e+00,  3.9473e-01],\n               [ 3.5029e+01,  2.8505e+01,  1.3211e+01,  ..., -5.5471e+00,\n                -6.9257e+00, -2.2475e+00],\n               [ 1.4213e+01,  2.3229e+01,  1.6051e+01,  ..., -1.8002e-01,\n                -3.7035e-01,  6.3750e+00],\n               ...,\n               [ 8.4374e-01,  4.3312e+00, -3.1439e-01,  ..., -4.4065e+00,\n                -1.1842e+00,  3.8927e+00],\n               [-1.3196e+00, -4.4678e-01, -2.1221e+00,  ..., -2.8409e+00,\n                -4.0868e+00,  2.5064e+00],\n               [-3.6652e-01, -9.4469e-01, -9.2601e+00,  ..., -3.7837e+00,\n                -1.7367e+00,  8.7911e+00]],\n\n              [[ 2.1170e+01,  1.9605e+01,  8.4797e+00,  ...,  1.7911e+01,\n                 8.9692e+00,  6.8814e+00],\n               [ 1.5301e+01,  1.3120e+01,  1.9084e+00,  ...,  1.5672e+01,\n                 1.0492e+01,  9.9011e+00],\n               [ 4.6515e+00, -1.0032e+00,  1.7698e+00,  ...,  9.0264e+00,\n                 7.3372e+00,  2.9377e+00],\n               ...,\n               [ 9.4733e+00,  1.7173e+01,  1.2714e+01,  ...,  2.1929e+01,\n                 1.6661e+01,  1.2190e+01],\n               [ 4.9162e+00,  1.5067e+01,  1.7427e+01,  ...,  1.6764e+01,\n                 1.1263e+01,  1.2861e+01],\n               [ 3.4370e+00,  1.0124e+01,  1.2676e+01,  ...,  2.0788e+01,\n                 1.6364e+01,  2.2770e+01]],\n\n              [[ 1.7073e+01,  1.2411e+01,  3.9940e+00,  ..., -2.5094e-01,\n                 1.0745e+00,  2.0860e+00],\n               [ 9.7372e+00,  2.9299e+00, -5.5394e+00,  ...,  1.9889e+00,\n                 3.3939e+00, -1.5617e+00],\n               [-2.5642e-01, -1.1352e+00, -8.8535e+00,  ..., -4.5452e+00,\n                -1.9397e+00, -7.2823e+00],\n               ...,\n               [-2.0316e+00, -2.9228e+00, -4.4708e+00,  ..., -3.9330e+00,\n                -1.1184e+01, -9.1596e+00],\n               [ 4.0511e-01, -5.1108e-01, -5.8130e+00,  ..., -3.4118e+00,\n                -4.2866e+00, -2.3001e+00],\n               [-3.2900e+00, -7.9968e+00, -4.4886e+00,  ..., -4.5846e+00,\n                -5.1580e+00, -5.1870e+00]],\n\n              ...,\n\n              [[-1.0441e+01,  3.6222e-01,  5.7083e+00,  ..., -8.7860e-01,\n                -5.9626e+00, -2.4280e+00],\n               [-3.2910e-01,  3.1008e+00,  1.1946e+01,  ...,  2.7110e+00,\n                -5.6768e+00, -6.1909e+00],\n               [-5.4021e+00,  1.0762e+01,  2.7438e+01,  ...,  5.8677e+00,\n                -1.0364e-01, -2.4942e+00],\n               ...,\n               [ 7.0567e-01, -3.1743e+00, -1.8512e+00,  ...,  4.5055e+00,\n                 1.3033e+00, -8.2645e+00],\n               [-3.0940e+00,  2.9549e+00,  1.0386e+01,  ..., -4.2258e+00,\n                -8.3496e+00, -6.9864e+00],\n               [-5.8167e+00,  5.5948e+00,  2.3419e+01,  ..., -6.6175e+00,\n                -1.2134e+01, -7.1784e+00]],\n\n              [[-1.5039e+01, -5.6994e+00,  6.0646e+00,  ...,  4.8761e+00,\n                -1.6647e+00, -3.3653e+00],\n               [-1.2232e+01,  3.2679e-01,  2.1025e+00,  ...,  4.5207e+00,\n                -6.3115e-01,  4.4318e-01],\n               [-1.6360e+01, -4.2183e-01, -2.9451e+00,  ..., -3.2819e+00,\n                -7.8153e+00, -3.9869e+00],\n               ...,\n               [-9.9416e+00,  1.5021e+01,  1.2003e+01,  ..., -5.2349e+00,\n                 2.6565e+00, -2.1975e+00],\n               [-1.3382e+01,  2.7150e+00,  7.4891e+00,  ..., -5.7495e+00,\n                 8.4802e+00,  6.6035e+00],\n               [-1.3810e+01,  7.6381e+00,  5.1958e+00,  ..., -1.8646e+00,\n                 3.0005e+00,  9.3771e-01]],\n\n              [[ 1.3810e+01,  1.7808e+01,  8.6061e+00,  ...,  1.1546e+01,\n                 8.8377e+00,  8.1417e+00],\n               [ 1.1785e+01,  1.7363e+01, -4.3577e+00,  ...,  3.6378e+00,\n                 1.6090e+01,  1.5553e+01],\n               [ 1.0965e+01,  2.3075e+00, -2.3853e+00,  ..., -8.7184e+00,\n                 1.7130e+01,  1.8968e+01],\n               ...,\n               [ 2.2361e+00,  2.7280e+00,  6.5013e+00,  ...,  5.6273e+00,\n                 8.9686e+00,  9.0201e+00],\n               [ 5.5273e+00,  3.4220e+00, -9.4277e+00,  ...,  8.6742e-01,\n                 7.3521e+00,  1.1186e+01],\n               [ 4.3832e+00, -6.7473e-01, -1.1554e+01,  ...,  6.7014e+00,\n                 8.0967e+00,  7.0652e+00]]],\n\n\n             [[[-2.6913e+01, -2.9568e+01, -8.9120e+00,  ..., -3.6777e+00,\n                -9.5993e+00, -1.3907e+01],\n               [-3.3114e+01, -4.0868e+01, -3.1856e+01,  ..., -2.1573e+00,\n                -7.5814e+00, -1.1474e+01],\n               [-2.7869e+01, -3.9983e+01, -2.6464e+01,  ...,  6.0545e+00,\n                -6.4322e+00, -6.3355e+00],\n               ...,\n               [-1.6259e+01, -1.7102e+01, -9.8119e+00,  ..., -2.6094e+00,\n                -9.1046e+00, -4.5698e+00],\n               [-4.2792e+00, -1.4512e+01, -7.5014e+00,  ..., -3.2948e-01,\n                -8.0459e+00, -5.2389e+00],\n               [-2.2665e-03, -1.0965e+01, -7.7879e+00,  ..., -9.6266e+00,\n                -1.0545e+01, -7.5697e-01]],\n\n              [[ 1.5671e+00,  1.2056e+01,  1.2069e+01,  ...,  2.4591e+01,\n                 3.2034e+01,  3.6939e+01],\n               [-3.8125e+00,  3.2298e+00,  1.2571e+01,  ...,  2.2087e+01,\n                 2.9005e+01,  1.8920e+01],\n               [ 7.9042e-01,  2.6120e+01,  3.5221e+01,  ...,  3.8826e+00,\n                 1.6314e+01,  2.7712e+01],\n               ...,\n               [-2.7849e+00, -1.0391e+01, -5.8262e+00,  ...,  6.0313e+00,\n                 6.1854e+00,  7.3942e+00],\n               [ 8.9106e+00,  6.6072e+00,  3.7478e+00,  ...,  4.4233e+00,\n                -1.8716e-01, -3.8388e-01],\n               [ 2.1626e+01,  1.3852e+01,  1.0201e+01,  ...,  3.2672e+00,\n                -1.1369e+00,  3.3530e+00]],\n\n              [[ 8.4911e+00,  8.4341e+00,  4.6288e+00,  ...,  1.3576e+01,\n                -7.2182e+00, -1.2758e+01],\n               [ 2.1120e+01,  1.2484e+01,  8.6090e-02,  ..., -1.1351e+00,\n                -1.4208e+01, -1.1975e+01],\n               [-3.1700e+00, -4.0954e+00, -1.6492e+00,  ..., -1.0604e+01,\n                -9.4700e+00,  6.6750e-01],\n               ...,\n               [ 2.9052e+00,  6.3958e+00,  1.3706e-01,  ...,  5.7390e+00,\n                 3.1581e+00, -6.0765e-01],\n               [-3.8417e+00, -4.2816e+00, -5.9917e-01,  ...,  6.7638e+00,\n                 5.0139e+00,  5.6882e+00],\n               [-1.8144e+01, -1.2465e+01, -4.2393e+00,  ...,  4.2922e+00,\n                 6.0590e+00,  9.7644e+00]],\n\n              ...,\n\n              [[-7.3576e+00,  1.0356e+01,  3.1724e-01,  ...,  1.8843e+01,\n                 1.8283e+01,  1.0810e+01],\n               [-7.3645e+00,  5.2899e+00,  1.3292e+00,  ...,  1.3994e+01,\n                 8.5479e+00, -4.9072e+00],\n               [-9.9293e+00,  2.7770e+00, -8.8405e+00,  ...,  3.1427e+00,\n                 1.9661e+00,  8.4572e+00],\n               ...,\n               [-8.4440e+00, -5.7747e+00, -7.7229e+00,  ...,  8.1245e+00,\n                -3.8143e+00, -9.9999e+00],\n               [ 6.4093e-01,  5.9969e+00, -6.3158e+00,  ...,  1.4593e+00,\n                -5.8362e+00, -1.3630e+01],\n               [ 4.7849e+00,  4.6974e+00,  3.1298e+00,  ..., -4.9041e+00,\n                -1.0216e+01, -1.2580e+01]],\n\n              [[-5.8014e+00, -1.0372e+01,  1.0254e+01,  ..., -4.6294e-01,\n                 1.8217e+01,  1.0038e+01],\n               [ 5.7797e+00,  9.2750e+00,  2.1096e+01,  ...,  8.8768e+00,\n                 2.2236e+01,  2.0851e+01],\n               [ 1.4236e+01,  3.3656e+01,  3.3081e+01,  ...,  2.2807e+01,\n                 3.4478e+01,  1.7018e+01],\n               ...,\n               [ 1.9373e+01,  1.7202e+01,  7.0146e-01,  ...,  9.4769e+00,\n                 5.3170e+00,  7.5832e+00],\n               [ 2.3346e+01,  2.6482e+01,  1.0779e+01,  ...,  1.0170e+00,\n                 9.9077e+00,  6.9110e+00],\n               [ 1.7564e+01,  2.1025e+01,  1.2639e+01,  ...,  4.6908e+00,\n                 5.8073e+00,  3.1757e+00]],\n\n              [[-5.2906e+00, -2.7127e+00,  1.5336e+01,  ..., -3.3235e+00,\n                 3.0431e+00,  4.4982e+00],\n               [-3.8740e+00, -2.7825e-01,  1.8444e+01,  ..., -6.8304e+00,\n                -9.7458e+00, -2.2539e+00],\n               [-2.3341e+00,  1.1108e+00,  9.1996e+00,  ...,  1.4541e+00,\n                 4.0044e+00,  5.3693e+00],\n               ...,\n               [-5.5670e+00, -3.2424e+00, -1.0724e+01,  ..., -2.3431e+00,\n                 3.1732e+00,  2.2250e+00],\n               [ 4.7924e+00,  2.9135e+00,  1.0695e+00,  ..., -3.1337e+00,\n                 3.6863e+00,  9.4232e+00],\n               [ 1.3064e+01,  1.4679e+00,  4.2512e+00,  ...,  2.0431e+00,\n                 5.5197e+00,  9.0792e+00]]],\n\n\n             [[[-4.1837e+00,  4.4228e+00,  7.5466e+00,  ...,  9.0294e+00,\n                 1.8985e+00,  1.3472e+00],\n               [ 3.4190e+00,  7.7751e+00,  1.4765e+01,  ...,  1.0512e+01,\n                 1.2134e+01,  1.3603e+01],\n               [ 1.1120e+01,  1.9633e+01,  1.5074e+01,  ...,  7.3059e+00,\n                 5.4364e+00,  1.3671e+01],\n               ...,\n               [ 7.7573e+00,  6.5959e+00,  2.4744e+00,  ...,  1.3797e+01,\n                 2.7515e+00,  2.0304e+00],\n               [ 5.0850e+00,  9.8228e+00,  7.2073e+00,  ...,  3.0873e-01,\n                -2.5682e+01, -1.1913e+01],\n               [-1.9177e+00,  1.0239e+00,  2.1603e+00,  ...,  7.6998e-02,\n                -1.0977e+01, -1.0928e+01]],\n\n              [[ 9.6061e+00,  1.5370e+00,  2.8735e+00,  ...,  5.0786e+00,\n                 1.1083e+01,  9.2034e+00],\n               [ 3.0860e+00,  7.1579e+00,  6.0781e+00,  ...,  7.9867e+00,\n                 3.1649e+00,  5.9685e+00],\n               [ 2.0483e+00,  1.4068e+01,  9.0135e+00,  ...,  4.9285e+00,\n                 6.5073e+00, -3.5669e+00],\n               ...,\n               [-1.0659e+00,  4.2513e+00,  3.0158e+00,  ...,  2.1836e+00,\n                -7.4681e+00, -1.0047e+01],\n               [-6.4479e+00, -1.6330e+00,  7.5435e+00,  ...,  1.3588e+01,\n                 1.1536e+00, -1.1312e+01],\n               [-4.2848e+00,  4.4101e+00,  9.2249e+00,  ...,  1.7328e+01,\n                 1.4831e+01, -7.3449e-01]],\n\n              [[-1.3682e+00, -5.1753e+00, -4.2238e+00,  ..., -1.3184e+01,\n                -1.0707e+01, -9.5371e+00],\n               [-1.6860e+00, -3.3730e+00,  3.3757e+00,  ..., -9.6586e+00,\n                -4.3802e+00, -4.1719e-02],\n               [-4.7046e-01, -4.2123e-01, -3.8035e+00,  ..., -9.6757e+00,\n                -1.5742e+01, -1.8032e+00],\n               ...,\n               [-4.2253e+00, -3.7856e+00, -4.7338e+00,  ...,  6.0688e+00,\n                -2.2821e+00,  6.7935e+00],\n               [-8.7673e+00, -2.8977e-01, -1.8588e+00,  ...,  7.1490e+00,\n                -4.2642e+00, -4.3403e-01],\n               [-1.3027e+01, -9.8119e+00, -4.1386e+00,  ..., -3.9124e+00,\n                -1.8475e+01, -5.6218e+00]],\n\n              ...,\n\n              [[-1.3048e+01, -6.4990e+00,  3.7771e+00,  ..., -3.5662e-01,\n                -2.1649e+01, -1.9933e+01],\n               [-1.3308e+01,  1.0137e+00,  4.0163e+00,  ...,  1.5178e+01,\n                -1.8908e+01, -2.6649e+01],\n               [-6.0838e+00,  4.4509e+00,  9.2461e+00,  ...,  1.2517e+01,\n                -6.9830e+00, -1.0261e+01],\n               ...,\n               [ 3.8149e+00,  1.0815e+01,  3.1839e+00,  ...,  9.5991e+00,\n                 3.8434e+00,  2.1540e+00],\n               [-2.1296e+00, -1.8112e+00, -2.8965e+00,  ..., -6.4432e+00,\n                -6.9710e+00, -5.0703e-01],\n               [ 7.2261e+00, -2.4904e+00, -6.8260e+00,  ..., -8.1620e+00,\n                -4.7305e+00, -1.3990e+00]],\n\n              [[ 6.4019e+00,  1.4235e+01,  1.1706e+01,  ...,  8.1696e+00,\n                 1.3590e+01,  1.1112e+01],\n               [ 9.0817e+00,  1.8450e+01,  1.8520e+00,  ...,  2.9886e+00,\n                 1.0353e+01,  1.9502e+01],\n               [ 1.2688e+01,  1.2391e+01,  7.0384e+00,  ..., -4.8993e+00,\n                 4.6831e+00,  1.3654e+01],\n               ...,\n               [ 6.8772e+00,  4.8467e+00,  3.6678e+00,  ..., -3.3305e+00,\n                 2.7749e+01,  3.7785e+01],\n               [ 1.4010e+01,  1.1408e+01,  5.3943e+00,  ..., -2.1537e+01,\n                 2.1972e+00,  3.4320e+01],\n               [ 1.4877e+01,  1.7063e+01,  6.9275e+00,  ..., -1.4477e+01,\n                -3.4017e+00,  3.0101e+01]],\n\n              [[-3.0831e+00,  3.0010e+00,  6.9832e+00,  ..., -1.8217e+00,\n                -7.1681e+00, -3.4423e+00],\n               [ 8.3796e+00,  3.6513e+00, -2.5611e+00,  ..., -6.8060e+00,\n                -1.4611e+01, -8.5084e+00],\n               [-6.5332e-01, -1.1911e+01, -1.2325e+01,  ..., -4.0894e+00,\n                -1.0735e+01,  1.7551e+00],\n               ...,\n               [ 3.4406e+00,  1.4224e+00, -3.6680e+00,  ..., -1.0623e+01,\n                -2.8591e+01, -1.4722e+01],\n               [ 3.2616e+00,  2.6747e+00, -1.9402e+00,  ..., -6.7384e+00,\n                -1.2895e+01, -1.5971e+01],\n               [ 1.2498e+01,  2.2431e+00, -3.8494e+00,  ..., -9.0071e+00,\n                -2.1719e+01, -1.8405e+01]]]], device='cuda:0',\n            grad_fn=<AliasBackward0>)\n\n\n\n2**bits, 0-255\n\n(tensor([  1,   2,   4,   8,  16,  32,  64, 128], device='cuda:0'), -255)\n\n\n\n(1*(projection>0)).shape,(2**bits[...,None,None]).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8, 1, 1]))\n\n\n\n\n\ntorch.Size([64, 10, 10])\n\n\n\nprojection.shape,(2.**bits).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8]))\n\n\n\n(1*(projection>0)).shape,(2**bits).shape\n\n(torch.Size([64, 8, 10, 10]), torch.Size([8]))\n\n\n\n2**bits,(1*(projection>0)).shape\n\n(tensor([  1,   2,   4,   8,  16,  32,  64, 128], device='cuda:0'),\n torch.Size([64, 8, 10, 10]))\n\n\n\n((1*(projection>0)[:,:,0,0])*(2**bits))\n\nTensorImage([[  0,   2,   0,   8,   0,   0,   0,   0],\n             [  1,   2,   0,   0,   0,  32,  64, 128],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   0,   0,   0,   0,   0],\n             [  0,   2,   4,   8,  16,   0,   0, 128],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   4,   0,   0,  32,  64, 128],\n             [  0,   2,   0,   0,   0,  32,   0,   0],\n             [  1,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  0,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,   0,  64,   0],\n             [  0,   2,   0,   0,   0,  32,  64, 128],\n             [  1,   2,   0,   0,   0,  32,   0,   0],\n             [  0,   2,   4,   8,   0,   0,   0,   0],\n             [  1,   2,   0,   0,   0,  32,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   4,   0,   0,  32,   0,   0],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  1,   2,   4,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   0,   0,   0,  64,   0],\n             [  1,   2,   4,   0,   0,  32,   0,   0],\n             [  0,   2,   0,   0,  16,   0,   0, 128],\n             [  1,   2,   0,   0,  16,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,  16,  32,   0,   0],\n             [  1,   2,   0,   0,   0,   0,  64, 128],\n             [  1,   2,   0,   0,   0,   0,   0,   0],\n             [  0,   2,   4,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  1,   2,   0,   0,   0,   0,   0, 128],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  0,   2,   0,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   8,   0,  32,  64,   0],\n             [  0,   0,   4,   8,  16,   0,  64, 128],\n             [  0,   0,   4,   8,  16,  32,   0, 128],\n             [  1,   0,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   4,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,  16,   0,   0, 128],\n             [  1,   2,   4,   8,   0,   0,   0, 128],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   0,   8,  16,   0,  64, 128],\n             [  0,   2,   0,   0,   0,  32,   0,   0],\n             [  0,   2,   4,   0,   0,   0,   0, 128],\n             [  1,   0,   0,   0,   0,  32,  64,   0],\n             [  1,   2,   0,   0,   0,  32,   0, 128],\n             [  0,   2,   4,   0,  16,  32,   0, 128],\n             [  1,   2,   0,   0,  16,   0,  64, 128],\n             [  0,   0,   0,   0,  16,  32,  64,   0],\n             [  1,   2,   4,   0,  16,   0,  64, 128],\n             [  0,   2,   0,   0,   0,   0,  64,   0],\n             [  0,   2,   0,   8,   0,   0,  64,   0],\n             [  0,   2,   0,   0,   0,   0,   0, 128],\n             [  0,   2,   4,   8,   0,   0,  64,   0],\n             [  1,   2,   0,   8,   0,   0,  64,   0],\n             [  1,   2,   0,   0,   0,  32,  64,   0],\n             [  0,   2,   4,   8,  16,   0,  64,   0],\n             [  1,   2,   4,   8,  16,   0,   0, 128],\n             [  0,   2,   4,   0,  16,   0,   0,   0],\n             [  0,   2,   0,   8,  16,   0,  64,   0]], device='cuda:0')\n\n\n\ntorch.einsum('ijkl,j->ikl',1.*(projection>0),(2.**bits))\n\nTensorImage([[[ 10.,  99., 226.,  ...,  66., 194., 194.],\n              [ 35., 163., 162.,  ..., 194., 162., 130.],\n              [211., 242., 178.,  ...,  67., 115., 119.],\n              ...,\n              [151., 134., 166.,  ..., 162., 166., 165.],\n              [134., 134., 166.,  ..., 163., 146., 182.],\n              [150., 150., 150.,  ..., 146., 178., 178.]],\n\n             [[227., 243., 211.,  ..., 226.,  66., 106.],\n              [227.,  98., 227.,  ..., 226.,  82.,  18.],\n              [ 34.,  98.,  98.,  ..., 114., 242., 166.],\n              ...,\n              [232., 169., 170.,  ...,  98., 102., 110.],\n              [ 42.,  42.,  43.,  ...,  98., 102., 102.],\n              [ 99.,  35., 111.,  ...,  42.,  38.,  38.]],\n\n             [[ 99.,  83.,  67.,  ...,  67., 227., 215.],\n              [ 67.,  67.,  87.,  ..., 227., 243., 231.],\n              [ 67.,  67.,  83.,  ..., 229., 119., 241.],\n              ...,\n              [ 99.,   3.,  67.,  ...,  34.,   2., 231.],\n              [ 35.,   3., 107.,  ..., 130., 130.,  71.],\n              [  3.,   8.,  75.,  ..., 130., 135.,  69.]],\n\n             ...,\n\n             [[159., 183., 231.,  ..., 194., 134., 151.],\n              [151., 247.,  99.,  ..., 246., 150., 210.],\n              [147., 161.,  35.,  ...,  34., 146., 147.],\n              ...,\n              [179., 195., 194.,  ..., 162., 226., 131.],\n              [150., 226.,  98.,  ..., 130., 194., 195.],\n              [146.,  98.,  98.,  ..., 130., 194., 195.]],\n\n             [[ 22.,  54., 246.,  ...,  38., 226., 226.],\n              [ 68., 102., 246.,  ...,  98.,  98.,  66.],\n              [ 82., 226., 210.,  ..., 227., 226., 230.],\n              ...,\n              [ 68.,  68.,  68.,  ..., 102., 198., 202.],\n              [226., 226., 194.,  ..., 102., 196., 204.],\n              [226., 226., 226.,  ..., 222., 212., 214.]],\n\n             [[ 90., 195., 227.,  ...,  91.,  83.,  83.],\n              [203., 235., 119.,  ..., 115.,  75.,  75.],\n              [ 75., 115., 115.,  ...,  51.,  83., 201.],\n              ...,\n              [249., 243., 115.,  ...,  63., 105., 109.],\n              [193., 193.,  91.,  ...,  31.,  90.,  64.],\n              [232., 203.,  91.,  ...,  27.,  18.,  64.]]], device='cuda:0')\n\n\n\n\nHow would we calculate the bucket for each x,y location?\n\nhash_loc=torch.einsum('bchw,c->bhw',(projection>0).float(),2.**bits).int()\nhash_loc.shape,hash_loc\n\n(torch.Size([64, 10, 10]),\n TensorImage([[[ 10,  99, 226,  ...,  66, 194, 194],\n               [ 35, 163, 162,  ..., 194, 162, 130],\n               [211, 242, 178,  ...,  67, 115, 119],\n               ...,\n               [151, 134, 166,  ..., 162, 166, 165],\n               [134, 134, 166,  ..., 163, 146, 182],\n               [150, 150, 150,  ..., 146, 178, 178]],\n \n              [[227, 243, 211,  ..., 226,  66, 106],\n               [227,  98, 227,  ..., 226,  82,  18],\n               [ 34,  98,  98,  ..., 114, 242, 166],\n               ...,\n               [232, 169, 170,  ...,  98, 102, 110],\n               [ 42,  42,  43,  ...,  98, 102, 102],\n               [ 99,  35, 111,  ...,  42,  38,  38]],\n \n              [[ 99,  83,  67,  ...,  67, 227, 215],\n               [ 67,  67,  87,  ..., 227, 243, 231],\n               [ 67,  67,  83,  ..., 229, 119, 241],\n               ...,\n               [ 99,   3,  67,  ...,  34,   2, 231],\n               [ 35,   3, 107,  ..., 130, 130,  71],\n               [  3,   8,  75,  ..., 130, 135,  69]],\n \n              ...,\n \n              [[159, 183, 231,  ..., 194, 134, 151],\n               [151, 247,  99,  ..., 246, 150, 210],\n               [147, 161,  35,  ...,  34, 146, 147],\n               ...,\n               [179, 195, 194,  ..., 162, 226, 131],\n               [150, 226,  98,  ..., 130, 194, 195],\n               [146,  98,  98,  ..., 130, 194, 195]],\n \n              [[ 22,  54, 246,  ...,  38, 226, 226],\n               [ 68, 102, 246,  ...,  98,  98,  66],\n               [ 82, 226, 210,  ..., 227, 226, 230],\n               ...,\n               [ 68,  68,  68,  ..., 102, 198, 202],\n               [226, 226, 194,  ..., 102, 196, 204],\n               [226, 226, 226,  ..., 222, 212, 214]],\n \n              [[ 90, 195, 227,  ...,  91,  83,  83],\n               [203, 235, 119,  ..., 115,  75,  75],\n               [ 75, 115, 115,  ...,  51,  83, 201],\n               ...,\n               [249, 243, 115,  ...,  63, 105, 109],\n               [193, 193,  91,  ...,  31,  90,  64],\n               [232, 203,  91,  ...,  27,  18,  64]]], device='cuda:0',\n             dtype=torch.int32))\n\n\n\nhash_loc.flatten().mode()\n\ntorch.return_types.mode(\nvalues=TensorImage(99, device='cuda:0', dtype=torch.int32),\nindices=TensorImage(1, device='cuda:0'))\n\n\n\nhash_loc==99\n\nTensorImage([[[False,  True, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [ True, False, False,  ..., False, False, False]],\n\n             [[ True, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [ True, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             ...,\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False,  True,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]],\n\n             [[False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              ...,\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False],\n              [False, False, False,  ..., False, False, False]]],\n            device='cuda:0')\n\n\n\n\nHow would we generate a mask for the locations that equal the mode(199)?\n\n(hash_loc==199).shape,(hash_loc==199)\n\n(torch.Size([64, 10, 10]),\n TensorImage([[[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ...,  True,  True,  True],\n               [False, False, False,  ...,  True, False, False]],\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]],\n \n              [[False, False, False,  ...,  True,  True,  True],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False,  True,  True],\n               [False, False, False,  ..., False, False,  True],\n               [False, False, False,  ...,  True, False, False]],\n \n              ...,\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ...,  True, False, False],\n               [ True, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]],\n \n              [[False, False, False,  ...,  True, False, False],\n               [False, False, False,  ...,  True, False, False],\n               [False, False, False,  ...,  True, False, False],\n               ...,\n               [False, False,  True,  ..., False, False, False],\n               [False, False,  True,  ..., False, False, False],\n               [False, False,  True,  ..., False, False, False]],\n \n              [[False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               ...,\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False],\n               [False, False, False,  ..., False, False, False]]],\n             device='cuda:0'))\n\n\n\nb.shape,hash_loc.shape\n\n(torch.Size([64, 3, 320, 320]), torch.Size([64, 10, 10]))\n\n\n\n%timeit b.view(64, 3, 10,320//10, 10,320//10)\n\n15.4 µs ± 2.08 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\ntiled_img=b.view(64, 3, 10,320//10, 10,320//10).permute(0,2,4,1,3,5)\nb.shape,tiled_img.shape\n\n(torch.Size([64, 3, 320, 320]), torch.Size([64, 10, 10, 3, 32, 32]))\n\n\n\nshow_images(b.view(64, 10, 10, 3, 32, 32)[0].flatten(end_dim=1),nrows=10)\n\n\n\n\n\nb.view(64, 3, 10,320//10, 10,320//10).shape\n\ntorch.Size([64, 3, 10, 32, 10, 32])\n\n\n\nshow_images(tiled_img[0].flatten(end_dim=1),nrows=10)\n\n\n\n\n\ntiled_img[1].shape,(hash_loc==99)[0].shape\n\n(torch.Size([10, 10, 3, 32, 32]), torch.Size([10, 10]))\n\n\n\ntiled_img[8][(hash_loc==99)[8]].shape\n\ntorch.Size([15, 3, 32, 32])\n\n\n\n\nHow would select select the patches of the image that fall into our mode bucket?\n\ntiled_img[1][hash_loc[1]==199].shape\n\ntorch.Size([11, 3, 32, 32])\n\n\n\nshow_image(b[0])\n\n<AxesSubplot:>\n\n\n\n\n\nSeems to detect “objects.” Other images included scales for weight things, and sea shells. Dog nose, might be due to that space being close.\n\nshow_images(tiled_img[0][hash_loc[0]==199][:64],nrows=4)\n\n\n\n\n\nshow_images(tiled_img[8][(hash_loc==99)[8]],nrows=4)\n\n\n\n\n\nshow_image(b[8])\n\n<AxesSubplot:>\n\n\n\n\n\n\nlen(dls.dataset)*10*10\n\n1295200\n\n\n\n1295200/256\n\n5059.375"
  },
  {
    "objectID": "posts/misc/Backprop.html",
    "href": "posts/misc/Backprop.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.MNIST)\n\n\npath.ls()\n\n(#2) [Path('/home/molly/data/mnist_png/testing'),Path('/home/molly/data/mnist_png/training')]\n\n\n\ndls = DataBlock((ImageBlock(cls=PILImageBW),CategoryBlock),\n                splitter=GrandparentSplitter('training','testing'),\n                item_tfms=Resize(32),\n                get_y=parent_label,\n                get_items=get_image_files).dataloaders(path)\n\n\nxs,ys=dls.one_batch()\n\nFirst, what is a prediction? A prediction is some “guess” that ranges from 0-1, or 0-100%. We define that here as something that ranges from 0 to 1. (we leave out actual 0 for math reasons).\n\npreds=np.linspace(0.001,1.)\n\nLets look at log likelihood. \\(p(x)\\) here is our label, and can be thought of as either 1 or 0.\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{q(x)}} \\]\nLets graph this, notice near 0, or a completely wrong prediction the error explodes.\n\nF.nll_loss(pt,torch.ones(50).long(),reduction='none').shape\n\ntorch.Size([50])\n\n\n\npt = torch.linspace(0.001,1.,50)[None].repeat(50,1)\nplt.plot(torch.linspace(0.001,1.,50),F.nll_loss(pt,torch.ones(50).long(),reduction='none'))\n\n\n\n\n\nplt.plot(preds,-np.log(preds))\n\n\n\n\nA quick note, cross entropy loss is also the sum of entropy and the KL divergence, KL divergence you will probably see in some diffusion papers. So whenever you see people minimizing the KL divergence, it is a proxy to attempting to minimize the log likelihood as well.\n\\[ H(p,q) =  H(p)+D_{KL}(P\\parallel Q)\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{p(x)\\log{p(x)}}+\\sum_{x\\in X}{p(x)\\log{\\frac{p(x)}{q(x)}}}\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{p(x)\\log{p(x)}}+\\sum_{x\\in X}{p(x)\\log{p(x)}}-\\sum_{x\\in X}{p(x)\\log{q(x)}}\\]\n\\[p(x)=1\\]\n\\[ H(p,q) =  \\sum_{x\\in X}{1\\log{1}}+\\sum_{x\\in X}{1\\log{1}}-\\sum_{x\\in X}{1\\log{q(x)}}\\]\n\\[ H(p,q) =  -\\sum_{x\\in X}{p(x)\\log{q(x)}}\\]\n\ndef entropy_plus_kl(p,q):\n    return p*np.log(p)+p*np.log(p/q)\n\n\nplt.plot(preds,-np.log(preds))\nplt.plot(preds,entropy_plus_kl(1,preds),linewidth=6,linestyle=':')\n\n\n\n\nOkay, now we know why we want to use cross-entropy. How do we generate values from 0 to 1? Well… lets use softmax.\n\\[ Softmax(x_i)=\\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\]\n\nxss=array([-2,-1,0,1,2])\nnp.exp(xss)/np.exp(xss).sum()\n\narray([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865])\n\n\n\nxss=array([-2,-1,0,1,2])\nnp.exp(xss),np.exp(xss)/np.exp(xss).sum()\n\n(array([0.13533528, 0.36787944, 1.        , 2.71828183, 7.3890561 ]),\n array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]))\n\n\n\nplt.plot(np.exp(np.linspace(0,10)))\n\n\n\n\n\nxss=array([100,0,0,0,0])\nnp.exp(xss[0])/np.exp(xss).sum()\n\n1.0\n\n\n\ndef relu(xss): return xss*(xss>0)\nrelu(xss)\n\narray([0, 0, 0, 1, 2])\n\n\n\n\n\n3\n\n\n\nsoftmax(xss).sum()\n\n1.0\n\n\n\ndef softmax(x): return np.exp(x)/np.exp(x).sum()\n\n\nplt.plot(np.linspace(-5,5),softmax(np.linspace(-5,5)))\n\n\n\n\nSoftmax can be applied to the entire real numberline, giving our model flexibility in its output. It pushes up the highest values to be a significant portion of the final probability.\nIt is probably important to pause here and look at what happens when crossentropy and softmax are combined.\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{q(x)}} \\]\n\\[ Softmax(x_i)=\\frac{e^{x_i}}{\\sum_j{e^{x_j}}} \\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{\\frac{e^{x_i}}{\\sum_j{e^{x_j}}}}} \\]\n\\[ \\log\\frac{x}{y} = \\log x-\\log y\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)\\log{e^{x_i}}}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j}})}}\\]\nNow, here comes the logsumexp trick. We can subtract a constant a from the exponent…\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^ae^{x_j-a}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)\\log{(e^a\\sum_j{e^{x_j-a}})}}\\]\n\\[ H(p,q) = -\\sum_{x\\in X}{p(x)x_i}+ \\sum_{x\\in X}{p(x)a}+\\sum_{x\\in X}{p(x)\\log{(\\sum_j{e^{x_j-a}})}}\\]\nor… in the case of \\(p(x)=1\\)\n\\[ H(p,q) = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\ndef cross_soft(x):\n    a=x.max()\n    return -x+a+torch.log(torch.exp(x-a).sum())\n\n\nplt.plot(torch.linspace(-2,2,50),cross_soft(torch.linspace(-2,2,50)))\nplt.plot(torch.linspace(-2,2,50),F.cross_entropy(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50),reduction='none'),linewidth=6,linestyle=':')\n\n\n\n\nNow, lets rewrite cross_soft so it can take a target, and take a mean.\n\ndef cross_soft2(x,targ,reduction='mean'):\n    x=x[range(targ.shape[0]),targ]\n    a=2\n    if(reduction=='mean'):\n        return (-x+a+torch.log(torch.exp(x-a).sum())).mean()\n    else:\n        return -x+a+torch.log(torch.exp(x-a).sum())\n\n\ncross_soft2(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))\n\ntensor(4.5290)\n\n\n\nF.cross_entropy(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50),)\n\ntensor(4.5290)\n\n\nOkay lets try a derivative …\n\\[ H(p,q) = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\ntorch.linspace(-2,2,50)[None].repeat(50,1)\n\ntensor([[-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        ...,\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000],\n        [-2.0000, -1.9184, -1.8367,  ...,  1.8367,  1.9184,  2.0000]])\n\n\n\\[ \\frac{H(p,q)}{dX} = -x_i+ a+\\log{(\\sum_j{e^{x_j-a}})}\\]\n\\[ \\frac{dH(p,q)}{dx_k} = -1+ 0+\\frac{e^{x_k-a}}{\\sum_j{e^{x_j-a}}}\\]\nj does not equal i\n\\[ \\frac{dH(p,q)}{dx_k} = 0+ 0+\\frac{e^{x_k-a}}{\\sum_j{e^{x_j-a}}}\\]\n\\[ \\frac{dH(p,q)}{dx_k} = -y+softmax(x)\\]\n\ndef cross_soft2_grad(x,targ,reduction='mean'):\n    a=x.max()\n    return ((torch.exp(x-a)/(torch.exp(x-a).sum(1)))-F.one_hot(targ))/targ.shape[0]\n\n\ncross_soft2_grad(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))\n\ntensor([[-1.9971e-02,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05, -1.9968e-02,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05, -1.9966e-02,  ...,  1.3545e-03,\n          1.4697e-03,  1.5947e-03],\n        ...,\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ..., -1.8646e-02,\n          1.4697e-03,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n         -1.8530e-02,  1.5947e-03],\n        [ 2.9208e-05,  3.1692e-05,  3.4388e-05,  ...,  1.3545e-03,\n          1.4697e-03, -1.8405e-02]])\n\n\n\nxs=torch.linspace(-2,2,50)[None].repeat(50,1).requires_grad_()\nloss=F.cross_entropy(xs,torch.arange(50),reduction='mean')\nloss.backward()\n((xs.grad-cross_soft2_grad(torch.linspace(-2,2,50)[None].repeat(50,1),torch.arange(50))).abs()<0.00000001).all()\n\ntensor(True)\n\n\n\\[ y_{ik}=\\sum_j{x_{ij}w_{jk}+b_k} \\]\n\\[ \\frac{dY}{dX} =X\\frac{dW}{dX}+\\frac{dX}{dX}W^T + \\frac{dB}{dX}\\]\n\\[ \\frac{dY}{dX} =W^T \\]\n\\[ \\frac{dY}{dW} = X \\]\n\\[ \\frac{dY}{dB} =1 \\]\n\\[ y_{ik}=\\sum_j{w_{ij}x_{jk}} \\]\n\\[ \\frac{dy_{ik}}{dx_{ij}}=\\sum_j{0x_{jk}+w_{ij}\\frac{dx_{jk}}{dx_{jk}}} \\]"
  },
  {
    "objectID": "posts/misc/image_augs.html",
    "href": "posts/misc/image_augs.html",
    "title": "blog",
    "section": "",
    "text": "path = untar_data(URLs.IMAGENETTE_320)\n\n\nimg = Image.open(path.ls()[0].ls()[3].ls()[0])\nimg = TensorImage(image2tensor(img)[None]/255.)\n\n\naug_transforms(max_lighting=.99)[1]\n\nBrightness -- {'max_lighting': 0.99, 'p': 1.0, 'draw': None, 'batch': False}:\nencodes: (TensorImage,object) -> encodes\ndecodes: \n\n\n\nshow_images(aug_transforms(max_lighting=.99)[1](img.repeat(12,1,1,1)))\n\n\n\n\n\nRandTransform\nNotice that p, can be set to control the probability of a transform being applied.\n\nclass RandTransform(DisplayedTransform):\n    \"A transform that before_call its state at each `__call__`\"\n    do,nm,supports,split_idx = True,None,[],0\n    def __init__(self, \n        p:float=1., # Probability of applying Transform\n        nm:str=None,\n        before_call:callable=None, # Optional batchwise preprocessing function\n        **kwargs\n    ):\n        store_attr('p')\n        super().__init__(**kwargs)\n        self.before_call = ifnone(before_call,self.before_call)\n\n    def before_call(self, \n        b, \n        split_idx:int, # Index of the train/valid dataset\n    ):\n        \"This function can be overridden. Set `self.do` based on `self.p`\"\n        self.do = self.p==1. or random.random() < self.p\n\n    def __call__(self, \n        b, \n        split_idx:int=None, # Index of the train/valid dataset\n        **kwargs\n    ):\n        self.before_call(b, split_idx=split_idx)\n        return super().__call__(b, split_idx=split_idx, **kwargs) if self.do else b\n\n\n\nAffine\nFastai has many affine transforms. These include crop, zoom, flip etc. Lets go through some now.\n\nxy_grid.shape\n\ntorch.Size([1, 320, 480, 2])\n\n\n\ndef show_grid(xy_grid):\n    neutral_dim=torch.zeros_like(xy_grid)[...,0,None]\n    normal_grid=torch.cat((xy_grid,neutral_dim),dim=3)\n    bad_mask = (normal_grid>1).int() + (normal_grid<-1).int()\n    bad_mask=-bad_mask.sum(-1)*10\n    normal_grid+=bad_mask[...,None]\n    show_images(((normal_grid+1)/2).clip(0,1))\n\n\ny_coords=torch.linspace(-1,1,img.shape[-2])\nx_coords=torch.linspace(-1,1,img.shape[-1])\nxy_grid=torch.meshgrid(x_coords, y_coords, indexing='xy')\nxy_grid=torch.stack(xy_grid,dim=2)[None]\nshow_grid(xy_grid)\n\n\n\n\n\nxy_grid\n\ntensor([[[[-1.0000, -1.0000],\n          [-0.9958, -1.0000],\n          [-0.9916, -1.0000],\n          ...,\n          [ 0.9916, -1.0000],\n          [ 0.9958, -1.0000],\n          [ 1.0000, -1.0000]],\n\n         [[-1.0000, -0.9937],\n          [-0.9958, -0.9937],\n          [-0.9916, -0.9937],\n          ...,\n          [ 0.9916, -0.9937],\n          [ 0.9958, -0.9937],\n          [ 1.0000, -0.9937]],\n\n         [[-1.0000, -0.9875],\n          [-0.9958, -0.9875],\n          [-0.9916, -0.9875],\n          ...,\n          [ 0.9916, -0.9875],\n          [ 0.9958, -0.9875],\n          [ 1.0000, -0.9875]],\n\n         ...,\n\n         [[-1.0000,  0.9875],\n          [-0.9958,  0.9875],\n          [-0.9916,  0.9875],\n          ...,\n          [ 0.9916,  0.9875],\n          [ 0.9958,  0.9875],\n          [ 1.0000,  0.9875]],\n\n         [[-1.0000,  0.9937],\n          [-0.9958,  0.9937],\n          [-0.9916,  0.9937],\n          ...,\n          [ 0.9916,  0.9937],\n          [ 0.9958,  0.9937],\n          [ 1.0000,  0.9937]],\n\n         [[-1.0000,  1.0000],\n          [-0.9958,  1.0000],\n          [-0.9916,  1.0000],\n          ...,\n          [ 0.9916,  1.0000],\n          [ 0.9958,  1.0000],\n          [ 1.0000,  1.0000]]]])\n\n\n\nshow_images(F.grid_sample(img,xy_grid))\n\n\n\n\n\ndef make_grid(x_coords,y_coords):\n    xy_grid=torch.meshgrid(x_coords, y_coords, indexing='xy')\n    xy_grid=torch.stack(xy_grid,dim=2)[None]\n    return xy_grid\n\n\ny_coords=torch.linspace(-1,1,img.shape[-2])\nx_coords=torch.linspace(-1,1,img.shape[-1])\nmake_grid(y_coords)\nshow_grid(xy_grid)\n\n\nSlide Left\n\nshow_grid(make_grid(x_coords+1,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(-1*x_coords,y_coords)))\n\n\n\n\n\n\nFlip\n\nshow_grid(make_grid(-1*x_coords,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(2.*x_coords,y_coords)))\n\n\n\n\n\n\nSquish/Resize x dim\n\nshow_grid(make_grid(2*x_coords,y_coords),)\n\n\n\n\n\nshow_images(F.grid_sample(img,make_grid(2*x_coords,y_coords)))\n\n\n\n\n\ny_coords.shape\n\ntorch.Size([320])\n\n\n\nx_coords.shape\n\ntorch.Size([480])\n\n\n\n\nWhy can’t we use the technique we have used to implement rotate/warp?\nI mostly introduced the previous techniques to make things easy to understand by making x and y indepentdent, but affine transformations can work off of the current x and y values, which takes a bit more code to implement. Lets jump straight into building these like fastai.\n\n\nF.affine_grid\nAffine grids work on much smaller grids.\n\ntranslate_grid=torch.tensor([[1.,0,.5],\n              [0,1,0]])\n\n\ncoords_grid=F.affine_grid(translate_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\nhttps://en.wikipedia.org/wiki/Affine_transformation\n\ntorch.tensor([[1,0,0],\n              [0,1,0]])\n\ntensor([[1, 0, 0],\n        [0, 1, 0]])\n\n\n\ntorch.tensor([[1.,0,0],\n              [0,1,0]])\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.]])\n\n\n\nidentity_grid=torch.tensor([[1.,0,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(identity_grid[None], img.shape)\nshow_grid(coords_grid)\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\n\n\nrot_grid=torch.tensor([[math.cos(.5),-math.sin(.5),0],\n              [math.sin(.5),math.cos(.5),0]])\ncoords_grid=F.affine_grid(rot_grid[None], img.shape)\nshow_grid(coords_grid)\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\n\n\n\n\nRotate\n\nmath.cos(.5)\n\n0.8775825618903728\n\n\n\nrotate_grid=torch.tensor([[math.cos(1),math.sin(1),0],\n              [-math.sin(1),math.cos(1),0]])\ncoords_grid=F.affine_grid(rotate_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\nshear_grid=torch.tensor([[1,.5,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(shear_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\n\nHow would we warp/skew?\n\nwarp_grid=torch.tensor([[1.,.5,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(warp_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid))\n\n\n\n\n\nCombining affine augmentations\nLets look at the affine grid identity.\n\naffine_grid=torch.tensor([[1.,0,0],\n              [0,1,0]])\ncoords_grid=F.affine_grid(affine_grid[None], img.shape)\nshow_grid(coords_grid)\n\n\n\n\nDoes this affine grid identity look familiar? Can you think of a way to combine affine transforms?\n\n\nImplementation\n\ndef combine_affines(affines):\n    id_row=lambda a:torch.cat((a,torch.tensor([.0,0,1])[None]))\n    comb_mat=id_row(affines[0])\n    for a in affines:\n        comb_mat@=id_row(a)\n    return comb_mat[:2]\n\n\nwrt_grid=combine_affines([warp_grid,rotate_grid,translate_grid])\n\n\nwrt_grid\n\ntensor([[-0.3012,  1.3818, -0.3012],\n        [-0.8415,  0.5403, -0.8415]])\n\n\n\ncoords_grid=F.affine_grid(wrt_grid[None], img.shape)\n\n\nshow_grid(coords_grid)\n\n\n\n\n\nshow_images(F.grid_sample(img,coords_grid)),show_images(img)\n\n(None, None)\n\n\n\n\n\n\n\n\n\n_BrightnessLogit??\n\nObject `_BrightnessLogit` not found.\n\n\n\n\n\nLighting\n\nshow_images((img+.4))\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nx=TensorImage(torch.tensor([.01* i for i in range(0,101)]))\nf_lin= lambda x:(2*(x-0.5)+0.5).clamp(0,1) #blue line\nf_log= lambda x:2*x #red line\nplt.plot(x,f_lin(x),'b',x,torch.sigmoid(f_log(logit(x))),'r');\n\n\n\n\nWhat is special about logit in relationship to sigmoid?\n\nshow_images(img+.5)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nshow_images(torch.sigmoid(logit(img)+logit(torch.tensor(.85))))\n\n\n\n\n\nshow_images(img)\n\n\n\n\n\nlogit??\n\n\nHow to do contrast?\n\nshow_images(torch.sigmoid(logit(img)*4))\n\n\n\n\n\n\n\nNext Section\nOpen other notebook"
  }
]